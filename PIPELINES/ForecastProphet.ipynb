{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "import kfp.components as comp\n",
    "from kfp import compiler, dsl\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kubernetes.client.models import V1EnvVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetThresholds(output_thresholds_path: OutputPath(str)):\n",
    "\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    thresholds = {\n",
    "        \"heat_meter\":{\n",
    "            \"building1\": [0,3],\n",
    "            \"building2\": [0,4],\n",
    "            \"datacenter1\": [3,5],\n",
    "            \"psnc_garden\": [3,5],\n",
    "            \"hvac\": [3,5],\n",
    "            \"office\":[3,5],\n",
    "            \"flat1\": [3,5],\n",
    "            \"eagle\": [3,5],\n",
    "            \"altair\": [3,5]\n",
    "        },\n",
    "        \"electricity_meter\": {\n",
    "            \"building1\": [0,6],\n",
    "            \"building2\": [0,6],\n",
    "            \"datacenter1\": [3,5],\n",
    "            \"psnc_garden\": [3,5],\n",
    "            \"hvac\": [3,5],\n",
    "            \"office\": [3,5],\n",
    "            \"flat1\": [3,5],\n",
    "            \"eagle\": [3,5],\n",
    "            \"altair\": [3,5]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(output_thresholds_path, \"w\") as file:\n",
    "        json.dump(thresholds, file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(measurement_name: str, min_date: str, max_date: str,url_pilot : str,type_measurement :str,key_measurement : str,\n",
    "            filter_vars:list , filter_cases: list, output_data_forecast: OutputPath(str)):\n",
    "\n",
    "    import requests # To REQUIREMENTS\n",
    "    import json\n",
    "    import pandas as pd # To REQUIREMENTS\n",
    "    import maya # To REQUIREMENTS\n",
    "    from tqdm import tqdm\n",
    "    from icecream import ic\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    from retry import retry # TO REQUIREMENTS\n",
    "\n",
    "    def GetRequest(url, headers ={}, payload = {}):\n",
    "        response = requests.request(\"GET\", url, headers = headers, data = payload)\n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            dict_ = {\n",
    "                \"status_code\": response.status_code,\n",
    "                \"text\": response.text\n",
    "            }\n",
    "            return dict_\n",
    "    def DownloadAssetsData(measurement_name, url_pilot,bucket = \"renergetic\", min_date = \"yesterday\", max_date = \"tomorrow\"):\n",
    "        \n",
    "        from datetime import datetime\n",
    "        import pandas as pd\n",
    "        import maya\n",
    "        from tqdm import tqdm\n",
    "        from icecream import ic\n",
    "\n",
    "        test = True\n",
    "\n",
    "        try:\n",
    "            min_date_from = maya.when(min_date).datetime()\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MIN_DATE\")\n",
    "        \n",
    "        try: \n",
    "            max_date_from = maya.when(max_date).datetime()\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MAX_DATE\")\n",
    "        \n",
    "        datelist = pd.date_range(min_date_from, max_date_from)\n",
    "\n",
    "        data_ = []\n",
    "        for i in tqdm(range(len(datelist)-1)):\n",
    "            from_obj = datelist[i]\n",
    "            to_obj = datelist[i+1]\n",
    "            from_ = datetime.strftime(from_obj, \"%Y-%m-%d 00:00:00\")\n",
    "            to_ = datetime.strftime(to_obj, \"%Y-%m-%d 00:00:00\")\n",
    "\n",
    "            url = url_pilot + \"/data?measurements={measurement_name}&from={from_}&to={to_}\"\\\n",
    "                .format(measurement_name = measurement_name, from_ = from_, to_= to_)\n",
    "            info_ = GetRequest(url)\n",
    "            if type(info_) == list:\n",
    "                data_ = data_ + info_\n",
    "            elif type(info_) == dict:\n",
    "                print(\"Error\")\n",
    "                print(from_)\n",
    "                print(to_)\n",
    "        return data_\n",
    "    \n",
    "    def DataFrameAssests(list_data, name_field):\n",
    "        dicts = []\n",
    "        for data in list_data:\n",
    "            try:\n",
    "                if \"energy\" in data[\"fields\"].keys():\n",
    "                    name_value = \"energy\"\n",
    "                else:\n",
    "                    name_value = name_field\n",
    "                dict_ = {\n",
    "                    \"asset_name\": data[\"tags\"][\"asset_name\"],\n",
    "                    \"value\": float(data[\"fields\"][name_value]),\n",
    "                    \"ds\": data[\"fields\"][\"time\"]\n",
    "                }\n",
    "\n",
    "                if \"type_data\" in data[\"tags\"].keys():\n",
    "                    dict_[\"type\"] = data[\"tags\"][\"type_data\"]\n",
    "                elif \"typeData\" in data[\"tags\"].keys():\n",
    "                    dict_[\"type\"] = data[\"tags\"][\"typeData\"]\n",
    "                else:\n",
    "                    dict_[\"type\"] = \"None\"\n",
    "\n",
    "                if \"measurement_type\" in data[\"tags\"].keys():\n",
    "                    dict_[\"measurement_type\"] = data[\"tags\"][\"measurement_type\"]\n",
    "                else:\n",
    "                    dict_[\"measurement_type\"] = \"None\"\n",
    "                \n",
    "                if \"direction\" in data[\"tags\"].keys():\n",
    "                    dict_[\"direction\"] = data[\"tags\"][\"direction\"]\n",
    "                else:\n",
    "                    dict_[\"direction\"] = \"None\"\n",
    "                if \"domain\" in data[\"tags\"].keys():\n",
    "                    dict_[\"domain\"] = data[\"tags\"][\"domain\"]\n",
    "                else:\n",
    "                    dict_[\"domain\"] = \"None\"\n",
    "                \n",
    "                if \"sensor_id\" in data[\"tags\"].keys():\n",
    "                    dict_[\"id_sensor\"] = data[\"tags\"][\"sensor_id\"]\n",
    "                else:\n",
    "                    dict_[\"id_sensor\"] = \"None\"\n",
    "                \n",
    "                if \"interpolation_method\" in data[\"tags\"].keys():\n",
    "                    dict_[\"interpolation\"] = data[\"tags\"][\"interpolation_method\"]\n",
    "                else:\n",
    "                    dict_[\"interpolation\"] = \"None\"\n",
    "\n",
    "                dicts.append(dict_)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return pd.DataFrame(dicts)\n",
    "    \n",
    "    @retry(tries= 3)\n",
    "    def DownloadAndProcess(measurement_name, url_pilot, min_date, max_date, key_measurement):\n",
    "        # max_date = maya.now().add(days = 3).iso8601()\n",
    "        list_ = DownloadAssetsData(measurement_name, url_pilot,min_date = min_date, max_date = max_date)\n",
    "        data = DataFrameAssests(list_, key_measurement)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def FilterCases(var_value, filter_cases):\n",
    "        if var_value in filter_cases:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    data = DownloadAndProcess(measurement_name, url_pilot, min_date, max_date, key_measurement)\n",
    "    ic(data.shape[0])\n",
    "    data = data[data.type == type_measurement]\n",
    "\n",
    "    for i in range(len(filter_vars)):\n",
    "        data[\"Filter\"] = data[filter_vars[i]].apply(FilterCases, filter_cases = filter_cases[i])\n",
    "        data = data[data[\"Filter\"] == True]\n",
    "\n",
    "    ic(data.shape[0])\n",
    "\n",
    "    if data.shape[0] == 0:\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Not enough data for {measurement_name}\".format(measurement_name = measurement_name)\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "        \n",
    "        raise ValueError(\"Void data to forecast\")\n",
    "\n",
    "    data_output = {\n",
    "        \"value\": data[\"value\"].tolist(),\n",
    "        \"time_registered\": data[\"ds\"].tolist(),\n",
    "        \"asset_name\": data[\"asset_name\"].tolist()\n",
    "    }\n",
    "\n",
    "    with open(output_data_forecast, \"w\") as file:\n",
    "        json.dump(data_output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessData(input_data_path: InputPath(str), hourly_aggregate, minute_aggregate ,min_date, max_date, output_data_forecast: OutputPath(str)):\n",
    "\n",
    "    import maya\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from icecream import ic\n",
    "    \n",
    "    min_date = datetime.strftime(maya.when(min_date).datetime(), \"%Y-%m-%d\")\n",
    "    max_date = datetime.strftime(maya.when(max_date).datetime(), \"%Y-%m-%d\")\n",
    "    \n",
    "    ic(hourly_aggregate)\n",
    "\n",
    "    with open(input_data_path) as file:\n",
    "        data_str = json.load(file)\n",
    "    \n",
    "    data = pd.DataFrame(data_str)\n",
    "    # DEFINE HOURLY AGGREGATE PROCESS\n",
    "\n",
    "    if hourly_aggregate in [\"mean\",\"sum\", \"max\"]:\n",
    "        \n",
    "        list_dicts = []\n",
    "        list_data = []\n",
    "\n",
    "        for asset_name in pd.unique(data.asset_name):\n",
    "            data_iter = data[data.asset_name == asset_name]\n",
    "            \n",
    "            def GetHourDate(str_):\n",
    "                import maya\n",
    "                from datetime import datetime\n",
    "\n",
    "                return datetime.strftime(maya.parse(str_).datetime(), \"%Y-%m-%d %H:00:00\")\n",
    "\n",
    "            data_iter[\"hour\"] = data_iter[\"time_registered\"].apply(GetHourDate)\n",
    "            data_group = (data_iter.groupby('hour')\n",
    "                .agg({'time_registered':'count', 'value': hourly_aggregate})\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            df = data_group[[\"hour\", \"value\"]]\n",
    "            df = df.rename(columns={'hour':'ds', 'value': 'y'})\n",
    "            last_value = df.y.tolist()[0]\n",
    "            for ds_obj in pd.date_range(min_date, max_date):\n",
    "                for i in range(24):\n",
    "                    if i < 10:\n",
    "                        str_i = \"0\"+str(i)\n",
    "                    else:\n",
    "                        str_i = str(i)\n",
    "                    ds_str = \"{date} {H}:00:00\".format(date = ds_obj.strftime(\"%Y-%m-%d\"), H = str_i)\n",
    "\n",
    "                    if df[df.ds == ds_str].shape[0] == 0:\n",
    "                        if minute_aggregate == \"max\":\n",
    "                                value_ = last_value\n",
    "                        else:\n",
    "                            value_ = 0\n",
    "\n",
    "                        dict_ = {\n",
    "                            \"time_registered\": ds_str,\n",
    "                            \"value\": value_,\n",
    "                            \"asset_name\": asset_name\n",
    "                        }\n",
    "                    else:\n",
    "                        dict_ = {\n",
    "                            \"time_registered\": ds_str,\n",
    "                            \"value\": df[df.ds == ds_str].y.tolist()[0],\n",
    "                            \"asset_name\": asset_name\n",
    "                        }\n",
    "                        if minute_aggregate == \"max\":\n",
    "                            last_value = dict_[\"value\"]\n",
    "                    list_dicts.append(dict_)\n",
    "            if minute_aggregate == \"max\":\n",
    "                data_1 = pd.DataFrame(list_dicts)\n",
    "                data_1[\"value_1\"] = data_1[\"value\"].shift(1)\n",
    "                data_1[\"value\"] = data_1[\"value\"] - data_1[\"value_1\"]\n",
    "                data_1 = data_1.drop([\"value_1\"], axis = 1)\n",
    "                data_1[\"value\"] = data_1[\"value\"].fillna(0)\n",
    "                list_data.append(data_1)\n",
    "                list_dicts = []\n",
    "\n",
    "        if minute_aggregate == \"max\":\n",
    "            output_data = pd.concat(list_data, ignore_index = True)\n",
    "        else:\n",
    "            output_data = pd.DataFrame(list_dicts)\n",
    "    elif minute_aggregate in [\"max\", \"sum\", \"mean\"]:\n",
    "        list_dicts = []\n",
    "        list_data = []\n",
    "        for asset_name in pd.unique(data.asset_name):\n",
    "            data_iter = data[data.asset_name == asset_name]\n",
    "            def GetHourMinuteDate(str_):\n",
    "                import maya\n",
    "                from datetime import datetime\n",
    "\n",
    "                return datetime.strftime(maya.parse(str_).datetime(), \"%Y-%m-%d %H:%M:00\")\n",
    "            \n",
    "            data_iter[\"minute\"] = data_iter[\"time_registered\"].apply(GetHourMinuteDate)\n",
    "            data_group = (data_iter.groupby('minute')\n",
    "                .agg({'time_registered':'count', 'value': minute_aggregate})\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            df = data_group[[\"minute\", \"value\"]]\n",
    "            df = df.rename(columns={'minute':'ds', 'value': 'y'})\n",
    "\n",
    "            last_value = df.y.tolist()[0]\n",
    "\n",
    "            for ds_obj in pd.date_range(min_date, max_date):\n",
    "                for i in range(24):\n",
    "                    for j in range(60):\n",
    "                        if i < 10:\n",
    "                            str_i = \"0\"+str(i)\n",
    "                        else:\n",
    "                            str_i = str(i)\n",
    "                        \n",
    "                        if j < 10:\n",
    "                            str_j = \"0\" + str(j)\n",
    "                        else:\n",
    "                            str_j = str(j)\n",
    "                        ds_str = \"{date} {H}:{M}:00\".format(date = ds_obj.strftime(\"%Y-%m-%d\"), H = str_i, M = str_j)\n",
    "\n",
    "                        if df[df.ds == ds_str].shape[0] == 0:\n",
    "\n",
    "                            if minute_aggregate == \"max\":\n",
    "                                value_ = last_value\n",
    "                            else:\n",
    "                                value_ = 0\n",
    "\n",
    "                            dict_ = {\n",
    "                                \"time_registered\": ds_str,\n",
    "                                \"value\": value_,\n",
    "                                \"asset_name\": asset_name\n",
    "                            }\n",
    "                        else:\n",
    "                            dict_ = {\n",
    "                                \"time_registered\": ds_str,\n",
    "                                \"value\": df[df.ds == ds_str].y.tolist()[0],\n",
    "                                \"asset_name\": asset_name\n",
    "                            }\n",
    "                            if minute_aggregate == \"max\":\n",
    "                                last_value = dict_[\"value\"]\n",
    "                        list_dicts.append(dict_)\n",
    "            if minute_aggregate == \"max\":\n",
    "                data_1 = pd.DataFrame(list_dicts)\n",
    "                data_1[\"value_1\"] = data_1[\"value\"].shift(1)\n",
    "                data_1[\"value_cummulative\"] = data_1[\"value\"].copy()\n",
    "                data_1[\"value\"] = data_1[\"value\"] - data_1[\"value_1\"]\n",
    "                data_1 = data_1.drop([\"value_1\"], axis = 1)\n",
    "                data_1[\"value\"] = data_1[\"value\"].fillna(0)\n",
    "                list_data.append(data_1)\n",
    "                list_dicts = []\n",
    "\n",
    "        if minute_aggregate == \"max\":\n",
    "            output_data = pd.concat(list_data, ignore_index = True)\n",
    "        else:\n",
    "            output_data = pd.DataFrame(list_dicts)\n",
    "\n",
    "    else:\n",
    "        output_data = data\n",
    "\n",
    "    ic(output_data)\n",
    "\n",
    "    data_output = {\n",
    "        \"value\": output_data[\"value\"].tolist(),\n",
    "        \"time_registered\": output_data[\"time_registered\"].tolist(),\n",
    "        \"asset_name\": output_data[\"asset_name\"].tolist()\n",
    "    }\n",
    "\n",
    "    if minute_aggregate == \"max\":\n",
    "        data_output[\"value_cummulative\"] = output_data[\"value_cummulative\"].tolist()\n",
    "\n",
    "    with open(output_data_forecast, \"w\") as file:\n",
    "        json.dump(data_output, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForecastProcess(input_data_path: InputPath(str),input_thresholds_path : InputPath(str), measurement_name,\n",
    "    path_minio,\n",
    "    access_key,\n",
    "    secret_key,\n",
    "    mode,\n",
    "    url_pilot,\n",
    "    diff_time,\n",
    "    pilot_name,\n",
    "    send_forecast,\n",
    "    mlpipeline_metrics_path: OutputPath('Metrics')\n",
    "    ):\n",
    "\n",
    "    import maya\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import json\n",
    "    from icecream import ic\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from prophet.serialize import model_to_json\n",
    "    from minio import Minio\n",
    "    import boto3\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    from datetime import datetime\n",
    "\n",
    "    with open(input_thresholds_path) as file:\n",
    "        thresholds = json.load(file)\n",
    "\n",
    "    try:\n",
    "        client = Minio(\n",
    "            path_minio,\n",
    "            access_key=access_key,\n",
    "            secret_key=secret_key,\n",
    "            secure = False\n",
    "        )\n",
    "\n",
    "        list_objects = client.list_objects(\"test\")\n",
    "        for obj_ in list_objects:\n",
    "            ic(obj_._object_name)\n",
    "    except:\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Cannot access minio server correctly - read data.\"\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "    \n",
    "    measures_per_hour = int(60/int(diff_time))\n",
    "    time_prediction = maya.now().epoch\n",
    "\n",
    "    def GetDateInfo(ds_str, time_value):\n",
    "        maya_obj = maya.parse(ds_str)\n",
    "        if time_value == \"year\":\n",
    "            return str(maya_obj.year)\n",
    "        elif time_value == \"month\":\n",
    "            return str(maya_obj.month)\n",
    "        elif time_value == \"weekday\":\n",
    "            return str(maya_obj.weekday)\n",
    "        elif time_value == \"hour\":\n",
    "            return str(maya_obj.hour)\n",
    "        \n",
    "    def ManageData(data_ds):\n",
    "\n",
    "        data_ds[\"year\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"year\")\n",
    "        data_ds[\"month\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"month\")\n",
    "        data_ds[\"weekday\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"weekday\")\n",
    "        data_ds[\"hour\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"hour\")\n",
    "\n",
    "        for var_ in [\"year\", \"month\", \"weekday\", \"hour\"]:\n",
    "            if len(pd.unique(data_ds[var_])) <= 2:\n",
    "                bin_main = list(pd.unique(data_ds[var_]))[0]\n",
    "                data_ds[var_] = (data_ds[var_] == bin_main)\n",
    "        \n",
    "        data_ds[\"prev_val_1\"] = data_ds[\"y\"].shift(1)\n",
    "        data_ds[\"prev_val_2\"] = data_ds[\"y\"].shift(2)\n",
    "        data_ds[\"prev_val_3\"] = data_ds[\"y\"].shift(3)\n",
    "        data_ds[\"prev_val_4\"] = data_ds[\"y\"].shift(4)\n",
    "        data_ds[\"prev_val_5\"] = data_ds[\"y\"].shift(5)\n",
    "\n",
    "        return data_ds\n",
    "\n",
    "\n",
    "\n",
    "    def ForecastData(data, asset_name, measurement_name, metrics_list, measures_per_hour, diff_time, mode = \"no notifications\"):\n",
    "        data_ds = data[data.asset_name == asset_name][[\"time_registered\", \"value\"]]\n",
    "        try:\n",
    "            last_cummulative_value = data[data.asset_name == asset_name][\"value_cummulative\"].tolist()[-1]\n",
    "        except:\n",
    "            last_cummulative_value = 0\n",
    "        data_ds.columns = [\"ds\", \"y\"]\n",
    "        max_date = max(data_ds.ds)\n",
    "        prev_value = data_ds\n",
    "        ic(data_ds.shape[0])\n",
    "\n",
    "        if len(pd.unique(data_ds.y)) >= 10:\n",
    "\n",
    "            n_train = int(data_ds.shape[0] - 24 * 2 * measures_per_hour)\n",
    "            train_data = data_ds[0:n_train]\n",
    "            test_data = data_ds[n_train:]\n",
    "\n",
    "            from prophet import Prophet\n",
    "\n",
    "            m = Prophet(daily_seasonality=True, weekly_seasonality=True, changepoint_prior_scale = 0.05)\n",
    "            m.fit(train_data)\n",
    "            future = m.make_future_dataframe(periods= 24*3*measures_per_hour , freq=\"{minutes}T\".format(minutes = diff_time))\n",
    "            forecast = m.predict(future)\n",
    "\n",
    "            forecast_test = forecast[\"yhat\"].tolist()[-24*measures_per_hour*3:-24*measures_per_hour]\n",
    "            try:\n",
    "                forecast_train = forecast[\"yhat\"].tolist()[:-24*4*3]\n",
    "                real_vals_train = train_data[\"y\"].tolist()\n",
    "                r2_score_train = r2_score(real_vals_train, forecast_train)\n",
    "\n",
    "            except:\n",
    "                r2_score_train = 0\n",
    "            \n",
    "            try:\n",
    "                asset_number = dict_asset[asset_name]\n",
    "            except:\n",
    "                asset_number = 3\n",
    "            \n",
    "            real_vals_test = test_data[\"y\"].tolist()\n",
    "\n",
    "            if len(forecast_test) == len(real_vals_test):\n",
    "                r2_score_test = r2_score(real_vals_test, forecast_test)\n",
    "                metrics = {\n",
    "                    'metrics': [\n",
    "                        {\n",
    "                        'name': 'r2_score_test',\n",
    "                        'numberValue':  float(r2_score_test),\n",
    "                        'format': \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            'name': 'r2_score_train',\n",
    "                            \"numberValue\": float(r2_score_train),\n",
    "                            \"format\": \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"asset_number\",\n",
    "                            \"numberValue\": asset_name,\n",
    "                            \"format\": \"RAW\"\n",
    "                        }\n",
    "                    ]}  \n",
    "                \n",
    "                metrics_list.append(metrics)\n",
    "            else:\n",
    "                ic(len(forecast_test))\n",
    "                ic(len(real_vals_test))\n",
    "\n",
    "            with open(\"/tmp/model_prophet.json\", 'w') as fout:\n",
    "                fout.write(model_to_json(m))  # Save model\n",
    "            \n",
    "\n",
    "            date = maya.when(\"now\").rfc2822()\n",
    "            f_name = \"model_{domain}_{asset}.json\"\\\n",
    "                .format(domain = measurement_name, asset = asset_name)\n",
    "            try:\n",
    "                result = client.fput_object(\n",
    "                    \"test\", f_name, \"/tmp/model_prophet.json\"\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    \"created {0} object; etag: {1}, version-id: {2}\".format(\n",
    "                        result.object_name, result.etag, result.version_id,\n",
    "                    ),\n",
    "                )\n",
    "            except:\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model not saved for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "                s3 = boto3.resource(\n",
    "                    service_name='s3',\n",
    "                    aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                    aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                    endpoint_url='https://s3.tebi.io'\n",
    "                )\n",
    "\n",
    "                for bucket in s3.buckets.all():\n",
    "                    ic(bucket.name)\n",
    "                \n",
    "                # Upload a new file\n",
    "                data = open('/tmp/model_prophet.json', 'rb')\n",
    "                f_name = \"model_{pilot}_{domain}_{asset}.json\"\\\n",
    "                .format(pilot = pilot_name,domain = measurement_name, asset = asset_name)\n",
    "                s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model sent to tebi for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "        else:\n",
    "            from catboost import CatBoostClassifier, Pool\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "            data_ds = ManageData(data_ds)\n",
    "\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(data_ds.drop([\"y\", \"ds\"], axis = 1), data_ds.y, test_size = 0.2)\n",
    "\n",
    "            train_pool = Pool(\n",
    "                data = X_train, label = Y_train, \n",
    "                cat_features = [\"year\", \"month\", \"weekday\", \"hour\", \"prev_val_1\", \n",
    "                                \"prev_val_2\",\"prev_val_3\",\"prev_val_4\",\"prev_val_5\"]\n",
    "                )\n",
    "            test_pool = Pool(\n",
    "                data = X_test, label = Y_test, \n",
    "                cat_features = [\"year\", \"month\", \"weekday\", \"hour\", \"prev_val_1\", \n",
    "                                \"prev_val_2\",\"prev_val_3\",\"prev_val_4\",\"prev_val_5\"]\n",
    "                )\n",
    "            \n",
    "            catboost_model = CatBoostClassifier(\n",
    "                iterarions = 20,\n",
    "                learning_rate = 0.5,\n",
    "                depth = 12\n",
    "            )\n",
    "\n",
    "            catboost_model.fit(train_pool)\n",
    "\n",
    "            yhat_test = catboost_model.predict(test_pool)\n",
    "            yhat_train = catboost_model.predict(train_pool)\n",
    "\n",
    "            accuracy_score_train = accuracy_score(Y_test, yhat_train)\n",
    "            accuracy_score_test = accuracy_score(Y_test, yhat_test)\n",
    "\n",
    "            data_ds[\"yhat\"] = last_cummulative_value\n",
    "\n",
    "            for i in range(24):\n",
    "                for j in range(measures_per_hour):\n",
    "                    last_row = data_ds.iloc[-1]\n",
    "                    ds_obj = maya.parse(last_row[\"ds\"]).add(minutes = diff_time)\n",
    "                    dict_input = [{\n",
    "                        \"year\": ds_obj.year,\n",
    "                        \"month\": ds_obj.month,\n",
    "                        \"weekday\": ds_obj.weekday,\n",
    "                        \"hour\": ds_obj.hour,\n",
    "                        \"prev_val_1\": last_row[\"y\"],\n",
    "                        \"prev_val_2\": last_row[\"prev_val_1\"],\n",
    "                        \"prev_val_3\": last_row[\"prev_val_2\"],\n",
    "                        \"prev_val_4\": last_row[\"prev_val_3\"],\n",
    "                        \"prev_val_5\": last_row[\"prev_val_4\"]\n",
    "                    }]\n",
    "                    data_input = pd.DataFrame(dict_input)\n",
    "                    pred_pool = Pool(\n",
    "                        data = data_input, \n",
    "                        cat_features = [\"year\", \"month\", \"weekday\", \"hour\", \"prev_val_1\", \n",
    "                                        \"prev_val_2\",\"prev_val_3\",\"prev_val_4\",\"prev_val_5\"]\n",
    "                        )\n",
    "                    pred_value = catboost_model.predict(pred_pool)[0]\n",
    "                    dict_input[0][\"y\"] = pred_value\n",
    "                    dict_input[0][\"ds\"] = datetime.strftime(ds_obj, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    dict_input[0][\"yhat\"] = last_row[\"yhat\"] + float(prev_value)\n",
    "                    data_add = pd.DataFrame(dict_input)\n",
    "                    data_ds = pd.concat([data_ds, data_add],ignore_index = True)\n",
    "            \n",
    "            forecast = data_ds\n",
    "\n",
    "            metrics = {\n",
    "                'metrics': [\n",
    "                    {\n",
    "                    'name': 'accuracy_train',\n",
    "                    'numberValue':  float(accuracy_score_train),\n",
    "                    'format': \"PERCENTAGE\"\n",
    "                    },\n",
    "                    {\n",
    "                        'name': 'accuracy_test',\n",
    "                        \"numberValue\": float(accuracy_score_test),\n",
    "                        \"format\": \"PERCENTAGE\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"asset_number\",\n",
    "                        \"numberValue\": asset_name,\n",
    "                        \"format\": \"RAW\"\n",
    "                    }\n",
    "                ]}  \n",
    "            \n",
    "            metrics_list.append(metrics)\n",
    "\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Model accuracy {asset_name}: train -> {accuracy_train}, test -> {accuracy_test}\".format(asset_name = asset_name, accuracy_train = accuracy_score_train, accuracy_test = accuracy_score_test)\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        return forecast[forecast.ds > max_date], metrics_list\n",
    "\n",
    "\n",
    "    if measurement_name == \"electricity_meter\":\n",
    "        domain_ = \"electricity\"\n",
    "    elif measurement_name == \"heat_meter\":\n",
    "        domain_ = \"heat\"\n",
    "    else:\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"There is a problem with the measurement_name, {measurement_name}\".format(measurement_name = measurement_name)\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "        domain_ = \"electricity\"\n",
    "        # raise ValueError\n",
    "\n",
    "    asset_list_dict = {\n",
    "        \"electricity_meter\": [\"building1\", \"building2\", \"pv_panel_1\", \"wind_farm_1\"],\n",
    "        \"heat_meter\": [\"building1\", \"building2\", \"solar_collector1\"],\n",
    "        \"energy_meter\": [\"psnc\", \"office\"],\n",
    "        \"pv\": [\"office\", \"de-nieuwe-dokken-pv-0198-xxxxx853\",\"de-nieuwe-dokken-pv-0198-xxxxx4B4\",\"de-nieuwe-dokken-pv-0198-xxxxx9C0\", \n",
    "        \"de-nieuwe-dokken-pv-017A-xxxxx9A1\" ]\n",
    "    }\n",
    "\n",
    "\n",
    "    asset_list = asset_list_dict[measurement_name]\n",
    "\n",
    "    with open(input_data_path) as file:\n",
    "        data_str = json.load(file)\n",
    "    \n",
    "    data = pd.DataFrame(data_str)\n",
    "    metrics_list = []\n",
    "    for asset_name in asset_list:\n",
    "\n",
    "        ic(asset_name)\n",
    "        # get notification code for anomaly high and low\n",
    "\n",
    "        url_notification_definition = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification/definition\"\n",
    "        payload={}\n",
    "        headers = {}\n",
    "\n",
    "        response = requests.request(\"GET\", url_notification_definition, headers=headers, data=payload)\n",
    "        try:\n",
    "            dict_notifications = response.json()\n",
    "        except:\n",
    "            dict_notifications = []\n",
    "\n",
    "        for notif in dict_notifications:\n",
    "            if notif[\"message\"] == \"message.anomaly.high\":\n",
    "                code_high = notif[\"code\"]\n",
    "            elif notif[\"message\"] == \"message.anomaly.low\":\n",
    "                code_low = notif[\"code\"]\n",
    "        \n",
    "        # get asset_id for asset_name\n",
    "\n",
    "        url_asset_name = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/assets?name={asset_name}\".format(asset_name = asset_name)\n",
    "        try:\n",
    "            response = requests.request(\"GET\", url_asset_name, headers=headers, data=payload)\n",
    "            dict_asset = response.json()[0]\n",
    "            id_asset = dict_asset[\"id\"]\n",
    "        except:\n",
    "            id_asset = -1\n",
    "\n",
    "        id_dashboard = 1\n",
    "\n",
    "        try:\n",
    "            threshold_min = thresholds[measurement_name][asset_name][0]\n",
    "        except:\n",
    "            threshold_min = 0\n",
    "        \n",
    "        try:\n",
    "            threshold_max = thresholds[measurement_name][asset_name][1]\n",
    "        except:\n",
    "            threshold_max = 10000\n",
    "\n",
    "        try:\n",
    "            forecasted_data, metrics_list = ForecastData(data, asset_name, measurement_name, metrics_list, measures_per_hour, diff_time)\n",
    "            max_ds = forecasted_data[\"ds\"].tolist()[-1]\n",
    "        except:\n",
    "            forecasted_dict = {\n",
    "                \"ds\": [],\n",
    "                \"yhat\": []\n",
    "            }\n",
    "            forecasted_data = pd.DataFrame(forecasted_dict)\n",
    "            metrics_list = []\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            forecasted_data.to_csv('/tmp/forecast_test_{asset_name}.csv'.format(asset_name = asset_name), index = False)\n",
    "            data_to_send = open('/tmp/forecast_test_{asset_name}.csv'.format(asset_name = asset_name), 'rb')\n",
    "            f_name = \"forecast_test_{pilot}_{asset_name}.csv\".format(pilot = pilot_name, asset_name = asset_name)\n",
    "            s3 = boto3.resource(\n",
    "                service_name='s3',\n",
    "                aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                endpoint_url='https://s3.tebi.io'\n",
    "            )\n",
    "            s3.Bucket('test-pf').put_object(Key=f_name, Body=data_to_send)\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Data File: {f_name} Saved to Tebi\".format(f_name = f_name)\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "        except:\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Unable to save data to tebi\"\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "\n",
    "            message = \"Values for {asset_name}: {list_values}\".format(asset_name = asset_name,list_values = forecasted_data.yhat.tolist())\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "        for index, row in tqdm(forecasted_data.iterrows(), total = forecasted_data.shape[0]):\n",
    "            time_ = str(row[\"ds\"])\n",
    "            time_epoch = maya.parse(time_).epoch\n",
    "            value = row[\"yhat\"]\n",
    "\n",
    "            if asset_name in [\"building1\", \"building2\", \"psnc\", \"office\"]:\n",
    "                direction_energy = \"in\"\n",
    "                type_ = \"None\"\n",
    "            elif measurement_name == \"pv\":\n",
    "                direction_energy = \"out\"\n",
    "                type_ = \"renewable\"\n",
    "            elif measurement_name == \"office\":\n",
    "                direction_energy = \"in\"\n",
    "                type_ = \"None\"\n",
    "            else:\n",
    "                direction_energy = \"out\"\n",
    "                type_ = \"renewable\"\n",
    "            data_post = {\n",
    "                    \"bucket\": \"renergetic\",\n",
    "                    \"measurement\": measurement_name,\n",
    "                    \"fields\":{\n",
    "                        \"energy\": value,\n",
    "                        \"time\": time_,\n",
    "                    },\n",
    "                    \"tags\":{\n",
    "                        \"domain\": domain_,\n",
    "                        \"typeData\": \"forecasting\",\n",
    "                        \"direction\": direction_energy,\n",
    "                        \"prediction_window\": \"24h\",\n",
    "                        \"asset_name\": asset_name,\n",
    "                        \"measurement_type\": type_,\n",
    "                        \"time_prediction\": time_prediction\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            # SEND NOTIFICATION\n",
    "            if mode == \"none\":\n",
    "                if value < threshold_min:\n",
    "                    date_from = maya.parse(time_).epoch\n",
    "                    mode = \"lower\"\n",
    "\n",
    "                    if time_ == max_ds:\n",
    "                        date_to = maya.parse(time_).add(minutes = 15).epoch\n",
    "                        dict_post = {\n",
    "                            \"notification_code\": code_low,\n",
    "                            \"date_from\": date_from,\n",
    "                            \"date_to\": date_to,\n",
    "                            \"asset_id\": id_asset,\n",
    "                            \"dashboard_id\": id_dashboard\n",
    "                        }\n",
    "                        url = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "                        headers = {\n",
    "                            \"Content-Type\": \"application/json\"\n",
    "                        }\n",
    "                        response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "                        status_code = response.status_code\n",
    "\n",
    "                        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                        message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "                            format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "                        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                        webhook.execute()\n",
    "                \n",
    "                if value > threshold_max:\n",
    "                    date_from = maya.parse(time_).epoch\n",
    "                    mode = \"upper\"\n",
    "\n",
    "                    if time_ == max_ds:\n",
    "                        date_to = maya.parse(time_).add(minutes = 15).epoch\n",
    "                        dict_post = {\n",
    "                            \"notification_code\": code_low,\n",
    "                            \"date_from\": date_from,\n",
    "                            \"date_to\": date_to,\n",
    "                            \"asset_id\": id_asset,\n",
    "                            \"dashboard_id\": id_dashboard\n",
    "                        }\n",
    "                        url = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "                        headers = {\n",
    "                            \"Content-Type\": \"application/json\"\n",
    "                        }\n",
    "                        response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "                        status_code = response.status_code\n",
    "\n",
    "                        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                        message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "                            format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "                        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                        webhook.execute()\n",
    "                \n",
    "            elif mode == \"lower\":\n",
    "                if value > threshold_min or time_ == max_ds:\n",
    "                    date_to = maya.parse(time_).epoch\n",
    "                    dict_post = {\n",
    "                        \"notification_code\": code_low,\n",
    "                        \"date_from\": date_from,\n",
    "                        \"date_to\": date_to,\n",
    "                        \"asset_id\": id_asset,\n",
    "                        \"dashboard_id\": id_dashboard\n",
    "                    }\n",
    "\n",
    "                    url = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "                    headers = {\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    }\n",
    "                    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "                    status_code = response.status_code\n",
    "\n",
    "                    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                    message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "                        format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "                    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                    webhook.execute()\n",
    "\n",
    "                    if value > threshold_max:\n",
    "                        mode = \"upper\"\n",
    "                        date_from = maya.parse(time_).epoch\n",
    "                    else:\n",
    "                        mode = \"none\"\n",
    "            \n",
    "            elif (mode == \"upper\" or time_ == max_ds) and (mode != \"no notification\"):\n",
    "                if value < threshold_max:\n",
    "                    date_to = maya.parse(time_).epoch\n",
    "                    dict_post = {\n",
    "                        \"notification_code\": code_high,\n",
    "                        \"date_from\": date_from*1000,\n",
    "                        \"date_to\": date_to*1000,\n",
    "                        \"asset_id\": id_asset,\n",
    "                        \"dashboard_id\": id_dashboard\n",
    "                    }\n",
    "\n",
    "                    if value < threshold_min:\n",
    "                        date_from = maya.parse(time_).epoch\n",
    "                        mode = \"lower\"\n",
    "                    else:\n",
    "                        mode = \"none\"\n",
    "\n",
    "                    url = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "                    headers = {\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    }\n",
    "                    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "                    status_code = response.status_code\n",
    "\n",
    "                    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                    message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "                        format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "                    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                    webhook.execute()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "            # url = \"http://influx-api-ren-prototype.apps.paas-dev.psnc.pl/api/measurement\"\n",
    "            url = url_pilot\n",
    "\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                if send_forecast == \"yes\":\n",
    "                    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(data_post))\n",
    "                    status_code = response.status_code\n",
    "                else:\n",
    "                    status_code = 200\n",
    "            except:\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Error in updating value for measurement name: {measurement_name} in asset: {asset_name} in time {time_pred}\"\\\n",
    "                    .format(measurement_name = \"electricity_meter\", asset_name = asset_name, time_pred = data_post[\"fields\"][\"time\"])\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "                status_code = 200\n",
    "            \n",
    "            if status_code > 299:\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Error in sending the value for measurement name: {measurement_name} in asset: {asset_name} in time {time_pred}\"\\\n",
    "                    .format(measurement_name = measurement_name, asset_name = asset_name, time_pred = data_post[\"fields\"][\"time\"])\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "    with open(\"/tmp/metrics_{domain}.json\".format(domain = domain_), \"w\") as file:\n",
    "        json.dump(metrics_list, file)\n",
    "    \n",
    "    \n",
    "\n",
    "    s3 = boto3.resource(\n",
    "                service_name='s3',\n",
    "                aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                endpoint_url='https://s3.tebi.io'\n",
    "            )\n",
    "    data = open(\"/tmp/metrics_{domain}.json\".format(domain = domain_), 'rb')\n",
    "    f_name = \"metrics_{domain}_latest.json\"\\\n",
    "    .format(domain = measurement_name)\n",
    "    s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "    \n",
    "\n",
    "\n",
    "    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "    message = \"Forecasting done for {domain}\".format(domain = domain_)\n",
    "    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "    webhook.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportModelToMinio(input_model_path: InputPath(str),measurement_name, \n",
    "    path_minio = \"minio.kubeflow-renergetic.svc:9000\",\n",
    "    access_key = \"minio\",\n",
    "    secret_key = \"DaTkKc45Hxr1YLR4LxR2xJP2\"\n",
    "    ):\n",
    "\n",
    "    from minio import Minio\n",
    "    import json\n",
    "    with open(input_model_path) as file:\n",
    "        model_serialiazed = json.load()\n",
    "    client = Minio(\n",
    "        path_minio,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fablab/opt/anaconda3/lib/python3.8/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"JsonArray\" based on the value \"[]\".\n",
      "  warnings.warn(\n",
      "/Users/fablab/opt/anaconda3/lib/python3.8/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"JsonArray\" based on the value \"['electricity_meter', 'heat_meter']\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ProcessData() missing 1 required positional argument: 'max_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-e1756468bff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREN_Forecast_Test_Pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Forecast_Data_Pipeline.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m             \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_CHECK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             self._create_and_write_workflow(\n\u001b[0m\u001b[1;32m   1176\u001b[0m                 \u001b[0mpipeline_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0mpipeline_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_and_write_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \"\"\"Compile the given pipeline function and dump it to specified file\n\u001b[1;32m   1226\u001b[0m         format.\"\"\"\n\u001b[0;32m-> 1227\u001b[0;31m         workflow = self._create_workflow(pipeline_func, pipeline_name,\n\u001b[0m\u001b[1;32m   1228\u001b[0m                                          \u001b[0mpipeline_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                                          pipeline_conf)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_create_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m             \u001b[0mpipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mpipeline_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_conf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdsl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m  \u001b[0;31m# Configuration passed to the compiler is overriding. Unfortunately, it's not trivial to detect whether the dsl_pipeline.conf was ever modified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-e1756468bff8>\u001b[0m in \u001b[0;36mREN_Forecast_Test_Pipeline\u001b[0;34m(url_pilot, diff_time, filter_vars, filter_case, url, access_key, secret_key, min_date, max_date, mode, list_measurements, key_measurement, type_measurement, pilot_name, hourly_aggregate, send_forecast)\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mset_cpu_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         .set_cpu_limit('2'))\n\u001b[0;32m---> 39\u001b[0;31m         process_task = (process_data_op(download_task.output, \n\u001b[0m\u001b[1;32m     40\u001b[0m                         \u001b[0mhourly_aggregate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                         \u001b[0mmin_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ProcessData() missing 1 required positional argument: 'max_date'"
     ]
    }
   ],
   "source": [
    "def REN_Forecast_Test_Pipeline(url_pilot,\n",
    "    diff_time:int,\n",
    "    filter_vars = [],\n",
    "    filter_case = [],\n",
    "    url = \"minio-kubeflow-renergetic.apps.dcw1-test.paas.psnc.pl\",\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"DaTkKc45Hxr1YLR4LxR2xJP2\",\n",
    "    min_date = \"yesterday\",\n",
    "    max_date = \"today\",\n",
    "    mode = \"no notifications\",\n",
    "    list_measurements = [\"electricity_meter\", \"heat_meter\"],\n",
    "    key_measurement = \"energy\",\n",
    "    type_measurement = \"simulation\",\n",
    "    pilot_name = \"Virtual\",\n",
    "    hourly_aggregate = \"no\",\n",
    "    minute_aggregate = \"no\",\n",
    "    send_forecast = \"yes\"\n",
    "    ):\n",
    "\n",
    "    env_var = V1EnvVar(name='HOME', value='/tmp')\n",
    "    download_data_op = comp.create_component_from_func(\n",
    "        GetData, packages_to_install = [\"requests\", \"numpy\", \"maya\",\"pandas\", \"icecream\", \"tqdm\", \"discord-webhook\", \"retry\"], output_component_file = \"download_data_op_component.yaml\")\n",
    "    get_thresholds_op = comp.create_component_from_func(\n",
    "        GetThresholds, packages_to_install= [\"requests\"], output_component_file= \"thresholds_component.yaml\"\n",
    "    )\n",
    "    process_data_op = comp.create_component_from_func(\n",
    "        ProcessData, packages_to_install= [\"maya\", \"pandas\", \"icecream\"], output_component_file= \"process_data_op_component.yaml\"\n",
    "    )\n",
    "    forecast_data_op = comp.create_component_from_func(\n",
    "        ForecastProcess, packages_to_install = [\"requests\", \"numpy\", \"maya\",\"pandas\", \"icecream\", \"prophet\", \"discord-webhook\", \"tqdm\", \"minio\", \"boto3\", \"scikit-learn\"], output_component_file = \"forecast_data_op_component.yaml\")\n",
    "    \n",
    "    get_thresholds_task = get_thresholds_op()\n",
    "    \n",
    "    with dsl.ParallelFor(list_measurements) as measurement:\n",
    "        download_task = (download_data_op(measurement, min_date, max_date, url_pilot,type_measurement, key_measurement, filter_vars, filter_case).add_env_variable(env_var)\n",
    "                        .set_memory_request('500M')\n",
    "                        .set_memory_limit('1Gi')\n",
    "                        .set_cpu_request('1')\n",
    "                        .set_cpu_limit('2'))\n",
    "        process_task = (process_data_op(download_task.output, \n",
    "                        hourly_aggregate,\n",
    "                        minute_aggregate,\n",
    "                        min_date, \n",
    "                        max_date))\n",
    "        if hourly_aggregate in [\"sum\", \"mean\"]:\n",
    "            diff_time = 60\n",
    "        forecast_task = (forecast_data_op(process_task.output, \n",
    "        get_thresholds_task.output,\n",
    "        measurement, \n",
    "        url, \n",
    "        access_key, \n",
    "        secret_key, \n",
    "        mode,\n",
    "        url_pilot,\n",
    "        diff_time,\n",
    "        pilot_name,\n",
    "        send_forecast).add_env_variable(env_var))\n",
    "                       \n",
    "    \n",
    "compiler.Compiler().compile(pipeline_func = REN_Forecast_Test_Pipeline, package_path =\"Forecast_Data_Pipeline.yaml\")\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02e15b1dc1a9053df8aede2000f19bd423bcf01320081b3a1e044a45ff452adb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
