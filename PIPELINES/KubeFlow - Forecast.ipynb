{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "import kfp.components as comp\n",
    "from kfp import compiler, dsl\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kubernetes.client.models import V1EnvVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMetrics_Func(name_measurement, output_timeseries_path: OutputPath(str), metrics_pipeline_path: OutputPath(\"Metrics\")):\n",
    "\n",
    "    # Task 1: Get Measurement Data.\n",
    "\n",
    "    # Import Libraries\n",
    "    import requests # To Requirements\n",
    "    import json\n",
    "    import pandas as pd # To Requirements\n",
    "    import maya # To Requirements\n",
    "    from tqdm import tqdm # To Requirements\n",
    "    \n",
    "    # Function 1: Get Request\n",
    "    def GetRequest(url, headers ={}, payload = {}):\n",
    "        response = requests.request(\"GET\", url, headers = headers, data = payload)\n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            dict_ = {\n",
    "                \"status_code\": response.status_code,\n",
    "                \"text\": response.text\n",
    "            }\n",
    "            return dict_\n",
    "    \n",
    "    # Function 2: Download Assets Data\n",
    "\n",
    "    def DownloadAssetsData(measurement_name, bucket = \"renergetic\", min_date = \"yesterday\", max_date = \"tomorrow\"):\n",
    "    \n",
    "        from datetime import datetime\n",
    "        \n",
    "\n",
    "        try:\n",
    "            min_date_from = maya.when(min_date).datetime()\n",
    "            print(\"Min Date\")\n",
    "            print(maya.when(min_date).iso8601())\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MIN_DATE\")\n",
    "        \n",
    "        try: \n",
    "            max_date_from = maya.when(max_date).datetime()\n",
    "            print(\"Max Date\")\n",
    "            print(maya.when(max_date).iso8601())\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MAX_DATE\")\n",
    "        \n",
    "        datelist = pd.date_range(min_date_from, max_date_from)\n",
    "        print(list(datelist)[-1])\n",
    "        data_ = []\n",
    "\n",
    "        for i in tqdm(range(len(datelist)-1)):\n",
    "            from_obj = datelist[i]\n",
    "            to_obj = datelist[i+1]\n",
    "            from_ = datetime.strftime(from_obj, \"%Y-%m-%d 00:00:00\")\n",
    "            if i != (len(datelist)-1):\n",
    "                to_ = datetime.strftime(to_obj, \"%Y-%m-%d 00:00:00\")\n",
    "            else:\n",
    "                to_obj = maya.now().add_minutes(15).to_datetime()\n",
    "                to_ = datetime.strftime(to_obj, \"%Y-%m-%d 00:00:00\")\n",
    "            url = \"http://influx-api-ren-prototype.apps.paas-dev.psnc.pl/api/measurement/data?measurements={measurement_name}&from={from_}&to={to_}\"\\\n",
    "                .format(measurement_name = measurement_name, from_ = from_, to_= to_)\n",
    "            message = \"Getting Data From {from_} to {to_}\"\\\n",
    "                .format(from_ = from_, to_ = to_)\n",
    "            print(message)\n",
    "            info_ = GetRequest(url)\n",
    "\n",
    "            if type(info_) == list:\n",
    "                data_ = data_ + info_\n",
    "            elif type(info_) == dict:\n",
    "                print(\"Error\")\n",
    "                print(from_)\n",
    "                print(to_)\n",
    "        return data_\n",
    "    \n",
    "    # Function 3: Transform List To DataFrame:\n",
    "\n",
    "    def DataFrameAssests(list_data, name_field):\n",
    "        dicts = []\n",
    "        for data in list_data:\n",
    "            try:\n",
    "                dict_ = {\n",
    "                    \"ds\": data[\"tags\"][\"time_prediction\"],\n",
    "                    \"asset_name\": data[\"tags\"][\"asset_name\"],\n",
    "                    \"direction\": data[\"tags\"][\"direction\"],\n",
    "                    \"value\": float(data[\"fields\"][name_field])\n",
    "                }\n",
    "                dicts.append(dict_)\n",
    "            except:\n",
    "                continue\n",
    "        return pd.DataFrame(dicts)\n",
    "\n",
    "    ####### EXECUTION OF CODE ############\n",
    "\n",
    "    list_energy = DownloadAssetsData(name_measurement, min_date = \"6 Oct 2022 13:15:00\")\n",
    "\n",
    "    measurements_units = {\n",
    "        \"heat_meter\": \"energy\",\n",
    "        \"renewability\": \"percentage\"\n",
    "    }\n",
    "\n",
    "    data_energy = DataFrameAssests(list_energy, measurements_units[name_measurement])\n",
    "    print(\"______________________________\")\n",
    "    print(\"Maximum Date Of Data Frame\")\n",
    "    print(maya.MayaDT(max(data_energy.ds.apply(int))).iso8601())\n",
    "\n",
    "    dict_output = {}\n",
    "    \n",
    "    for column_name in data_energy.columns.values:\n",
    "        dict_output[column_name] = data_energy[column_name].tolist()\n",
    "    \n",
    "    with open(output_timeseries_path, \"w\") as out_file:\n",
    "        json.dump(dict_output, out_file)\n",
    "    \n",
    "    try:\n",
    "        numb_assets = len(list(pd.unique(data_energy[\"asset_name\"])))\n",
    "    except:\n",
    "        numb_assets = 5\n",
    "    numb_metrics = len(list_energy)/numb_assets\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\":[{\n",
    "            \"name\": \"Number Assets\",\n",
    "            \"numberValue\": numb_assets\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Number Metrics\",\n",
    "            \"numberValue\": numb_metrics\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    with open(metrics_pipeline_path, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    return numb_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModelSMS(name_asset, input_metrics_path : InputPath(str), output_predictions_path : OutputPath(str), \n",
    "                    ml_metrics_path : OutputPath(\"Metrics\"),split_train = 0.8, freq = 8):\n",
    "\n",
    "    # Task 2 (Option A): Train Model and share predictions (only smoothing)\n",
    "    \n",
    "    # Import Libraries\n",
    "    import json\n",
    "    import pandas as pd # To Requirements\n",
    "    from statsmodels.tsa.arima.model import ARIMA # To Requirements\n",
    "    from statsmodels.tsa.holtwinters import ExponentialSmoothing # To Requirements\n",
    "    from sklearn.metrics import mean_absolute_error # To Requirements\n",
    "    import maya\n",
    "\n",
    "    with open(input_metrics_path) as data_file:\n",
    "        data_str = json.load(data_file)\n",
    "    \n",
    "    print(name_asset)\n",
    "\n",
    "    freq = int(freq)\n",
    "\n",
    "    ts_data = pd.DataFrame(data_str)\n",
    "    ts_data = ts_data[(ts_data.asset_name == name_asset)][[\"value\", \"ds\"]]\n",
    "    try:\n",
    "        st = float(split_train)\n",
    "        n_row = int(ts_data.shape[0])\n",
    "        print(\"Details\")\n",
    "        print(st)\n",
    "        print(n_row)\n",
    "        partition = int(st * n_row)\n",
    "    except:\n",
    "        partition = 200\n",
    "        print(\"Error\")\n",
    "        print(split_train)\n",
    "        print(ts_data.shape[0])\n",
    "        \n",
    "\n",
    "    ts = ts_data[\"value\"].tolist()[0:partition]\n",
    "\n",
    "    # ARIMA Model\n",
    "    model = ARIMA(ts, order=(5,1,0))\n",
    "    model_arima = model.fit()\n",
    "\n",
    "    #Exponentional Smoothing Model\n",
    "    model = ExponentialSmoothing(ts)\n",
    "    model_expon = model.fit()\n",
    "\n",
    "    ts_test = ts_data[\"value\"].tolist()[partition:]\n",
    "\n",
    "    pred_arima = model_arima.predict(partition, ts_data.shape[0]-1)\n",
    "    pred_exp = model_expon.predict(partition,ts_data.shape[0]-1)\n",
    "    \n",
    "    mae_train_arima = mean_absolute_error(ts, model_arima.predict())\n",
    "    mae_train_exp = mean_absolute_error(ts, model_arima.predict())\n",
    "    mae_arima = mean_absolute_error(ts_test, pred_arima)\n",
    "    mae_exp = mean_absolute_error(ts_test, pred_exp)\n",
    "\n",
    "    if mae_arima < mae_exp:\n",
    "        print(\"ARIMA model used for \" + name_asset + \" predictions\")\n",
    "        print(str(mae_train_arima))\n",
    "        print(str(mae_arima))\n",
    "        model = \"ARIMA\"\n",
    "        mae = mae_arima\n",
    "        predictions = model_arima.predict(ts_data.shape[0], ts_data.shape[0]+freq)\n",
    "    else:\n",
    "        print(\"Exponential model used for \" + name_asset + \" predictions\")\n",
    "        print(str(mae_train_exp))\n",
    "        print(str(mae_exp))\n",
    "        model = \"Exponential Smoothing\"\n",
    "        mae = mae_exp\n",
    "        predictions = model_expon.predict(ts_data.shape[0], ts_data.shape[0]+freq)\n",
    "\n",
    "    \n",
    "    last_date = int(ts_data[\"ds\"].tolist()[-1])\n",
    "    list_dates = []\n",
    "\n",
    "    for i in range(freq):\n",
    "        last_date = maya.MayaDT(last_date).add(minutes = 15).epoch\n",
    "        list_dates.append(last_date)\n",
    "\n",
    "\n",
    "    dict_output = {\n",
    "        \"ds\": list_dates,\n",
    "        \"preds\": list(predictions)\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [{\n",
    "            \"mae\": mae\n",
    "        }]\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(output_predictions_path, \"w\") as out_file:\n",
    "        json.dump(dict_output, out_file)\n",
    "    \n",
    "    with open(ml_metrics_path, \"w\") as out_file:\n",
    "        json.dump(metrics, out_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModelB(name_asset, input_metrics_path : InputPath(str), output_predictions_path : OutputPath(str), \n",
    "                    ml_metrics_path : OutputPath(\"Metrics\"),split_train = 0.8, freq = 8):\n",
    "\n",
    "    # Task 2 (Option B): Train Model and share predictions (smoothing + AutoKeras)\n",
    "    \n",
    "    # Import Libraries\n",
    "    import json\n",
    "    import pandas as pd # To Requirements\n",
    "    from statsmodels.tsa.arima.model import ARIMA # To Requirements\n",
    "    from statsmodels.tsa.holtwinters import ExponentialSmoothing # To Requirements\n",
    "    from sklearn.metrics import mean_absolute_error # To Requirements\n",
    "    import maya # To Requirements\n",
    "    import autokeras as ak # To Requirements\n",
    "    from tqdm import tqdm # To Requirements\n",
    "    import numpy as np # To Requirements\n",
    "    from discord_webhook import DiscordWebhook\n",
    "\n",
    "    with open(input_metrics_path) as data_file:\n",
    "        data_str = json.load(data_file)\n",
    "    \n",
    "    print(name_asset)\n",
    "\n",
    "    freq = int(freq)\n",
    "\n",
    "    ts_data = pd.DataFrame(data_str)\n",
    "    ts_data = ts_data[(ts_data.asset_name == name_asset)][[\"value\", \"ds\"]]\n",
    "\n",
    "    print(name_asset)\n",
    "    last_date = max(ts_data[\"ds\"].apply(int))\n",
    "    print(maya.MayaDT(last_date).iso8601())\n",
    "    try:\n",
    "        st = float(split_train)\n",
    "        n_row = int(ts_data.shape[0])\n",
    "        print(\"Details\")\n",
    "        print(st)\n",
    "        print(n_row)\n",
    "        partition = int(st * n_row)\n",
    "    except:\n",
    "        partition = 200\n",
    "        print(\"Error\")\n",
    "        print(split_train)\n",
    "        print(ts_data.shape[0])\n",
    "        \n",
    "\n",
    "    ts = ts_data[\"value\"].tolist()[0:partition]\n",
    "\n",
    "    # ARIMA Model\n",
    "    model = ARIMA(ts, order=(5,1,0))\n",
    "    model_arima = model.fit()\n",
    "\n",
    "    #Exponentional Smoothing Model\n",
    "    model = ExponentialSmoothing(ts)\n",
    "    model_expon = model.fit()\n",
    "\n",
    "    ts_test = ts_data[\"value\"].tolist()[partition:]\n",
    "\n",
    "    pred_arima = model_arima.predict(partition, ts_data.shape[0]-1)\n",
    "    pred_exp = model_expon.predict(partition,ts_data.shape[0]-1)\n",
    "    \n",
    "    mae_train_arima = mean_absolute_error(ts, model_arima.predict())\n",
    "    mae_train_exp = mean_absolute_error(ts, model_arima.predict())\n",
    "    mae_arima = mean_absolute_error(ts_test, pred_arima)\n",
    "    mae_exp = mean_absolute_error(ts_test, pred_exp)\n",
    "\n",
    "    # Process and Train AutoKeras Model\n",
    "\n",
    "    def PrepareAKData(data):\n",
    "        if type(data) != int:\n",
    "            data = data[[\"ds\",\"value\"]]\n",
    "            list_values = []\n",
    "            for index, row in tqdm(data.iterrows(), total = data.shape[0]):\n",
    "                dict_ = {\n",
    "                    \"ds\": row[\"ds\"]\n",
    "                }\n",
    "\n",
    "                for hour in range(24):\n",
    "                    if maya.MayaDT(int(row[\"ds\"])).hour == hour:\n",
    "                        dict_[\"hour_\" + str(hour)] = True\n",
    "                    else:\n",
    "                        dict_[\"hour_\" + str(hour)] = False\n",
    "                \n",
    "                for weekday in range(1,8):\n",
    "                    if maya.MayaDT(int(row[\"ds\"])).weekday == weekday:\n",
    "                        dict_[\"weekday_\" + str(weekday)] = True\n",
    "                    else:\n",
    "                        dict_[\"weekday_\" + str(weekday)] = False\n",
    "                \n",
    "                for month in range(1,13):\n",
    "                    if maya.MayaDT(int(row[\"ds\"])).month == month:\n",
    "                        dict_[\"month_\" + str(month)] = True\n",
    "                    else:\n",
    "                        dict_[\"month_\" + str(month)] = False\n",
    "                \n",
    "                dict_[\"value\"] = row[\"value\"]\n",
    "                list_values.append(dict_)\n",
    "        else:\n",
    "            dict_ = {\n",
    "                    \"ds\": data\n",
    "                }\n",
    "\n",
    "            for hour in range(24):\n",
    "                if maya.MayaDT(data).hour == hour:\n",
    "                    dict_[\"hour_\" + str(hour)] = True\n",
    "                else:\n",
    "                    dict_[\"hour_\" + str(hour)] = False\n",
    "            \n",
    "            for weekday in range(1,8):\n",
    "                if maya.MayaDT(data).weekday == weekday:\n",
    "                    dict_[\"weekday_\" + str(weekday)] = True\n",
    "                else:\n",
    "                    dict_[\"weekday_\" + str(weekday)] = False\n",
    "            \n",
    "            for month in range(1,13):\n",
    "                if maya.MayaDT(data).month == month:\n",
    "                    dict_[\"month_\" + str(month)] = True\n",
    "                else:\n",
    "                    dict_[\"month_\" + str(month)] = False\n",
    "            list_values = [dict_]\n",
    "        return pd.DataFrame(list_values)\n",
    "    \n",
    "    def GetPrevData(init, end, data, ts, name = \"value_\"):\n",
    "        list_dict = []\n",
    "        for i in tqdm(range(init, len(data))):\n",
    "            vals = data[(i-init): (i -end)]\n",
    "            dict_ = {}\n",
    "            for j in range(len(vals)):\n",
    "                name_val = name + str(len(vals) - j + end)\n",
    "                dict_[name_val] = vals[j]\n",
    "            dict_[\"ds\"] = ts[i]\n",
    "            list_dict.append(dict_)\n",
    "        return pd.DataFrame(list_dict)\n",
    "    try:\n",
    "        train_ak = PrepareAKData(ts_data[0:partition])\n",
    "        test_ak = PrepareAKData(ts_data[partition:])\n",
    "    except:\n",
    "        message = \"There was an error while Preparing AutoKeras Data measurement {name} with name {asset}\\nThe message is {text}\"\\\n",
    "            .format(name = \"heat consumption\", asset = name_asset, text = \"Not Available Message\")\n",
    "        webhook = DiscordWebhook(url='https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT', content=message)\n",
    "        response = webhook.execute()\n",
    "    try:\n",
    "        prepared_data = GetPrevData(24*4, 1, ts_data[\"value\"].tolist(), ts_data[\"ds\"].tolist())\n",
    "    except:\n",
    "        message = \"There was an error while Preparing Previous Data measurement {name} with name {asset}\\nThe message is {text}\"\\\n",
    "            .format(name = \"heat consumption\", asset = name_asset, text = \"Not Available Message\")\n",
    "        webhook = DiscordWebhook(url='https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT', content=message)\n",
    "        response = webhook.execute()\n",
    "    train_prev = prepared_data\n",
    "    test_prev = prepared_data\n",
    "    \n",
    "\n",
    "\n",
    "    train_data = pd.merge(train_ak, train_prev, on = \"ds\")\n",
    "    test_data = pd.merge(test_ak, test_prev, on = \"ds\")\n",
    "    \n",
    "    print(train_data.columns.values)\n",
    "\n",
    "    train_np = np.array(train_data.drop([\"value\",\"ds\"], axis = 1))\n",
    "    target_np = np.array(train_data[\"value\"])\n",
    "\n",
    "    test_np = np.array(test_data.drop([\"value\",\"ds\"], axis = 1))\n",
    "    real_np = np.array(test_data[\"value\"])\n",
    "\n",
    "    model_ak = ak.AutoModel(\n",
    "        ak.StructuredDataInput(),\n",
    "        ak.RegressionHead(),\n",
    "        directory= \"/tmp\",\n",
    "        max_trials= 4\n",
    "    )\n",
    "    model_ak.fit(\n",
    "        train_np,\n",
    "        target_np,\n",
    "        epochs = 10\n",
    "    )\n",
    "\n",
    "    y_hat = model_ak.predict(test_np)\n",
    "    y_hat_pred = []\n",
    "    for y in y_hat:\n",
    "        if pd.isna(y[0]):\n",
    "            y_hat_pred.append(0)\n",
    "        else:\n",
    "            y_hat_pred.append(y[0])\n",
    "    \n",
    "    mae_autokeras = mean_absolute_error(real_np, y_hat_pred)\n",
    "\n",
    "    if mae_arima < mae_exp:\n",
    "        print(\"ARIMA model used for \" + name_asset + \" predictions\")\n",
    "        print(str(mae_train_arima))\n",
    "        print(str(mae_arima))\n",
    "        model = \"ARIMA\"\n",
    "        mae = mae_arima\n",
    "        predictions = model_arima.predict(ts_data.shape[0], ts_data.shape[0]+freq)\n",
    "    else:\n",
    "        print(\"Exponential model used for \" + name_asset + \" predictions\")\n",
    "        print(str(mae_train_exp))\n",
    "        print(str(mae_exp))\n",
    "        model = \"Exponential Smoothing\"\n",
    "        mae = mae_exp\n",
    "        predictions = model_expon.predict(ts_data.shape[0], ts_data.shape[0]+freq)\n",
    "\n",
    "    \n",
    "    list_dates = []\n",
    "\n",
    "    for i in range(freq):\n",
    "        last_date = maya.MayaDT(last_date).add(minutes = 15).epoch\n",
    "        list_dates.append(last_date)\n",
    "\n",
    "    if mae_autokeras < mae:\n",
    "        print(\"AutoKeras model used for \" + name_asset + \" predictions\")\n",
    "        print(str(mae_autokeras))\n",
    "        mae = mae_autokeras\n",
    "        values = ts_data[\"value\"].tolist()\n",
    "        ts_ds = ts_data[\"ds\"].tolist()\n",
    "        predictions = []\n",
    "        dates_predictions = []\n",
    "        for date_epoch in list_dates:\n",
    "            ak_data = PrepareAKData(date_epoch)\n",
    "            prev_data = GetPrevData(24*4, 1, ts_data[\"value\"].tolist() + predictions + [0,0], ts_data[\"ds\"].tolist() + dates_predictions + [date_epoch, date_epoch + 1])\n",
    "            pred_data = pd.merge(ak_data, prev_data, on = \"ds\")\n",
    "            try:\n",
    "                pred_data = pred_data.drop(\"value\", axis = 1)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                pred_data = pred_data.drop(\"ds\", axis = 1)\n",
    "            except:\n",
    "                pass\n",
    "            y_hat = model_ak.predict(pred_data)\n",
    "            predictions.append(float(y_hat[0][0]))\n",
    "            dates_predictions.append(date_epoch)\n",
    "        \n",
    "        list_dates = dates_predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dict_output = {\n",
    "        \"ds\": list_dates,\n",
    "        \"preds\": list(predictions)\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [{\n",
    "            \"MAE\": mae,\n",
    "            \"MAE_AUTOKERAS\": mae_autokeras,\n",
    "            \"MAE_ARIMA\": mae_arima,\n",
    "            \"MAE_SMOOTH\": mae_exp\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    with open(output_predictions_path, \"w\") as out_file:\n",
    "        json.dump(dict_output, out_file)\n",
    "    \n",
    "    with open(ml_metrics_path, \"w\") as out_file:\n",
    "        json.dump(metrics, out_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckModel(input_data_path: InputPath(str), numb_occurences_min: int, numb_occurences_max: int) -> bool:\n",
    "    import json\n",
    "    print(\"Minimum\")\n",
    "    print(numb_occurences_min)\n",
    "    print(\"Maximum\")\n",
    "    print(numb_occurences_max)\n",
    "    with open(input_data_path) as data_file:\n",
    "        data_str = json.load(data_file)\n",
    "    print(\"Length data\")\n",
    "    print(len(data_str[\"ds\"]))\n",
    "\n",
    "    if numb_occurences_max != 0:\n",
    "        if (len(data_str) >= numb_occurences_min) and (len(data_str[\"ds\"]) < numb_occurences_max):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if len(data_str[\"ds\"]) >= numb_occurences_min:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SendPredictionsToInfluxDB(input_pred_building1_path: InputPath(str), input_pred_building2_path: InputPath(str), input_pred_sc1_path: InputPath(str)):\n",
    "\n",
    "    import json\n",
    "    import requests # TO REQUIREMENTS\n",
    "    from discord_webhook import DiscordWebhook # TO REQUIREMENTS\n",
    "\n",
    "    def GetDictPost(measurement_name, value, asset_name, time_pred):\n",
    "        if measurement_name == \"heat_supply\":\n",
    "            direction_energy = \"out\"\n",
    "        elif measurement_name == \"heat_consumption\":\n",
    "            direction_energy = \"in\"\n",
    "        else:\n",
    "            direction_energy = \"None\"\n",
    "        \n",
    "        data_post = {\n",
    "            \"bucket\": \"renergetic\",\n",
    "            \"measurement\": \"heat_meter\",\n",
    "            \"fields\":{\n",
    "                \"energy\": value\n",
    "            },\n",
    "            \"tags\":{\n",
    "                \"typeData\": \"forecast\",\n",
    "                \"domain\": \"heat\",\n",
    "                \"direction\": direction_energy,\n",
    "                \"prediction_window\": \"2h\",\n",
    "                \"asset_name\": asset_name,\n",
    "                \"time_prediction\": time_pred\n",
    "            }\n",
    "        }\n",
    "            \n",
    "        return data_post\n",
    "    \n",
    "    def SendData2Influx(measurement_name, value, asset_name, time_pred):\n",
    "        import json \n",
    "        url =  \"http://influx-api-ren-prototype.apps.paas-dev.psnc.pl/api/measurement\"\n",
    "\n",
    "        headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        data_post = GetDictPost(measurement_name, value, asset_name, time_pred)\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=json.dumps(data_post))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(\"Error in post \\n\"+ str(response.status_code) + \" \" + response.text)\n",
    "\n",
    "    with open(input_pred_building1_path) as data_file:\n",
    "        data_b1 = json.load(data_file)\n",
    "    \n",
    "    with open(input_pred_building2_path) as data_file:\n",
    "        data_b2 = json.load(data_file)\n",
    "    \n",
    "    with open(input_pred_sc1_path) as data_file:\n",
    "        data_sc1 = json.load(data_file)\n",
    "    \n",
    "    for i in range(len(data_b1[\"ds\"])):\n",
    "        try:\n",
    "            data_post_b1 = SendData2Influx(\"heat_consumption\", data_b1[\"preds\"][i], \"building1\", data_b1[\"ds\"][i])\n",
    "        except:\n",
    "            message = \"There was an error while trying to upload data to Influx for measurement {name} with name {asset}\\nThe message is {text}\"\\\n",
    "                .format(name = \"heat consumption\", asset = \"building1\", text = \"Not Available Message\")\n",
    "            webhook = DiscordWebhook(url='https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT', content=message)\n",
    "            response = webhook.execute()\n",
    "        try:\n",
    "            data_post_b2 = SendData2Influx(\"heat_consumption\", data_b2[\"preds\"][i], \"building2\", data_b2[\"ds\"][i])\n",
    "        except:\n",
    "            message = \"There was an error while trying to upload data to Influx for measurement {name} with name {asset}\\nThe message is {text}\"\\\n",
    "                .format(name = \"heat consumption\", asset = \"building2\", text = \"Not Available Message\")\n",
    "            webhook = DiscordWebhook(url='https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT', content=message)\n",
    "            response = webhook.execute()\n",
    "        try:\n",
    "            data_post_sc1 = SendData2Influx(\"heat_supply\", data_sc1[\"preds\"][i], \"solar_collector1\", data_sc1[\"ds\"][i])\n",
    "        except:\n",
    "            message = \"There was an error while trying to upload data to Influx for measurement {name} with name {asset}\\nThe message is {text}\"\\\n",
    "                .format(name = \"heat consumption\", asset = \"solar_collector1\", text = \"Not Available Message\")\n",
    "            webhook = DiscordWebhook(url='https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT', content=message)\n",
    "            response = webhook.execute()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE COMPILATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fablab/opt/anaconda3/lib/python3.8/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"Float\" based on the value \"0.8\".\n",
      "  warnings.warn(\n",
      "/Users/fablab/opt/anaconda3/lib/python3.8/site-packages/kfp/components/_data_passing.py:227: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"8\".\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def REN_Forecast_Pipeline(min_value_forecast):\n",
    "    env_var = V1EnvVar(name='HOME', value='/tmp')\n",
    "    get_measurement_op = comp.create_component_from_func(\n",
    "        GetMetrics_Func, packages_to_install = [\"requests\", \"pandas\", \"maya\", \"tqdm\"], output_component_file = \"metric_op_component.yaml\")\n",
    "    check_op = comp.create_component_from_func(\n",
    "        CheckModel, output_component_file=\"check_op.yaml\"\n",
    "    )\n",
    "    TrainingModel_op = comp.create_component_from_func(\n",
    "        TrainModelSMS, packages_to_install = [\"requests\", \"pandas\", \"maya\", \"statsmodels\", \"sklearn\"], output_component_file = \"model_op_component.yaml\")\n",
    "    TrainingModel_B_op = comp.create_component_from_func(\n",
    "        TrainModelB, packages_to_install = [\"requests\", \"pandas\", \"maya\", \"statsmodels\", \"sklearn\", \"autokeras\", \"tqdm\", \"numpy\", \"discord_webhook\"], output_component_file = \"model_b_op_component.yaml\")\n",
    "    \n",
    "    SendData_op = comp.create_component_from_func(\n",
    "        SendPredictionsToInfluxDB, packages_to_install= [\"requests\", \"discord_webhook\"], output_component_file= \"send_operation_component.yaml\"\n",
    "    )\n",
    "    \n",
    "    # Get Data from Influx Tasks\n",
    "    get_heat_task = (get_measurement_op(\"heat_meter\").add_env_variable(env_var)\n",
    "                    .set_memory_request('1Gi')\n",
    "                    .set_memory_limit('2Gi')\n",
    "                    .set_cpu_request('1')\n",
    "                    .set_cpu_limit('2'))\n",
    "    \n",
    "    check_A_task = (check_op(get_heat_task.outputs[\"output_timeseries\"], 0, min_value_forecast).add_env_variable(env_var)\n",
    "                    .set_memory_request('1Gi')\n",
    "                    .set_memory_limit('2Gi')\n",
    "                    .set_cpu_request('1')\n",
    "                    .set_cpu_limit('2'))\n",
    "    \n",
    "    check_B_task = (check_op(get_heat_task.outputs[\"output_timeseries\"], min_value_forecast, 0).add_env_variable(env_var)\n",
    "                    .set_memory_request('1Gi')\n",
    "                    .set_memory_limit('2Gi')\n",
    "                    .set_cpu_request('1')\n",
    "                    .set_cpu_limit('2'))\n",
    "\n",
    "    \n",
    "    #Develop and Train Model Tasks\n",
    "    with dsl.Condition(check_A_task.output == True):\n",
    "        modeling_b1_task = (TrainingModel_op(\"building1\",get_heat_task.outputs[\"output_timeseries\"]).add_env_variable(env_var).after(get_heat_task)\n",
    "                            .set_memory_request('1Gi')\n",
    "                            .set_memory_limit('2Gi')\n",
    "                            .set_cpu_request('1')\n",
    "                            .set_cpu_limit('2'))\n",
    "        modeling_b2_task = (TrainingModel_op(\"building2\",get_heat_task.outputs[\"output_timeseries\"]).add_env_variable(env_var).after(get_heat_task)\n",
    "                            .set_memory_request('1Gi')\n",
    "                            .set_memory_limit('2Gi')\n",
    "                            .set_cpu_request('1')\n",
    "                            .set_cpu_limit('2'))\n",
    "        \n",
    "        modeling_sc1_task = (TrainingModel_op(\"solar_collector1\",get_heat_task.outputs[\"output_timeseries\"]).add_env_variable(env_var).after(get_heat_task)\n",
    "                            .set_memory_request('1Gi')\n",
    "                            .set_memory_limit('2Gi')\n",
    "                            .set_cpu_request('2')\n",
    "                            .set_cpu_limit('4'))\n",
    "        \n",
    "        sending_data_task = (SendData_op(modeling_b1_task.outputs[\"output_predictions\"], modeling_b2_task.outputs[\"output_predictions\"], modeling_sc1_task.outputs[\"output_predictions\"]).add_env_variable(env_var)\n",
    "                            .set_memory_request('1Gi')\n",
    "                            .set_memory_limit('2Gi')\n",
    "                            .set_cpu_request('1')\n",
    "                            .set_cpu_limit('2'))\n",
    "    \n",
    "    with dsl.Condition(check_B_task.output == \"True\"):\n",
    "        modeling_b_b1_task = (TrainingModel_B_op(\"building1\",get_heat_task.outputs[\"output_timeseries\"]).add_env_variable(env_var).after(get_heat_task)\n",
    "                            .set_memory_request('1Gi')\n",
    "                            .set_memory_limit('2Gi')\n",
    "                            .set_cpu_request('1')\n",
    "                            .set_cpu_limit('2'))\n",
    "        modeling_b_b2_task = (TrainingModel_B_op(\"building2\",get_heat_task.outputs[\"output_timeseries\"]).add_env_variable(env_var).after(get_heat_task)\n",
    "                            .set_memory_request('1Gi')\n",
    "                            .set_memory_limit('2Gi')\n",
    "                            .set_cpu_request('1')\n",
    "                            .set_cpu_limit('2'))\n",
    "        modeling_b_sc1_task = (TrainingModel_B_op(\"solar_collector1\",get_heat_task.outputs[\"output_timeseries\"]).add_env_variable(env_var).after(get_heat_task)\n",
    "                            .set_memory_request('1Gi')\n",
    "                            .set_memory_limit('2Gi')\n",
    "                            .set_cpu_request('1')\n",
    "                            .set_cpu_limit('2'))\n",
    "        sending_b_data_task = (SendData_op(modeling_b_b1_task.outputs[\"output_predictions\"], modeling_b_b2_task.outputs[\"output_predictions\"], modeling_b_sc1_task.outputs[\"output_predictions\"]).add_env_variable(env_var)\n",
    "                            .set_memory_request('1Gi')\n",
    "                            .set_memory_limit('2Gi')\n",
    "                            .set_cpu_request('1')\n",
    "                            .set_cpu_limit('2'))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func = REN_Forecast_Pipeline, package_path =\"forecast_test_Pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-10-21T00:15:47Z'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import maya\n",
    "maya.MayaDT(1666311347).iso8601()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02e15b1dc1a9053df8aede2000f19bd423bcf01320081b3a1e044a45ff452adb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
