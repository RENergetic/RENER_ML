{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "import kfp.components as comp\n",
    "from kfp import compiler, dsl\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kubernetes.client.models import V1EnvVar\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. [Get Thresholds](#Thresholds)\n",
    "2. [Download Data](#Download)\n",
    "3. [Process Data](#process)\n",
    "4. [Forecast with previous model](#forecastprevious)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetThresholds(url_pilot: str, name_pilot: str,output_thresholds_path: OutputPath(str)):\n",
    "\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    def GetRequest(url, headers ={}, payload = {}):\n",
    "\n",
    "        from urllib3.exceptions import InsecureRequestWarning\n",
    "        import warnings\n",
    "        import contextlib\n",
    "\n",
    "        old_merge_environment_settings = requests.Session.merge_environment_settings\n",
    "\n",
    "        @contextlib.contextmanager\n",
    "        def no_ssl_verification():\n",
    "            opened_adapters = set()\n",
    "\n",
    "            def merge_environment_settings(self, url, proxies, stream, verify, cert):\n",
    "                # Verification happens only once per connection so we need to close\n",
    "                # all the opened adapters once we're done. Otherwise, the effects of\n",
    "                # verify=False persist beyond the end of this context manager.\n",
    "                opened_adapters.add(self.get_adapter(url))\n",
    "\n",
    "                settings = old_merge_environment_settings(self, url, proxies, stream, verify, cert)\n",
    "                settings['verify'] = False\n",
    "\n",
    "                return settings\n",
    "\n",
    "            requests.Session.merge_environment_settings = merge_environment_settings\n",
    "\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "                    yield\n",
    "            finally:\n",
    "                requests.Session.merge_environment_settings = old_merge_environment_settings\n",
    "\n",
    "                for adapter in opened_adapters:\n",
    "                    try:\n",
    "                        adapter.close()\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        with no_ssl_verification():\n",
    "            response = requests.request(\"GET\", url, headers = headers, data = payload)\n",
    "            \n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            dict_ = {\n",
    "                \"status_code\": response.status_code,\n",
    "                \"text\": response.text\n",
    "            }\n",
    "            return dict_\n",
    "    \n",
    "    if name_pilot != \"Virtual\":\n",
    "        url_ = \"{url_pilot}/api-postgre/1.0/api/threshold\".format(\n",
    "            url_pilot = url_pilot\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        url_ = \"http://api-ren-prototype.apps.paas-dev.psnc.pl/api/threshold\"\n",
    "        \n",
    "    try:\n",
    "        thresholds = GetRequest(url_)\n",
    "    except:\n",
    "        thresholds = {}\n",
    "\n",
    "    \n",
    "\n",
    "    with open(output_thresholds_path, \"w\") as file:\n",
    "        json.dump(thresholds, file)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(measurement_name: str, min_date: str, max_date: str,url_pilot : str, pilot_name:str, type_measurement :str,key_measurement : str,\n",
    "            filter_vars:list , filter_cases: list, output_data_forecast: OutputPath(str), output_data_metric : OutputPath(str)):\n",
    "\n",
    "    import requests # To REQUIREMENTS\n",
    "    import json\n",
    "    import pandas as pd # To REQUIREMENTS\n",
    "    import maya # To REQUIREMENTS\n",
    "    from tqdm import tqdm\n",
    "    from icecream import ic\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    from retry import retry # TO REQUIREMENTS\n",
    "\n",
    "    #Functions definitions\n",
    "\n",
    "    def GetRequest(url, headers ={}, payload = {}):\n",
    "\n",
    "        from urllib3.exceptions import InsecureRequestWarning\n",
    "        import warnings\n",
    "        import contextlib\n",
    "\n",
    "        old_merge_environment_settings = requests.Session.merge_environment_settings\n",
    "\n",
    "        @contextlib.contextmanager\n",
    "        def no_ssl_verification():\n",
    "            opened_adapters = set()\n",
    "\n",
    "            def merge_environment_settings(self, url, proxies, stream, verify, cert):\n",
    "                # Verification happens only once per connection so we need to close\n",
    "                # all the opened adapters once we're done. Otherwise, the effects of\n",
    "                # verify=False persist beyond the end of this context manager.\n",
    "                opened_adapters.add(self.get_adapter(url))\n",
    "\n",
    "                settings = old_merge_environment_settings(self, url, proxies, stream, verify, cert)\n",
    "                settings['verify'] = False\n",
    "\n",
    "                return settings\n",
    "\n",
    "            requests.Session.merge_environment_settings = merge_environment_settings\n",
    "\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "                    yield\n",
    "            finally:\n",
    "                requests.Session.merge_environment_settings = old_merge_environment_settings\n",
    "\n",
    "                for adapter in opened_adapters:\n",
    "                    try:\n",
    "                        adapter.close()\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        with no_ssl_verification():\n",
    "            response = requests.request(\"GET\", url, headers = headers, data = payload)\n",
    "            \n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            dict_ = {\n",
    "                \"status_code\": response.status_code,\n",
    "                \"text\": response.text\n",
    "            }\n",
    "            return dict_\n",
    "    def DownloadAssetsData(measurement_name, url_pilot,bucket = \"renergetic\", min_date = \"yesterday\", max_date = \"tomorrow\"):\n",
    "        \n",
    "        from datetime import datetime\n",
    "        import pandas as pd\n",
    "        import maya\n",
    "        from tqdm import tqdm\n",
    "        from icecream import ic\n",
    "\n",
    "        test = True\n",
    "\n",
    "        try:\n",
    "            min_date_from = maya.when(min_date).datetime()\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MIN_DATE\")\n",
    "        \n",
    "        try: \n",
    "            max_date_from = maya.when(max_date).datetime()\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MAX_DATE\")\n",
    "        \n",
    "        datelist = pd.date_range(min_date_from, max_date_from)\n",
    "\n",
    "        data_ = []\n",
    "        for i in tqdm(range(len(datelist)-1)):\n",
    "            from_obj = datelist[i]\n",
    "            to_obj = datelist[i+1]\n",
    "            from_ = datetime.strftime(from_obj, \"%Y-%m-%d 00:00:00\")\n",
    "            to_ = datetime.strftime(to_obj, \"%Y-%m-%d 00:00:00\")\n",
    "\n",
    "            if pilot_name == \"Virtual\":\n",
    "                url = \"http://influx-api-ren-prototype.apps.paas-dev.psnc.pl/api/measurement/data?measurements={measurement_name}&from={from_}&to={to_}\"\\\n",
    "                    .format(measurement_name = measurement_name, from_ = from_, to_= to_)\n",
    "            else:\n",
    "                url = url_pilot + \"/api-measurement/1.0/api/measurement/data?measurements={measurement_name}&from={from_}&to={to_}\"\\\n",
    "                    .format(measurement_name = measurement_name, from_ = from_, to_= to_)\n",
    "            info_ = GetRequest(url)\n",
    "            if type(info_) == list:\n",
    "                data_ = data_ + info_\n",
    "            elif type(info_) == dict:\n",
    "                print(\"Error\")\n",
    "                print(from_)\n",
    "                print(to_)\n",
    "        return data_\n",
    "    def DataFrameAssests(list_data, name_field):\n",
    "        dicts = []\n",
    "        for data in list_data:\n",
    "            try:\n",
    "                if \"energy\" in data[\"fields\"].keys():\n",
    "                    name_value = \"energy\"\n",
    "                else:\n",
    "                    name_value = name_field\n",
    "                dict_ = {\n",
    "                    \"asset_name\": data[\"tags\"][\"asset_name\"],\n",
    "                    \"value\": float(data[\"fields\"][name_value]),\n",
    "                    \"ds\": data[\"fields\"][\"time\"]\n",
    "                }\n",
    "\n",
    "                if \"type_data\" in data[\"tags\"].keys():\n",
    "                    dict_[\"type\"] = data[\"tags\"][\"type_data\"]\n",
    "                elif \"typeData\" in data[\"tags\"].keys():\n",
    "                    dict_[\"type\"] = data[\"tags\"][\"typeData\"]\n",
    "                else:\n",
    "                    dict_[\"type\"] = \"None\"\n",
    "\n",
    "                if \"measurement_type\" in data[\"tags\"].keys():\n",
    "                    dict_[\"measurement_type\"] = data[\"tags\"][\"measurement_type\"]\n",
    "                else:\n",
    "                    dict_[\"measurement_type\"] = \"None\"\n",
    "                \n",
    "                if \"direction\" in data[\"tags\"].keys():\n",
    "                    dict_[\"direction\"] = data[\"tags\"][\"direction\"]\n",
    "                else:\n",
    "                    dict_[\"direction\"] = \"None\"\n",
    "                if \"domain\" in data[\"tags\"].keys():\n",
    "                    dict_[\"domain\"] = data[\"tags\"][\"domain\"]\n",
    "                else:\n",
    "                    dict_[\"domain\"] = \"None\"\n",
    "                \n",
    "                if \"sensor_id\" in data[\"tags\"].keys():\n",
    "                    dict_[\"id_sensor\"] = data[\"tags\"][\"sensor_id\"]\n",
    "                else:\n",
    "                    dict_[\"id_sensor\"] = \"None\"\n",
    "                \n",
    "                if \"interpolation_method\" in data[\"tags\"].keys():\n",
    "                    dict_[\"interpolation\"] = data[\"tags\"][\"interpolation_method\"]\n",
    "                else:\n",
    "                    dict_[\"interpolation\"] = \"None\"\n",
    "\n",
    "                dicts.append(dict_)\n",
    "            except:\n",
    "                continue\n",
    "        return pd.DataFrame(dicts)\n",
    "    \n",
    "    @retry(tries= 3)\n",
    "    def DownloadAndProcess(measurement_name, url_pilot, min_date, max_date, key_measurement):\n",
    "        # max_date = maya.now().add(days = 3).iso8601()\n",
    "        list_ = DownloadAssetsData(measurement_name, url_pilot,min_date = min_date, max_date = max_date)\n",
    "        data = DataFrameAssests(list_, key_measurement)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def FilterCases(var_value, filter_cases):\n",
    "        if var_value in filter_cases:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def FilterData(data, type_measurement, filter_vars, filter_cases):\n",
    "\n",
    "        for i in range(len(filter_vars)):\n",
    "            data[\"Filter\"] = data[filter_vars[i]].apply(FilterCases, filter_cases = filter_cases[i])\n",
    "            data = data[data[\"Filter\"] == True]\n",
    "        \n",
    "        return data    \n",
    "    \n",
    "    def SendAlert(data):\n",
    "        ic(data.shape[0])\n",
    "\n",
    "        if data.shape[0] == 0:\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Not enough data for {measurement_name}\".format(measurement_name = measurement_name)\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "            \n",
    "            raise ValueError(\"Void data to forecast\")\n",
    "\n",
    "    # Code Execution\n",
    "    \n",
    "    data_all = DownloadAndProcess(measurement_name, url_pilot, min_date, max_date, key_measurement)\n",
    "    data_filtered = FilterData(data_all, type_measurement, filter_vars, filter_cases)\n",
    "    data_to_train = data_filtered[data_filtered.type == type_measurement]\n",
    "    SendAlert(data_to_train)\n",
    "    \n",
    "\n",
    "    data_output = {\n",
    "        \"value\": data_to_train[\"value\"].tolist(),\n",
    "        \"time_registered\": data_to_train[\"ds\"].tolist(),\n",
    "        \"asset_name\": data_to_train[\"asset_name\"].tolist()\n",
    "    }\n",
    "\n",
    "    with open(output_data_forecast, \"w\") as file:\n",
    "        json.dump(data_output, file)\n",
    "\n",
    "    \n",
    "    data_output_metrics = {\n",
    "        \"value\": data_filtered[\"value\"].tolist(),\n",
    "        \"asset_name\": data_filtered[\"asset_name\"].tolist(),\n",
    "        \"time_registered\": data_filtered[\"ds\"].tolist(),\n",
    "        \"type\": data_filtered[\"type\"].tolist()\n",
    "    }\n",
    "\n",
    "    with open(output_data_metric, \"w\") as file:\n",
    "        json.dump(data_output_metrics, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessData(input_data_path: InputPath(str), hourly_aggregate, minute_aggregate ,min_date, max_date, output_data_forecast: OutputPath(str)):\n",
    "\n",
    "    import maya\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from icecream import ic\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    min_date = datetime.strftime(maya.when(min_date).datetime(), \"%Y-%m-%d\")\n",
    "    max_date = datetime.strftime(maya.when(max_date).datetime(), \"%Y-%m-%d\")\n",
    "    \n",
    "    ic(hourly_aggregate)\n",
    "    ic(minute_aggregate)\n",
    "\n",
    "    with open(input_data_path) as file:\n",
    "        data_str = json.load(file)\n",
    "    \n",
    "    data = pd.DataFrame(data_str)\n",
    "\n",
    "    # DEFINE HOURLY AGGREGATE PROCESS\n",
    "\n",
    "    def ProcessHourly(data, hourly_aggregate, min_date, max_date):\n",
    "        list_dicts = []\n",
    "        list_data = []\n",
    "\n",
    "        for asset_name in pd.unique(data.asset_name):\n",
    "            data_iter = data[data.asset_name == asset_name]\n",
    "            \n",
    "            def GetHourDate(str_):\n",
    "                import maya\n",
    "                from datetime import datetime\n",
    "\n",
    "                return datetime.strftime(maya.parse(str_).datetime(), \"%Y-%m-%d %H:00:00\")\n",
    "\n",
    "            data_iter[\"hour\"] = data_iter[\"time_registered\"].apply(GetHourDate)\n",
    "            data_group = (data_iter.groupby('hour')\n",
    "                .agg({'time_registered':'count', 'value': hourly_aggregate})\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            df = data_group[[\"hour\", \"value\"]]\n",
    "            df = df.rename(columns={'hour':'ds', 'value': 'y'})\n",
    "            last_value = df.y.tolist()[0]\n",
    "            for ds_obj in tqdm(pd.date_range(min_date, max_date)):\n",
    "                for i in range(24):\n",
    "                    if i < 10:\n",
    "                        str_i = \"0\"+str(i)\n",
    "                    else:\n",
    "                        str_i = str(i)\n",
    "                    ds_str = \"{date} {H}:00:00\".format(date = ds_obj.strftime(\"%Y-%m-%d\"), H = str_i)\n",
    "\n",
    "                    if df[df.ds == ds_str].shape[0] == 0:\n",
    "                        if hourly_aggregate == \"max\":\n",
    "                                value_ = last_value\n",
    "                        else:\n",
    "                            value_ = 0\n",
    "\n",
    "                        dict_ = {\n",
    "                            \"time_registered\": ds_str,\n",
    "                            \"value\": value_,\n",
    "                            \"asset_name\": asset_name\n",
    "                        }\n",
    "                    else:\n",
    "                        dict_ = {\n",
    "                            \"time_registered\": ds_str,\n",
    "                            \"value\": df[df.ds == ds_str].y.tolist()[0],\n",
    "                            \"asset_name\": asset_name\n",
    "                        }\n",
    "                        if minute_aggregate == \"max\":\n",
    "                            last_value = dict_[\"value\"]\n",
    "                    list_dicts.append(dict_)\n",
    "            if hourly_aggregate == \"max\":\n",
    "                data_1 = pd.DataFrame(list_dicts)\n",
    "                data_1[\"value_1\"] = data_1[\"value\"].shift(1)\n",
    "                data_1[\"value\"] = data_1[\"value\"] - data_1[\"value_1\"]\n",
    "                data_1 = data_1.drop([\"value_1\"], axis = 1)\n",
    "                data_1[\"value\"] = data_1[\"value\"].fillna(0)\n",
    "                list_data.append(data_1)\n",
    "                list_dicts = []\n",
    "\n",
    "        if hourly_aggregate == \"max\":\n",
    "            output_data = pd.concat(list_data, ignore_index = True)\n",
    "        else:\n",
    "            output_data = pd.DataFrame(list_dicts)\n",
    "        \n",
    "        return output_data\n",
    "    \n",
    "    def ProcessMinutely(data, minute_aggregate, min_date, max_date):\n",
    "        list_dicts = []\n",
    "        list_data = []\n",
    "        for asset_name in tqdm(pd.unique(data.asset_name)):\n",
    "            data_iter = data[data.asset_name == asset_name]\n",
    "            def GetHourMinuteDate(str_):\n",
    "                import maya\n",
    "                from datetime import datetime\n",
    "\n",
    "                return datetime.strftime(maya.parse(str_).datetime(), \"%Y-%m-%d %H:%M:00\")\n",
    "            \n",
    "            data_iter[\"minute\"] = data_iter[\"time_registered\"].apply(GetHourMinuteDate)\n",
    "            data_group = (data_iter.groupby('minute')\n",
    "                .agg({'time_registered':'count', 'value': minute_aggregate})\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            df = data_group[[\"minute\", \"value\"]]\n",
    "            df = df.rename(columns={'minute':'ds', 'value': 'y'})\n",
    "\n",
    "            last_value = df.y.tolist()[0]\n",
    "\n",
    "            for ds_obj in tqdm(pd.date_range(min_date, max_date)):\n",
    "                for i in range(24):\n",
    "                    for j in range(60):\n",
    "                        if i < 10:\n",
    "                            str_i = \"0\"+str(i)\n",
    "                        else:\n",
    "                            str_i = str(i)\n",
    "                        \n",
    "                        if j < 10:\n",
    "                            str_j = \"0\" + str(j)\n",
    "                        else:\n",
    "                            str_j = str(j)\n",
    "                        ds_str = \"{date} {H}:{M}:00\".format(date = ds_obj.strftime(\"%Y-%m-%d\"), H = str_i, M = str_j)\n",
    "\n",
    "                        if df[df.ds == ds_str].shape[0] == 0:\n",
    "\n",
    "                            if minute_aggregate == \"max\":\n",
    "                                value_ = last_value\n",
    "                            else:\n",
    "                                value_ = 0\n",
    "\n",
    "                            dict_ = {\n",
    "                                \"time_registered\": ds_str,\n",
    "                                \"value\": value_,\n",
    "                                \"asset_name\": asset_name\n",
    "                            }\n",
    "                        else:\n",
    "                            dict_ = {\n",
    "                                \"time_registered\": ds_str,\n",
    "                                \"value\": df[df.ds == ds_str].y.tolist()[0],\n",
    "                                \"asset_name\": asset_name\n",
    "                            }\n",
    "                            if minute_aggregate == \"max\":\n",
    "                                last_value = dict_[\"value\"]\n",
    "                        list_dicts.append(dict_)\n",
    "            if minute_aggregate == \"max\":\n",
    "                data_1 = pd.DataFrame(list_dicts)\n",
    "                data_1[\"value_1\"] = data_1[\"value\"].shift(1)\n",
    "                data_1[\"value_cummulative\"] = data_1[\"value\"].copy()\n",
    "                data_1[\"value\"] = data_1[\"value\"] - data_1[\"value_1\"]\n",
    "                data_1 = data_1.drop([\"value_1\"], axis = 1)\n",
    "                data_1[\"value\"] = data_1[\"value\"].fillna(0)\n",
    "                list_data.append(data_1)\n",
    "                list_dicts = []\n",
    "\n",
    "        if minute_aggregate == \"max\":\n",
    "            output_data = pd.concat(list_data, ignore_index = True)\n",
    "        else:\n",
    "            output_data = pd.DataFrame(list_dicts)\n",
    "        \n",
    "        return output_data\n",
    "    \n",
    "    if hourly_aggregate in [\"mean\",\"sum\", \"max\"]:\n",
    "        print(\"hourly process\")\n",
    "        output_data = ProcessHourly(data, hourly_aggregate, min_date, max_date)\n",
    "    elif minute_aggregate in [\"max\", \"sum\", \"mean\"]:\n",
    "        print(\"minutely process\")\n",
    "        output_data = ProcessMinutely(data, minute_aggregate, min_date, max_date)\n",
    "    else:\n",
    "        output_data = data\n",
    "\n",
    "    \n",
    "\n",
    "    data_output = {\n",
    "        \"value\": output_data[\"value\"].tolist(),\n",
    "        \"time_registered\": output_data[\"time_registered\"].tolist(),\n",
    "        \"asset_name\": output_data[\"asset_name\"].tolist()\n",
    "    }\n",
    "\n",
    "    if minute_aggregate == \"max\" or hourly_aggregate == \"max\":\n",
    "        data_output[\"value_cummulative\"] = output_data[\"value_cummulative\"].tolist()\n",
    "\n",
    "    with open(output_data_forecast, \"w\") as file:\n",
    "        json.dump(data_output, file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Weather Data From Open Meteo API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownloadWeatherData_OpenMeteo(input_weather_influx: InputPath(str), city_name: str, start_date:str, output_weather_data : OutputPath(str)):\n",
    "    import maya\n",
    "    from datetime import datetime\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    def ManageDateTime(ds_obj):\n",
    "        return datetime.strftime(maya.parse(ds_obj).datetime(), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    class InvalidArgument(Exception):\n",
    "        \"Input correctly City name or Lat AND Lon for city\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "    def GetWeatherData(city = \"None\", lon = \"no\", lat= \"no\", start_date= \"yesterday\", end_date= \"today\"):\n",
    "\n",
    "        # Input city or lat/lon + start date and end date\n",
    "\n",
    "        if city != \"None\":\n",
    "            url = \"https://geocoding-api.open-meteo.com/v1/search?name={city_name}&count=10&language=en&format=json\".format(city_name = city)\n",
    "            geo_ = requests.get(url)\n",
    "            try:\n",
    "                results_ = geo_.json()[\"results\"][0]\n",
    "                lat = results_[\"latitude\"]\n",
    "                lon = results_[\"longitude\"]\n",
    "            except KeyError:\n",
    "                raise InvalidArgument\n",
    "        elif lat == \"no\" or lon == \"no\":\n",
    "            raise InvalidArgument\n",
    "        start_date_parsed = datetime.strftime(maya.when(start_date).datetime(), \"%Y-%m-%d\")\n",
    "        end_date_parsed = datetime.strftime(maya.when(end_date).datetime(), \"%Y-%m-%d\")\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive?latitude={lat}&longitude={lon}&start_date={start_date}&end_date={end_date}&hourly=temperature_2m,cloudcover,shortwave_radiation,direct_radiation,diffuse_radiation,direct_normal_irradiance\"\\\n",
    "            .format(lat = lat, lon = lon, start_date = start_date_parsed, end_date = end_date_parsed)\n",
    "        data = requests.get(url)\n",
    "        try:\n",
    "            data = data.json()[\"hourly\"]\n",
    "        except:\n",
    "            print(data)\n",
    "        data = pd.DataFrame(data)\n",
    "        data[\"ds\"] = data[\"time\"].apply(ManageDateTime)\n",
    "\n",
    "        url = \"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&past_days=10&hourly=temperature_2m,cloudcover,shortwave_radiation,direct_radiation,diffuse_radiation,direct_normal_irradiance\"\\\n",
    "            .format(latitude = lat, longitude = lon)\n",
    "        forecast_data = requests.get(url)\n",
    "        forecast_data = pd.DataFrame(forecast_data.json()[\"hourly\"])\n",
    "        forecast_data[\"ds\"] = forecast_data[\"time\"].apply(ManageDateTime)\n",
    "\n",
    "        data[\"type\"] = \"real\"\n",
    "        forecast_data[\"type\"] = \"forecast\"\n",
    "\n",
    "\n",
    "        return pd.concat([data, forecast_data], ignore_index= True)\n",
    "    def ProcessWeatherData(weather_data):\n",
    "        weather_data_real = weather_data[weather_data.type == \"real\"]\n",
    "        weather_data_real[\"check\"] = weather_data_real[\"temperature_2m\"].apply(lambda x: pd.isna(x) != True)\n",
    "        weather_data_real = weather_data_real[weather_data_real.check == True]\n",
    "        real_ds = weather_data_real.ds.tolist()\n",
    "\n",
    "        weather_data_forecast = weather_data[weather_data.type == \"forecast\"]\n",
    "        weather_data_forecast[\"check\"] = weather_data_forecast[\"ds\"].apply(lambda x: x not in real_ds)\n",
    "        weather_data_forecast = weather_data_forecast[weather_data_forecast.check == True]\n",
    "\n",
    "        weather_data = pd.concat([weather_data_real.drop([\"check\"], axis = 1), weather_data_forecast.drop([\"check\"], axis = 1)], ignore_index= True)\n",
    "\n",
    "        weather_data = weather_data[[\"ds\", 'temperature_2m', 'cloudcover', 'shortwave_radiation',\n",
    "        'direct_radiation', 'diffuse_radiation', 'direct_normal_irradiance']]\n",
    "        weather_data.columns = [\"ds_hour\", 'temperature_2m', 'cloudcover', 'shortwave_radiation',\n",
    "            'direct_radiation', 'diffuse_radiation', 'direct_normal_irradiance']\n",
    "        \n",
    "        return weather_data\n",
    "    \n",
    "    try:\n",
    "        weather_influx = pd.read_feather(input_weather_influx)\n",
    "    except:\n",
    "        weather_influx = None\n",
    "    \n",
    "    if weather_influx == None:\n",
    "        try:\n",
    "            weather_influx = pd.read_csv(input_weather_influx)\n",
    "        except:\n",
    "            weather_influx = None\n",
    "\n",
    "    if city_name == \"Virtual\":\n",
    "        city_name = \"Madrid\"\n",
    "\n",
    "    if weather_influx == None:\n",
    "        weather_data = GetWeatherData(city_name, start_date = start_date)\n",
    "        weather_data = ProcessWeatherData(weather_data)\n",
    "    else:\n",
    "        weather_data = weather_influx\n",
    "    try:\n",
    "        weather_data.to_feather(output_weather_data)\n",
    "    except:\n",
    "        weather_data.to_csv(output_weather_data)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data from Influx DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownloadDataFromInfluxDB(timestamp:float, output_weather_info_path: OutputPath(str)):\n",
    "    import pandas as pd\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    try:\n",
    "        data.to_feather(output_weather_info_path)\n",
    "    except:\n",
    "        data.to_csv(output_weather_info_path, index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateForecastMetrics(input_data_metric_path: InputPath(str), asset_name, mae_threshold: float) -> bool:\n",
    "    \n",
    "    import json\n",
    "    import maya\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from icecream import ic\n",
    "\n",
    "    with open(input_data_metric_path) as file:\n",
    "        data_metrics = json.load(file)\n",
    "    \n",
    "    data_metrics = pd.DataFrame(data_metrics)\n",
    "    data_metrics = data_metrics[data_metrics.asset_name == asset_name]\n",
    "\n",
    "    max_ds = max(data_metrics.time_registered)\n",
    "    min_ds = min(data_metrics.time_registered)\n",
    "    \n",
    "    if (maya.when(max_ds)- maya.when(min_ds)).days < 30:\n",
    "        return True\n",
    "    else:\n",
    "        date_metric = datetime.strftime(maya.when(max_ds).add(days = -7).datetime(), \"%Y-%m-%d\")\n",
    "        data_metrics_real = data_metrics[(data_metrics.time_registered >= date_metric) & (data_metrics.type == \"real\")][[\"time_registered\", \"value\"]]\n",
    "        data_metrics_forecast = data_metrics[(data_metrics.time_registered >= date_metric) & (data_metrics.type == \"forecast\")][[\"time_registered\", \"value\"]]\n",
    "        data_metrics_real.columns = [\"ds\", \"value_real\"]\n",
    "        data_metrics_forecast.columns = [\"ds\", \"value_forecast\"]\n",
    "\n",
    "        data_metrics = pd.merge(data_metrics_real, data_metrics_forecast, on = \"ds\")\n",
    "        try:\n",
    "            mae_metric = mean_absolute_error(data_metrics[\"value_real\"], data_metrics[\"value_forecast\"])\n",
    "        except ValueError:\n",
    "            mae_metric = mae_threshold + 1\n",
    "\n",
    "        ic(mae_metric)\n",
    "        ic(mae_threshold)\n",
    "\n",
    "        if mae_metric > mae_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast_Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictFromPreviousModel(input_data_path:InputPath(str), input_weather_path: InputPath(str),\n",
    "        name_pilot, measurement_name, asset_name, name_model, max_date, num_days, diff_time,\n",
    "        forecast_data_path: OutputPath(str)):\n",
    "\n",
    "    import maya\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import json\n",
    "    from icecream import ic\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from prophet.serialize import model_to_json, model_from_json\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from minio import Minio\n",
    "    import boto3\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    from datetime import datetime\n",
    "\n",
    "    def ManageDateHour(ds_obj):\n",
    "            return datetime.strftime(maya.parse(ds_obj).datetime(), \"%Y-%m-%d %H:00:00\")\n",
    "\n",
    "    def ModifyData(data, asset_name):\n",
    "        data_ds = data[data.asset_name == asset_name][[\"time_registered\", \"value\"]]\n",
    "        try:\n",
    "            last_cummulative_value = data[data.asset_name == asset_name][\"value_cummulative\"].tolist()[-1]\n",
    "        except:\n",
    "            last_cummulative_value = 0\n",
    "        data_ds.columns = [\"ds\", \"y\"]\n",
    "        if data_ds.shape[0] == 0:\n",
    "            max_date = datetime.strftime(maya.when(\"now\").datetime(),\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            max_date = max(data_ds.ds)\n",
    "        ic(max_date)\n",
    "        ic(last_cummulative_value)\n",
    "        ic(data_ds.shape)\n",
    "        ic(len(pd.unique(data_ds.y)))\n",
    "\n",
    "        return data_ds, max_date, last_cummulative_value\n",
    "\n",
    "    s3 = boto3.resource(\n",
    "        service_name='s3',\n",
    "        aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "        aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "        endpoint_url='https://s3.tebi.io'\n",
    "    )\n",
    "\n",
    "    s3_client = boto3.client('s3',\n",
    "        aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "        aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "        endpoint_url='https://s3.tebi.io'\n",
    "    )\n",
    "\n",
    "    # model_Ghent_pv_de-nieuwe-dokken-pv-017A-xxxxx9A1.json\n",
    "    with open(input_data_path) as file:\n",
    "        data_str = json.load(file)\n",
    "    \n",
    "    data = pd.DataFrame(data_str)\n",
    "    data, max_date, last_cummulative_value = ModifyData(data, asset_name)\n",
    "    metrics_list = []\n",
    "    ic(asset_name)\n",
    "\n",
    "    weather_data = pd.read_feather(input_weather_path)\n",
    "    \n",
    "    my_bucket = s3.Bucket('test-pf')\n",
    "    list_objects = []\n",
    "    for my_bucket_object in my_bucket.objects.all():\n",
    "        list_objects.append(my_bucket_object.key)\n",
    "    \n",
    "    try:\n",
    "        file_stats = \"stats_{name_pilot}_{measurement_name}_{asset_name}.json\".format(\n",
    "            name_pilot = name_pilot,measurement_name = measurement_name, asset_name = asset_name\n",
    "        )\n",
    "        \n",
    "        with open(file_stats, 'wb') as f:\n",
    "            s3_client.download_fileobj('test-pf', file_stats, f)\n",
    "        \n",
    "        with open(file_stats) as f:\n",
    "            stats_asset_models = json.load(f)\n",
    "        \n",
    "        if \"catboost\" in stats_asset_models.keys():\n",
    "            last_date_catboost = stats_asset_models[\"catboost\"][\"last_update_date\"]\n",
    "        else:\n",
    "            last_date_catboost = datetime.strftime(maya.when(\"1 Jan 1970\").datetime(), format = \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        if \"prophet\" in stats_asset_models.keys():\n",
    "            last_date_prophet = stats_asset_models[\"prophet\"][\"last_update_date\"]\n",
    "        else:\n",
    "            last_date_prophet = datetime.strftime(maya.when(\"1 Jan 1970\").datetime(), format = \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        if last_date_prophet == last_date_catboost:\n",
    "            raise ValueError(\"No models trained\")\n",
    "        elif last_date_catboost > last_date_prophet:\n",
    "            type_ = \"catboost\"\n",
    "        else:\n",
    "            type_ = \"prophet\"\n",
    "\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    name_model = \"model_{name_pilot}_{measurement_name}_{asset_name}_{type_}\".format(\n",
    "        name_pilot = name_pilot,measurement_name = measurement_name, asset_name = asset_name, type_ = type_\n",
    "    )\n",
    "\n",
    "    if type_ == \"prophet\":\n",
    "        with open('model_prophet.json', 'r') as fin:\n",
    "            m = model_from_json(fin.read())\n",
    "\n",
    "        from_date_obj = maya.parse(last_date_prophet).add(days = -2)\n",
    "        to_date_obj = maya.when(max_date).add(days = num_days)\n",
    "        days_forecast = (to_date_obj - from_date_obj).days\n",
    "        measures_per_hour = 60/diff_time\n",
    "        future = m.make_future_dataframe(periods= 24*(3 + days_forecast)*measures_per_hour , freq=\"{minutes}T\".format(minutes = diff_time))\n",
    "        future[\"ds\"] = future[\"ds\"].apply(str)\n",
    "        future[\"ds_hour\"] = future[\"ds\"].apply(ManageDateHour)\n",
    "        future = pd.merge(future, weather_data, on = \"ds_hour\")\n",
    "\n",
    "        forecast = m.predict(future)\n",
    "\n",
    "        forecast[forecast.ds > max_date].to_csv(forecast_data_path, index = False)\n",
    "\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForecastProcess(input_data_path: InputPath(str), input_weather_path: InputPath(str),\n",
    "    measurement_name,\n",
    "    path_minio,\n",
    "    access_key,\n",
    "    secret_key,\n",
    "    mode,\n",
    "    url_pilot,\n",
    "    diff_time,\n",
    "    pilot_name,\n",
    "    send_forecast,\n",
    "    asset_name,\n",
    "    num_days,\n",
    "    mode_prophet,\n",
    "    daily_seasonality,\n",
    "    weekly_seasonality,\n",
    "    mlpipeline_metrics_path: OutputPath('Metrics'),\n",
    "    forecast_data_path: OutputPath(str)\n",
    "    ):\n",
    "\n",
    "    import maya\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import json\n",
    "    from icecream import ic\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from prophet.serialize import model_to_json\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from minio import Minio\n",
    "    import boto3\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "    from datetime import datetime\n",
    "\n",
    "    from darts.models import RNNModel\n",
    "    from darts import TimeSeries\n",
    "    from darts.dataprocessing.transformers import Scaler\n",
    "    from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        client = Minio(\n",
    "            path_minio,\n",
    "            access_key=access_key,\n",
    "            secret_key=secret_key,\n",
    "            secure = False\n",
    "        )\n",
    "\n",
    "        list_objects = client.list_objects(\"test\")\n",
    "        for obj_ in list_objects:\n",
    "            ic(obj_._object_name)\n",
    "    except:\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Cannot access minio server correctly - read data.\"\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "    \n",
    "\n",
    "    def ForecastData(data, asset_name, measurement_name, metrics_list, measures_per_hour, diff_time, weather_data ,mode_prophet,daily_seasonality, weekly_seasonality,mode = \"no notifications\"):\n",
    "        \n",
    "        # Generic Processing Functions\n",
    "\n",
    "        def ModifyData(data, asset_name):\n",
    "            data_ds = data[data.asset_name == asset_name][[\"time_registered\", \"value\"]]\n",
    "            try:\n",
    "                last_cummulative_value = data[data.asset_name == asset_name][\"value_cummulative\"].tolist()[-1]\n",
    "            except:\n",
    "                last_cummulative_value = 0\n",
    "            data_ds.columns = [\"ds\", \"y\"]\n",
    "            if data_ds.shape[0] == 0:\n",
    "                max_date = datetime.strftime(maya.when(\"now\").datetime(),\"%Y-%m-%d %H:%M:%S\")\n",
    "            else:\n",
    "                max_date = max(data_ds.ds)\n",
    "            ic(max_date)\n",
    "            ic(last_cummulative_value)\n",
    "            ic(data_ds.shape)\n",
    "            ic(len(pd.unique(data_ds.y)))\n",
    "\n",
    "            return data_ds, max_date, last_cummulative_value\n",
    "        # Categorical Processing Functions\n",
    "\n",
    "        def GetDateInfo(ds_str, time_value):\n",
    "            maya_obj = maya.parse(ds_str)\n",
    "            if time_value == \"year\":\n",
    "                return str(maya_obj.year)\n",
    "            elif time_value == \"month\":\n",
    "                return str(maya_obj.month)\n",
    "            elif time_value == \"weekday\":\n",
    "                return str(maya_obj.weekday)\n",
    "            elif time_value == \"hour\":\n",
    "                return str(maya_obj.hour)\n",
    "            \n",
    "        def ManageData(data_ds, num_prevs = 24):\n",
    "            data_ds[\"year\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"year\")\n",
    "            data_ds[\"month\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"month\")\n",
    "            data_ds[\"weekday\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"weekday\")\n",
    "            data_ds[\"hour\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"hour\")\n",
    "\n",
    "            for var_ in [\"year\", \"month\", \"weekday\", \"hour\"]:\n",
    "                if len(pd.unique(data_ds[var_])) <= 2:\n",
    "                    bin_main = list(pd.unique(data_ds[var_]))[0]\n",
    "                    data_ds[var_] = (data_ds[var_] == bin_main)\n",
    "            \n",
    "            for i in range(1,num_prevs + 1):\n",
    "                name_var = \"prev_val_{i}\".format(i = i)\n",
    "                data_ds[name_var] = data_ds[\"y\"].shift(i).apply(str)\n",
    "            data_ds = data_ds[(num_prevs+1):]\n",
    "\n",
    "            cat_features_names = [\"year\", \"month\", \"weekday\", \"hour\"]\n",
    "            names_prevs_vars = []\n",
    "            for name_var in data_ds.columns.values:\n",
    "                if \"prev_val\" in name_var:\n",
    "                    names_prevs_vars.append(name_var)\n",
    "            cat_features_names = cat_features_names + names_prevs_vars\n",
    "\n",
    "\n",
    "            return data_ds, names_prevs_vars, cat_features_names\n",
    "        \n",
    "        def Train_CatBoost(data_ds, cat_features_names):\n",
    "\n",
    "                # Process data\n",
    "                X_train, X_test, Y_train, Y_test = train_test_split(data_ds.drop([\"y\", \"ds\"], axis = 1), data_ds.y, test_size = 0.2)\n",
    "                \n",
    "\n",
    "                # Pool Creation\n",
    "\n",
    "                train_pool = Pool(\n",
    "                    data = X_train, label = Y_train, \n",
    "                    cat_features = cat_features_names\n",
    "                    )\n",
    "                test_pool = Pool(\n",
    "                    data = X_test, label = Y_test, \n",
    "                    cat_features = cat_features_names\n",
    "                    )\n",
    "                \n",
    "                catboost_model = CatBoostClassifier(\n",
    "                    iterations = 10,\n",
    "                    learning_rate = 1,\n",
    "                    depth = 8\n",
    "                )\n",
    "                print(\"Model To Train\")\n",
    "                catboost_model.fit(train_pool)\n",
    "                print(\"Model trained\")\n",
    "                yhat_test = catboost_model.predict(test_pool)\n",
    "                yhat_train = catboost_model.predict(train_pool)\n",
    "                print(\"Y hat obtained\")\n",
    "                accuracy_score_train = accuracy_score(Y_train, yhat_train)\n",
    "                accuracy_score_test = accuracy_score(Y_test, yhat_test)\n",
    "                ic(accuracy_score_train)\n",
    "                ic(accuracy_score_test)\n",
    "            \n",
    "                return catboost_model, accuracy_score_train, accuracy_score_test\n",
    "\n",
    "        def Save_CatBoost(catboost_model, pilot_name, measurement_name, asset_name):\n",
    "            catboost_model.save_model(\"/tmp/catboost_model.cbm\", format = \"cbm\")\n",
    "            s3 = boto3.resource(\n",
    "                service_name='s3',\n",
    "                aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                endpoint_url='https://s3.tebi.io'\n",
    "            )\n",
    "\n",
    "            for bucket in s3.buckets.all():\n",
    "                ic(bucket.name)\n",
    "            \n",
    "            # Upload a new file\n",
    "            data = open('/tmp/catboost_model.cbm', 'rb')\n",
    "            f_name = \"model_{pilot}_{domain}_{asset}_latest_catboost.cbm\"\\\n",
    "            .format(pilot = pilot_name,domain = measurement_name, asset = asset_name)\n",
    "            s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Model sent to tebi for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "        def Predict_CatBoost(catboost_model, data_ds, last_cummulative_value, num_days, measures_per_hour, diff_time, cat_features_names, names_prevs_vars):\n",
    "            data_ds[\"yhat\"] = last_cummulative_value\n",
    "            for day_ in range(num_days):\n",
    "                for i in tqdm(range(24)):\n",
    "                    for j in range(measures_per_hour):\n",
    "                        last_row = data_ds.iloc[-1]\n",
    "                        ds_obj = maya.parse(last_row[\"ds\"]).add(minutes = int(diff_time))\n",
    "                        dict_input = {\n",
    "                            \"year\": ds_obj.year,\n",
    "                            \"month\": ds_obj.month,\n",
    "                            \"weekday\": ds_obj.weekday,\n",
    "                            \"hour\": ds_obj.hour\n",
    "                        }\n",
    "                        for k_var in names_prevs_vars:\n",
    "                            k = int(k_var[9:])\n",
    "                            if k == 1:\n",
    "                                dict_input[k_var] = str(last_row[\"y\"])\n",
    "                            else:\n",
    "                                dict_input[k_var] = str(last_row[\"prev_val_{i}\".format(i = k -1)])\n",
    "                        dict_input = [dict_input]\n",
    "                        data_input = pd.DataFrame(dict_input)\n",
    "                        pred_pool = Pool(\n",
    "                            data = data_input, \n",
    "                            cat_features = cat_features_names\n",
    "                            )\n",
    "                        pred_value = catboost_model.predict(pred_pool)[0][0]\n",
    "                        dict_input[0][\"y\"] = pred_value\n",
    "                        dict_input[0][\"ds\"] = datetime.strftime(ds_obj.datetime(), \"%Y-%m-%d %H:%M:%S\")\n",
    "                        dict_input[0][\"yhat\"] = last_row[\"yhat\"] + float(pred_value)\n",
    "                        data_add = pd.DataFrame(dict_input)\n",
    "                        data_ds = pd.concat([data_ds, data_add],ignore_index = True)\n",
    "            return data_ds\n",
    "\n",
    "        def Metrics_CatBoost(accuracy_score_train, accuracy_score_test, metrics_list):\n",
    "            metrics = {\n",
    "                'metrics': [\n",
    "                    {\n",
    "                    'name': 'accuracy_train',\n",
    "                    'numberValue':  float(accuracy_score_train),\n",
    "                    'format': \"PERCENTAGE\"\n",
    "                    },\n",
    "                    {\n",
    "                        'name': 'accuracy_test',\n",
    "                        \"numberValue\": float(accuracy_score_test),\n",
    "                        \"format\": \"PERCENTAGE\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"asset_number\",\n",
    "                        \"numberValue\": asset_name,\n",
    "                        \"format\": \"RAW\"\n",
    "                    }\n",
    "                ]}  \n",
    "            \n",
    "            metrics_list.append(metrics)\n",
    "            return metrics_list\n",
    "\n",
    "        # Prophet Functions\n",
    "        \n",
    "        def ManageDateTime(ds_obj):\n",
    "            return datetime.strftime(maya.parse(ds_obj).datetime(), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        def ManageDateMinute(ds_obj):\n",
    "            return datetime.strftime(maya.parse(ds_obj).datetime(), \"%Y-%m-%d %H:%M:00\")\n",
    "        def ManageDateHour(ds_obj):\n",
    "            return datetime.strftime(maya.parse(ds_obj).datetime(), \"%Y-%m-%d %H:00:00\")\n",
    "\n",
    "        def Train_Prophet(train_data, num_days, measures_per_hour, diff_time, weather_data, mode_prophet, daily_seasonality, weekly_seasonality):\n",
    "            from prophet import Prophet\n",
    "\n",
    "            train_data[\"ds\"] = train_data[\"ds\"].apply(ManageDateMinute)\n",
    "            train_data = train_data[[\"ds\", \"y\"]].groupby(\"ds\").mean().reset_index(level = \"ds\")\n",
    "            train_data[\"ds_hour\"] = train_data[\"ds\"].apply(ManageDateHour)\n",
    "            weather_data[\"ds_hour\"] = weather_data[\"ds_hour\"].apply(str)\n",
    "\n",
    "            ic(train_data[\"ds_hour\"].tolist()[-20:])\n",
    "            ic(weather_data[\"ds_hour\"].tolist()[-20:])\n",
    "            train_data = pd.merge(train_data, weather_data, on = \"ds_hour\")\n",
    "\n",
    "            ic(train_data.shape[0])\n",
    "\n",
    "            min_date = maya.parse(min(train_data.ds))\n",
    "            max_date = maya.parse(max(train_data.ds))\n",
    "            days_train = (max_date - min_date).days\n",
    "\n",
    "            if days_train >= 365:\n",
    "                yearly_seasonality = True\n",
    "            else:\n",
    "                yearly_seasonality = False\n",
    "            # Define Model to be trained\n",
    "            m = Prophet(daily_seasonality=daily_seasonality, weekly_seasonality=weekly_seasonality, yearly_seasonality = yearly_seasonality,changepoint_prior_scale = 0.05, seasonality_mode=mode_prophet)\n",
    "\n",
    "            m.add_regressor('shortwave_radiation')\n",
    "            m.add_regressor('temperature_2m')\n",
    "            m.add_regressor(\"direct_radiation\")\n",
    "            m.add_regressor(\"diffuse_radiation\")\n",
    "            m.add_regressor(\"direct_normal_irradiance\")\n",
    "\n",
    "            # Train Model\n",
    "            m.fit(train_data)\n",
    "            future = m.make_future_dataframe(periods= 24*(3 + num_days)*measures_per_hour , freq=\"{minutes}T\".format(minutes = diff_time))\n",
    "            future[\"ds\"] = future[\"ds\"].apply(str)\n",
    "            future[\"ds_hour\"] = future[\"ds\"].apply(ManageDateHour)\n",
    "            future = pd.merge(future, weather_data, on = \"ds_hour\")\n",
    "            \n",
    "            forecast = m.predict(future)\n",
    "\n",
    "            print(forecast.tail(5))\n",
    "\n",
    "            return forecast, m\n",
    "\n",
    "        def GetMetricsProphet(forecast,train_data, test_data, num_days, dict_asset, metrics_list, date_train):\n",
    "            try:\n",
    "                asset_number = dict_asset[asset_name]\n",
    "            except:\n",
    "                asset_number = 3\n",
    "            \n",
    "            forecast[\"ds\"] = forecast[\"ds\"].apply(str)\n",
    "\n",
    "            try:\n",
    "                forecast_test = forecast[forecast.ds >= date_train][\"yhat\"].tolist()\n",
    "                train_data = pd.merge(train_data, forecast[[\"ds\", \"yhat\"]], on = \"ds\")\n",
    "                real_vals_train = train_data[\"y\"].tolist()\n",
    "                forecast_train = train_data[\"yhat\"].tolist()\n",
    "                r2_score_train = r2_score(real_vals_train, forecast_train)\n",
    "                mae_score_train = mean_absolute_error(real_vals_train, forecast_train)\n",
    "            except:\n",
    "                r2_score_train = 0\n",
    "                mae_score_train = 0\n",
    "            \n",
    "            real_vals_test = test_data[\"y\"].tolist()\n",
    "\n",
    "            if len(forecast_test) == len(real_vals_test):\n",
    "                r2_score_test = r2_score(real_vals_test, forecast_test)\n",
    "                mae_score_test = mean_absolute_error(real_vals_test, forecast_test)\n",
    "                metrics = {\n",
    "                    'metrics': [\n",
    "                        {\n",
    "                        'name': 'r2_score_test',\n",
    "                        'numberValue':  float(r2_score_test),\n",
    "                        'format': \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            'name': 'r2_score_train',\n",
    "                            \"numberValue\": float(r2_score_train),\n",
    "                            \"format\": \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"asset_number\",\n",
    "                            \"numberValue\": asset_name,\n",
    "                            \"format\": \"RAW\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"mae_train\",\n",
    "                            \"numberValue\": mae_score_train,\n",
    "                            \"format\": \"RAW\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"mae_test\",\n",
    "                            \"numberValue\": mae_score_test,\n",
    "                            \"format\": \"RAW\"\n",
    "                        }\n",
    "                    ]}  \n",
    "                \n",
    "                metrics_list.append(metrics)\n",
    "                \n",
    "            else:\n",
    "                ic(len(forecast_test))\n",
    "                ic(len(real_vals_test))\n",
    "            \n",
    "            return metrics_list\n",
    "\n",
    "        def SaveModelProphet(model, measurement_name, asset_name, pilot_name):\n",
    "            with open(\"/tmp/model_prophet.json\", 'w') as fout:\n",
    "                fout.write(model_to_json(model))  # Save model\n",
    "            \n",
    "\n",
    "            date = maya.when(\"now\").rfc2822()\n",
    "            f_name = \"model_{domain}_{asset}.json\"\\\n",
    "                .format(domain = measurement_name, asset = asset_name)\n",
    "            try:\n",
    "                result = client.fput_object(\n",
    "                    \"test\", f_name, \"/tmp/model_prophet.json\"\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    \"created {0} object; etag: {1}, version-id: {2}\".format(\n",
    "                        result.object_name, result.etag, result.version_id,\n",
    "                    ),\n",
    "                )\n",
    "            except:\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model not saved for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "                s3 = boto3.resource(\n",
    "                    service_name='s3',\n",
    "                    aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                    aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                    endpoint_url='https://s3.tebi.io'\n",
    "                )\n",
    "\n",
    "                for bucket in s3.buckets.all():\n",
    "                    ic(bucket.name)\n",
    "                \n",
    "                # Upload a new file\n",
    "                data = open('/tmp/model_prophet.json', 'rb')\n",
    "                f_name = \"model_{pilot}_{domain}_{asset}_latest_prophet.json\"\\\n",
    "                .format(pilot = pilot_name,domain = measurement_name, asset = asset_name)\n",
    "                s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model sent to tebi for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "        # LSTM Functions\n",
    "        def Train_LSTM(data, split_proportion, diff_time, num_days=1, \n",
    "                    measures_per_hour=1, n_epochs=100, batch_size=16):\n",
    "            \n",
    "            \n",
    "\n",
    "            if not isinstance(data, pd.DataFrame) or 'ds' not in data.columns or 'y' not in data.columns:\n",
    "                raise ValueError(\"The input data must be a pandas DataFrame with 'ds' and 'y' columns.\")\n",
    "\n",
    "            if not (0 < split_proportion < 1):\n",
    "                raise ValueError(\"The split_proportion must be a float between 0 and 1.\")\n",
    "\n",
    "            # fill missing values with the last available value\n",
    "            data = data.fillna(method='ffill')\n",
    "            \n",
    "            # Create a time series\n",
    "            series = TimeSeries.from_dataframe(data, 'ds', 'y',fill_missing_dates=True, freq=\"{minutes}T\".format(minutes = diff_time))\n",
    "\n",
    "            # Create training and validation sets:\n",
    "            train, val = series.split_after(pd.Timestamp(series.start_time() + pd.Timedelta(hours=int(len(series) * split_proportion))))\n",
    "\n",
    "            # Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
    "            transformer = Scaler()\n",
    "            train_transformed = transformer.fit_transform(train)\n",
    "            val_transformed = transformer.transform(val)\n",
    "            series_transformed = transformer.transform(series)\n",
    "\n",
    "            # predict *num_days* days ahead\n",
    "            pred_ahead = 24 * (2 + num_days) * measures_per_hour\n",
    "\n",
    "            my_model = RNNModel(\n",
    "                input_chunk_length=2 * pred_ahead,\n",
    "                model=\"LSTM\",\n",
    "                hidden_dim=25, \n",
    "                n_rnn_layers=1,\n",
    "                dropout=0.2,\n",
    "                training_length=pred_ahead,\n",
    "                batch_size=batch_size,\n",
    "                n_epochs=n_epochs,\n",
    "                optimizer_kwargs={\"lr\": 1e-3},\n",
    "                model_name=\"data_RNN\",\n",
    "                log_tensorboard=True,\n",
    "                random_state=42,\n",
    "                force_reset=True,\n",
    "                save_checkpoints=True,\n",
    "            )\n",
    "\n",
    "            my_model.fit(\n",
    "                train_transformed,\n",
    "                val_series=val_transformed,\n",
    "                verbose=True,\n",
    "            )\n",
    "            \n",
    "            historical_forecast = my_model.historical_forecasts(\n",
    "                                            series_transformed,\n",
    "                                            start=pd.Timestamp(val.start_time() - pd.Timedelta(hours=1)),\n",
    "                                            forecast_horizon=pred_ahead,\n",
    "                                            retrain=False,\n",
    "                                            verbose=True,\n",
    "                                        )\n",
    "            \n",
    "            historical_forecast = transformer.inverse_transform(historical_forecast)\n",
    "            historical_forecast = historical_forecast.pd_dataframe().reset_index()\n",
    "            # rename columns\n",
    "            historical_forecast.columns = ['ds', 'y']\n",
    "            historical_forecast.columns.name = None\n",
    "\n",
    "            # Predict\n",
    "            forecast = my_model.predict(n=pred_ahead, series=val_transformed)\n",
    "\n",
    "            # Inverse-transform forecasts and obtain the real predicted values\n",
    "            forecast = transformer.inverse_transform(forecast)\n",
    "            forecast = forecast.pd_dataframe().reset_index()\n",
    "            forecast.columns.name = None\n",
    "\n",
    "            forecast = pd.concat([historical_forecast, forecast], axis=0).reset_index(drop=True)\n",
    "\n",
    "            # Check the dataframe if the frequency is always {diff_time} minute\n",
    "            full_range = pd.date_range(forecast['ds'].iloc[0], forecast['ds'].iloc[-1], freq=\"{minutes}T\".format(minutes = diff_time))\n",
    "            assert full_range.difference(forecast['ds']).shape[0] == 0\n",
    "\n",
    "            forecast_test = forecast[\"y\"].tolist()[-24*measures_per_hour*(2 + num_days):-24*measures_per_hour]\n",
    "\n",
    "            return forecast, forecast_test, my_model\n",
    "        \n",
    "        def GetMetricsLSTM(forecast, forecast_test, train_data, test_data, num_days, dict_assets, metrics_list):\n",
    "            try:\n",
    "                asset_number = dict_assets[asset_name]\n",
    "            except:\n",
    "                asset_number = 3\n",
    "            try:\n",
    "                # take the comman ds for forecast and train_data\n",
    "                forecast_train = forecast[forecast['ds'].isin(train_data['ds'])].reset_index(drop=True)\n",
    "                real_vals_train = train_data[train_data['ds'].isin(forecast['ds'])].reset_index(drop=True)\n",
    "                r2_score_train = r2_score(real_vals_train['y'].to_list(), forecast_train['y'].to_list())\n",
    "                mae_score_train = mean_absolute_error(real_vals_train['y'].to_list(), forecast_train['y'].to_list())\n",
    "            except:\n",
    "                r2_score_train = 0\n",
    "            \n",
    "            real_vals_test = test_data[\"y\"].tolist()\n",
    "\n",
    "            if len(forecast_test) == len(real_vals_test):\n",
    "                r2_score_test = r2_score(real_vals_test, forecast_test)\n",
    "                mae_score_test = mean_absolute_error(real_vals_test, forecast_test)\n",
    "                metrics = {\n",
    "                    'metrics': [\n",
    "                        {\n",
    "                        'name': 'r2_score_test',\n",
    "                        'numberValue':  float(r2_score_test),\n",
    "                        'format': \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            'name': 'r2_score_train',\n",
    "                            \"numberValue\": float(r2_score_train),\n",
    "                            \"format\": \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"asset_number\",\n",
    "                            \"numberValue\": asset_name,\n",
    "                            \"format\": \"RAW\"\n",
    "                        },\n",
    "                         {\n",
    "                            \"name\": \"mae_train\",\n",
    "                            \"numberValue\": mae_score_train,\n",
    "                            \"format\": \"RAW\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"mae_test\",\n",
    "                            \"numberValue\": mae_score_test,\n",
    "                            \"format\": \"RAW\"\n",
    "                        }\n",
    "                    ]}  \n",
    "                \n",
    "                metrics_list.append(metrics)\n",
    "                \n",
    "            else:\n",
    "                ic(len(forecast_test))\n",
    "                ic(len(real_vals_test))\n",
    "            return metrics_list\n",
    "\n",
    "        def SaveModelLSTM(model, measurement_name, asset_name, pilot_name):\n",
    "            model.save(\"/tmp/lstm_model.pt\")\n",
    "            # model_loaded = RNNModel.load(\"/tmp/lstm_model.pt\")\n",
    "            \n",
    "            date = maya.when(\"now\").rfc2822()\n",
    "            f_name = \"model_{domain}_{asset}.pt\"\\\n",
    "                .format(domain = measurement_name, asset = asset_name)\n",
    "            try:\n",
    "                result = client.fput_object(\n",
    "                    \"test\", f_name, \"/tmp/lstm_model.pt\"\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    \"created {0} object; etag: {1}, version-id: {2}\".format(\n",
    "                        result.object_name, result.etag, result.version_id,\n",
    "                    ),\n",
    "                )\n",
    "            except:\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model not saved for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "                s3 = boto3.resource(\n",
    "                    service_name='s3',\n",
    "                    aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                    aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                    endpoint_url='https://s3.tebi.io'\n",
    "                )\n",
    "\n",
    "                for bucket in s3.buckets.all():\n",
    "                    ic(bucket.name)\n",
    "                \n",
    "                # Upload a new file\n",
    "                data = open('/tmp/lstm_model.pt', 'rb')\n",
    "                f_name = \"model_{pilot}_{domain}_{asset}_latest_lstm.pt\"\\\n",
    "                .format(pilot = pilot_name,domain = measurement_name, asset = asset_name)\n",
    "                s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model sent to tebi for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "\n",
    "        ############\n",
    "\n",
    "        print(\"Modifying Data\")\n",
    "        print(maya.now().rfc2822())\n",
    "\n",
    "        data_ds, max_date, last_cummulative_value = ModifyData(data, asset_name)\n",
    "\n",
    "        print(\"Start Training\")\n",
    "        \n",
    "        if len(pd.unique(data_ds.y)) >= 20:\n",
    "\n",
    "            date_train = datetime.strftime(maya.parse(max_date).add(days = -1).datetime(), \"%Y-%m-%d\")\n",
    "            train_data = data_ds[data_ds.ds < date_train]\n",
    "            test_data = data_ds[data_ds.ds >= date_train]\n",
    "\n",
    "            # Train Prophet\n",
    "            print(\"Training Prophet\")\n",
    "            print(maya.now().rfc2822())\n",
    "            forecast_prophet, model = Train_Prophet(train_data, num_days, measures_per_hour, diff_time, weather_data, mode_prophet, daily_seasonality, weekly_seasonality)\n",
    "            metrics_list_prophet = GetMetricsProphet(forecast_prophet,train_data, test_data, num_days, dict_assets, metrics_list, date_train)\n",
    "            \n",
    "            # Train LSTM\n",
    "            print(\"Training LSTM\")\n",
    "            print(maya.now().rfc2822())\n",
    "\n",
    "            try:\n",
    "                ic(diff_time)\n",
    "                forecast_lstm, forecast_test, model = Train_LSTM(data=train_data, split_proportion=0.9, diff_time=diff_time,\n",
    "                                                            num_days=num_days, measures_per_hour=measures_per_hour, \n",
    "                                                            n_epochs=25)\n",
    "                metrics_list_lstm = GetMetricsLSTM(forecast_lstm, forecast_test, train_data, test_data, num_days, dict_assets, metrics_list)\n",
    "            except ValueError:\n",
    "                metrics_list_lstm = [{\n",
    "                    \"name\": \"mae_test\",\n",
    "                    \"numberValue\": -1\n",
    "                }]\n",
    "            print(\"Finish Training\")\n",
    "            print(maya.now().rfc2822())\n",
    "            ############\n",
    "\n",
    "            # Compare Models\n",
    "\n",
    "            forecast = forecast_prophet\n",
    "            metrics_list = metrics_list_prophet\n",
    "            \n",
    "            print(\"Metrics Prophet\")\n",
    "\n",
    "            for metric in metrics_list:\n",
    "                try:\n",
    "                    print(metric[\"name\"])\n",
    "                    print(metric[\"numberValue\"])\n",
    "                except:\n",
    "                    print(metrics_list)\n",
    "                    break\n",
    "\n",
    "            print(\"Metrics LSTM\")\n",
    "\n",
    "            for metric in metrics_list_lstm:\n",
    "                try:\n",
    "                    print(metric[\"name\"])\n",
    "                    print(metric[\"numberValue\"])\n",
    "                except:\n",
    "                    print(metrics_list)\n",
    "                    break\n",
    "\n",
    "            ###########\n",
    "            \n",
    "            SaveModelProphet(model, measurement_name, asset_name, pilot_name)\n",
    "\n",
    "            try:\n",
    "                SaveModelLSTM(model, measurement_name, asset_name, pilot_name)\n",
    "            except:\n",
    "                print(\"LSTM Model Not Saved\")\n",
    "\n",
    "        elif data_ds.shape[0] < 10:\n",
    "            print(\"Not enough values\")\n",
    "            forecast = data_ds\n",
    "\n",
    "        else:\n",
    "            data_ds, names_prevs_vars, cat_features_names = ManageData(data_ds)\n",
    "            \n",
    "            catboost_model, accuracy_score_train, accuracy_score_test = Train_CatBoost(data_ds, cat_features_names)\n",
    "\n",
    "            forecast = Predict_CatBoost(catboost_model,\n",
    "                            data_ds, \n",
    "                            last_cummulative_value,\n",
    "                            num_days, measures_per_hour, diff_time,\n",
    "                            cat_features_names, names_prevs_vars)\n",
    "\n",
    "            Save_CatBoost(catboost_model, pilot_name, measurement_name, asset_name)\n",
    "            metrics_list = Metrics_CatBoost(accuracy_score_train, accuracy_score_test,\n",
    "                                            metrics_list)\n",
    "\n",
    "        ic(metrics_list)\n",
    "        return forecast[forecast.ds > max_date], metrics_list\n",
    "\n",
    "    \n",
    "    # Get Parameters\n",
    "    \n",
    "    dict_assets = {}\n",
    "    measures_per_hour = int(60/int(diff_time))\n",
    "    time_prediction = maya.now().epoch\n",
    "    num_days = int(num_days)\n",
    "\n",
    "\n",
    "    with open(input_data_path) as file:\n",
    "        data_str = json.load(file)\n",
    "    \n",
    "    data = pd.DataFrame(data_str)\n",
    "    metrics_list = []\n",
    "    ic(asset_name)\n",
    "\n",
    "    weather_data = pd.read_feather(input_weather_path)\n",
    "    \n",
    "    class TestError(Exception):\n",
    "        \"Input correctly City name or Lat AND Lon for city\"\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        ic(diff_time)\n",
    "        forecasted_data, metrics_list = ForecastData(data, asset_name, measurement_name, metrics_list, measures_per_hour, diff_time, weather_data, mode_prophet, daily_seasonality, weekly_seasonality)\n",
    "    except TestError:\n",
    "        forecasted_dict = {\n",
    "            \"ds\": [],\n",
    "            \"yhat\": []\n",
    "        }\n",
    "        forecasted_data = pd.DataFrame(forecasted_dict)\n",
    "        metrics_list = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        forecasted_data.to_csv('/tmp/forecast_test_{asset_name}.csv'.format(asset_name = asset_name), index = False)\n",
    "        data_to_send = open('/tmp/forecast_test_{asset_name}.csv'.format(asset_name = asset_name), 'rb')\n",
    "        f_name = \"forecast_test_{pilot}_{asset_name}.csv\".format(pilot = pilot_name, asset_name = asset_name)\n",
    "        s3 = boto3.resource(\n",
    "            service_name='s3',\n",
    "            aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "            aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "            endpoint_url='https://s3.tebi.io'\n",
    "        )\n",
    "        s3.Bucket('test-pf').put_object(Key=f_name, Body=data_to_send)\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Data File: {f_name} Saved to Tebi\".format(f_name = f_name)\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "    except:\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Unable to save data to tebi\"\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "\n",
    "        message = \"Values for {asset_name}: {list_values}\".format(asset_name = asset_name,list_values = forecasted_data.yhat.tolist())\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "\n",
    "    forecasted_data[\"yhat\"] = forecasted_data[\"yhat\"].apply(lambda x : max(x,0))\n",
    "\n",
    "    forecasted_data.to_csv(forecast_data_path, index = False)\n",
    "\n",
    "\n",
    "    domain_ = \"electricity\"\n",
    "\n",
    "    with open(\"/tmp/metrics_{domain}.json\".format(domain = domain_), \"w\") as file:\n",
    "        json.dump(metrics_list, file)\n",
    "    \n",
    "    \n",
    "\n",
    "    s3 = boto3.resource(\n",
    "                service_name='s3',\n",
    "                aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                endpoint_url='https://s3.tebi.io'\n",
    "            )\n",
    "    data = open(\"/tmp/metrics_{domain}.json\".format(domain = domain_), 'rb')\n",
    "    f_name = \"metrics_{domain}_latest.json\"\\\n",
    "    .format(domain = measurement_name)\n",
    "    s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "    \n",
    "\n",
    "\n",
    "    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "    message = \"Forecasting done for {domain} and asset name : {asset_name}\".format(domain = domain_, asset_name = asset_name)\n",
    "    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "    webhook.execute()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckSendForecast(send_forecast:str) -> bool:\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "    message = \"Forecast not send, the option is not 'yes' or 'no', please check this, the option sent was{option}\".format(option = send_forecast)\n",
    "    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "    \n",
    "    if send_forecast == \"yes\":\n",
    "        return True\n",
    "    elif send_forecast == \"no\":\n",
    "        return False\n",
    "    else:\n",
    "        webhook.execute()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SendForecast(input_forecast_data_path: InputPath(str),url_pilot:str, pilot_name:str, asset_name : str, measurement_name: str, key_measurement: str, num_days):\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from icecream import ic\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import maya\n",
    "    from tqdm import tqdm\n",
    "    from datetime import datetime\n",
    "\n",
    "    from urllib3.exceptions import InsecureRequestWarning\n",
    "    import warnings\n",
    "    import contextlib\n",
    "\n",
    "    ## Function Definition ## \n",
    "\n",
    "    def GetRequest(url, headers ={}, payload = {}):\n",
    "\n",
    "        old_merge_environment_settings = requests.Session.merge_environment_settings\n",
    "\n",
    "        @contextlib.contextmanager\n",
    "        def no_ssl_verification():\n",
    "            opened_adapters = set()\n",
    "\n",
    "            def merge_environment_settings(self, url, proxies, stream, verify, cert):\n",
    "                # Verification happens only once per connection so we need to close\n",
    "                # all the opened adapters once we're done. Otherwise, the effects of\n",
    "                # verify=False persist beyond the end of this context manager.\n",
    "                opened_adapters.add(self.get_adapter(url))\n",
    "\n",
    "                settings = old_merge_environment_settings(self, url, proxies, stream, verify, cert)\n",
    "                settings['verify'] = False\n",
    "\n",
    "                return settings\n",
    "\n",
    "            requests.Session.merge_environment_settings = merge_environment_settings\n",
    "\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "                    yield\n",
    "            finally:\n",
    "                requests.Session.merge_environment_settings = old_merge_environment_settings\n",
    "\n",
    "                for adapter in opened_adapters:\n",
    "                    try:\n",
    "                        adapter.close()\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        with no_ssl_verification():\n",
    "            response = requests.request(\"GET\", url, headers = headers, data = payload)\n",
    "            \n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            dict_ = {\n",
    "                \"status_code\": response.status_code,\n",
    "                \"text\": response.text\n",
    "            }\n",
    "            return dict_\n",
    "\n",
    "    def GetFeaturesMeasurement(dicts_measurements, dicts_assets, asset_name, measurement_name):\n",
    "        domain_ = \"None\"\n",
    "        direction = \"None\"\n",
    "        type_ = \"None\"\n",
    "\n",
    "        for dict_ in dicts_measurements:\n",
    "            if dict_[\"name\"] == measurement_name:\n",
    "                try:\n",
    "                    if dict_[\"asset\"][\"name\"] == asset_name:\n",
    "                        print(\"Asset Found\")\n",
    "                        domain_ = dict_[\"domain\"]\n",
    "                        direction = dict_[\"direction\"]\n",
    "                        type_ = dict_[\"type\"][\"name\"]\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if domain_ == \"None\" and direction == \"None\" and type_ == \"None\":\n",
    "            for dict_ in dicts_assets:\n",
    "                if dict_[\"name\"] == asset_name:\n",
    "                    for meas in dict_[\"measurements\"]:\n",
    "                        if meas[\"sensor_name\"] == measurement_name:\n",
    "                            print(\"Measurement Found\")\n",
    "                            domain_ = meas[\"domain\"]\n",
    "                            direction = meas[\"direction\"]\n",
    "                            type_ = meas[\"type\"][\"name\"]\n",
    "\n",
    "                            break\n",
    "\n",
    "        \n",
    "        return domain_, direction, type_\n",
    "\n",
    "    def GetMeasurementInfo(pilot_name, url_pilot, measurement_name, asset_name):\n",
    "        if pilot_name != \"Virtual\":\n",
    "            url_measurements = \"{url_pilot}/api-postgre/1.0/api/measurements\".format(\n",
    "                        url_pilot = url_pilot\n",
    "                    )\n",
    "            url_assets = \"{url_pilot}/api-postgre/1.0/api/assets\".format(\n",
    "                        url_pilot = url_pilot\n",
    "                    )\n",
    "            \n",
    "            dict_measurement = GetRequest(url_measurements)\n",
    "            dict_asset = GetRequest(url_assets)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "\n",
    "            dict_asset= [\n",
    "                        {\"name\": \"building1\", \"measurements\":[{\"sensor_name\": \"heat_meter\", \"domain\": \"heat\",\"direction\":\"in\", \"type\":{\"name\": \"power_wh\"}}, \n",
    "                                                                {\"sensor_name\": \"electricity_meter\", \"domain\": \"electricity\",\"direction\":\"in\", \"type\":{\"name\": \"power_wh\"}}]},\n",
    "                        {\"name\": \"building2\", \"measurements\":[{\"sensor_name\": \"heat_meter\", \"domain\": \"heat\",\"direction\":\"in\", \"type\":{\"name\": \"power_wh\"}}, \n",
    "                                                                {\"sensor_name\": \"electricity_meter\", \"domain\": \"electricity\",\"direction\":\"in\", \"type\":{\"name\": \"power_wh\"}}]},\n",
    "                        {\"name\": \"gas_boiler1\", \"measurements\":[{\"sensor_name\": \"heat_meter\", \"domain\": \"heat\",\"direction\":\"out\", \"type\":{\"name\": \"power_wh\"}}]},\n",
    "                        {\"name\": \"gas_boiler2\", \"measurements\":[{\"sensor_name\": \"heat_meter\", \"domain\": \"heat\",\"direction\":\"out\", \"type\":{\"name\": \"power_wh\"}}]},\n",
    "                        {\"name\": \"cogenerator1\", \"measurements\":[{\"sensor_name\": \"electricity_meter\", \"domain\": \"electricity\",\"direction\":\"out\", \"type\":{\"name\": \"power_wh\"}}]},\n",
    "                        {\"name\": \"cogenerator2\", \"measurements\":[{\"sensor_name\": \"electricity_meter\", \"domain\": \"electricity\",\"direction\":\"out\", \"type\":{\"name\": \"power_wh\"}}]},\n",
    "                        {\"name\": \"wind_farm_1\", \"measurements\":[{\"sensor_name\": \"electricity_meter\", \"domain\": \"electricity\",\"direction\":\"out\", \"type\":{\"name\": \"power_wh\"}}]},\n",
    "                        {\"name\": \"pv_panels_1\", \"measurements\":[{\"sensor_name\": \"electricity_meter\", \"domain\": \"electricity\",\"direction\":\"out\", \"type\":{\"name\": \"power_wh\"}}]},\n",
    "                        {\"name\": \"solar_collector1\", \"measurements\":[{\"sensor_name\": \"heat_meter\", \"domain\": \"heat\",\"direction\":\"out\", \"type\":{\"name\": \"power_wh\"}}]}\n",
    "\n",
    "                    ]\n",
    "            dict_measurement = []\n",
    "\n",
    "        return GetFeaturesMeasurement(dict_measurement, dict_asset, asset_name, measurement_name)\n",
    " \n",
    "    def GetPostData(time_, value, \n",
    "                    measurement_name, \n",
    "                    asset_name,  domain_,\n",
    "                    direction_energy, type_, \n",
    "                    time_prediction):\n",
    "\n",
    "        data_post = {\n",
    "                \"bucket\": \"renergetic\",\n",
    "                \"measurement\": measurement_name,\n",
    "                \"fields\":{\n",
    "                    key_measurement: value,\n",
    "                    \"time\": time_,\n",
    "                },\n",
    "                \"tags\":{\n",
    "                    \"domain\": domain_,\n",
    "                    \"type_data\": \"forecasting\",\n",
    "                    \"direction\": direction_energy,\n",
    "                    \"prediction_window\": \"{hours}h\".format(hours = int(num_days) * 24),\n",
    "                    \"asset_name\": asset_name,\n",
    "                    \"measurement_type\": type_,\n",
    "                    \"time_prediction\": time_prediction\n",
    "                }\n",
    "            }\n",
    "        return data_post\n",
    "    \n",
    "    def PostData(data_post, url_pilot, pilot_name):\n",
    "        from urllib3.exceptions import InsecureRequestWarning\n",
    "        import warnings\n",
    "        import contextlib\n",
    "\n",
    "        old_merge_environment_settings = requests.Session.merge_environment_settings\n",
    "\n",
    "        @contextlib.contextmanager\n",
    "        def no_ssl_verification():\n",
    "            opened_adapters = set()\n",
    "\n",
    "            def merge_environment_settings(self, url, proxies, stream, verify, cert):\n",
    "                # Verification happens only once per connection so we need to close\n",
    "                # all the opened adapters once we're done. Otherwise, the effects of\n",
    "                # verify=False persist beyond the end of this context manager.\n",
    "                opened_adapters.add(self.get_adapter(url))\n",
    "\n",
    "                settings = old_merge_environment_settings(self, url, proxies, stream, verify, cert)\n",
    "                settings['verify'] = False\n",
    "\n",
    "                return settings\n",
    "\n",
    "            requests.Session.merge_environment_settings = merge_environment_settings\n",
    "\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "                    yield\n",
    "            finally:\n",
    "                requests.Session.merge_environment_settings = old_merge_environment_settings\n",
    "\n",
    "                for adapter in opened_adapters:\n",
    "                    try:\n",
    "                        adapter.close()\n",
    "                    except:\n",
    "                        pass\n",
    "        if pilot_name == \"Virtual\":\n",
    "            url = \"http://influx-api-ren-prototype.apps.paas-dev.psnc.pl/api/measurement\"\n",
    "        else:\n",
    "            url = url_pilot + \"/api-measurement/1.0/api/measurement\"\n",
    "\n",
    "        with no_ssl_verification():\n",
    "            response = requests.request(\"POST\", url, headers=headers, data=json.dumps(data_post))\n",
    "        \n",
    "        return response.status_code\n",
    "\n",
    "\n",
    "    class TestError(Exception):\n",
    "        \"This error shows to avoid the completion of the task for test purposes\"\n",
    "        pass\n",
    "    \n",
    "    ## Procedure ##\n",
    "\n",
    "    \n",
    "\n",
    "    forecasted_data = pd.read_csv(input_forecast_data_path)\n",
    "    domain_, direction_, type_ = GetMeasurementInfo(pilot_name, url_pilot,measurement_name, asset_name)\n",
    "    ic(type_)\n",
    "    ic(direction_)\n",
    "    ic(domain_)\n",
    "    time_prediction = datetime.strftime(maya.now().datetime(), \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    if measurement_name == \"electricity_meter\":\n",
    "        domain_ = \"electricity\"\n",
    "    elif measurement_name == \"heat_meter\":\n",
    "        domain_ = \"heat\"\n",
    "    \n",
    "    values_ok = []\n",
    "    values_not_ok = []\n",
    "\n",
    "    for index, row in tqdm(forecasted_data.iterrows(), total = forecasted_data.shape[0]):\n",
    "        time_ = str(row[\"ds\"])\n",
    "        value = row[\"yhat\"]\n",
    "\n",
    "        \n",
    "        data_post = GetPostData(time_, value, \n",
    "                                measurement_name= measurement_name,\n",
    "                                domain_ = domain_, asset_name= asset_name, direction_energy= direction_,\n",
    "                                type_ = type_, time_prediction= time_prediction)\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            status_code = PostData(data_post, url_pilot, pilot_name)\n",
    "\n",
    "        except:\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Error in updating value for measurement name: {measurement_name} in asset: {asset_name} in time {time_pred}\"\\\n",
    "                .format(measurement_name = \"electricity_meter\", asset_name = asset_name, time_pred = data_post[\"fields\"][\"time\"])\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "            status_code = 400\n",
    "        \n",
    "        if status_code > 299:\n",
    "            ic(time_)\n",
    "            ic(value)\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Error in sending the value for measurement name: {measurement_name} in asset: {asset_name} in time {time_pred}\"\\\n",
    "                .format(measurement_name = measurement_name, asset_name = asset_name, time_pred = data_post[\"fields\"][\"time\"])\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "            if len(values_not_ok) == 0:\n",
    "                print(data_post)\n",
    "                print(url_pilot)\n",
    "            \n",
    "            values_not_ok.append(time_)\n",
    "\n",
    "        else:\n",
    "            values_ok.append(time_)\n",
    "        \n",
    "    ic(values_ok)\n",
    "    ic(values_not_ok)\n",
    "    ic(data_post)\n",
    "    ic(url_pilot)\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckSendNotification(send_notifications_check:str) -> bool:\n",
    "    if send_notifications_check == \"no notifications\":\n",
    "        return False\n",
    "    else: \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SendNotification(forecast_data_path: InputPath(str), threshold_data_path: InputPath(str), asset_name, pilot_name, url_pilot):\n",
    "    \n",
    "    import json \n",
    "    import pandas as pd\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import maya\n",
    "    from datetime import datetime\n",
    "    import requests\n",
    "    import fuckit\n",
    "    from icecream import ic\n",
    "    import numpy as np\n",
    "\n",
    "    \n",
    "    # get notification code for anomaly high and low\n",
    "    @fuckit\n",
    "    def GetNotificationCodes(pilot_name, url_pilot):\n",
    "        if pilot_name == \"Virtual\":\n",
    "            url_notification_definition = \"http://api-ren-prototype.apps.paas-dev.psnc.pl/api/notification/definition\"\n",
    "        else:\n",
    "            url_notification_definition = \"{url_pilot}/api-postgre/1.0/api/notification/definition\".format(\n",
    "            url_pilot = url_pilot\n",
    "        )\n",
    "        payload={}\n",
    "        headers = {}\n",
    "\n",
    "        response = requests.request(\"GET\", url_notification_definition, headers=headers, data=payload)\n",
    "\n",
    "        \n",
    "\n",
    "        try:\n",
    "            dict_notifications = response.json()\n",
    "            if response.status_code > 299:\n",
    "                ic(url_notification_definition)\n",
    "                raise ValueError(\"The request was not successful\")\n",
    "        except:\n",
    "            return {}\n",
    "        \n",
    "        code_high = 0\n",
    "        code_low = 0\n",
    "\n",
    "        for notif in dict_notifications:\n",
    "            if notif[\"message\"] == \"message.anomaly.high\":\n",
    "                code_high = notif[\"code\"]\n",
    "            elif notif[\"message\"] == \"message.anomaly.low\":\n",
    "                code_low = notif[\"code\"]\n",
    "        codes = {\n",
    "            \"code_high\": code_high,\n",
    "            \"code_low\": code_low\n",
    "        }\n",
    "        return codes\n",
    "    \n",
    "    def ObtainCodes(codes):\n",
    "        if \"code_low\" in codes.keys():\n",
    "            code_low = codes[\"code_low\"]\n",
    "        else:\n",
    "            code_low = 0\n",
    "\n",
    "        if \"code_high\" in codes.keys():\n",
    "            code_high = codes[\"code_high\"]\n",
    "        else:\n",
    "            code_high = 0\n",
    "        return code_high, code_low\n",
    "    \n",
    "    def GetIds(asset_name, pilot_name, url_pilot):\n",
    "        # get asset_id for asset_name\n",
    "        payload = {}\n",
    "        headers = {}\n",
    "        \n",
    "        if pilot_name == \"Virtual\":\n",
    "            url_asset_name = \"http://api-ren-prototype.apps.paas-dev.psnc.pl/api/assets?name={asset_name}\".format(asset_name = asset_name)\n",
    "        else:\n",
    "            url_asset_name = \"{url_pilot}/api-postgre/1.0/api/assets?name={asset_name}\".format(url_pilot = url_pilot, asset_name = asset_name)\n",
    "        \n",
    "        try:\n",
    "            response = requests.request(\"GET\", url_asset_name, headers=headers, data=payload)\n",
    "            dict_asset = response.json()[0]\n",
    "            id_asset = dict_asset[\"id\"]\n",
    "        except:\n",
    "            dict_asset = {}\n",
    "            id_asset = -1\n",
    "        \n",
    "        if \"measurements\" in dict_asset and len(dict_asset[\"measurements\"]) > 0:\n",
    "            id_measurement = dict_asset[\"measurements\"][0][\"id\"]\n",
    "        else:\n",
    "            id_measurement = -1\n",
    "\n",
    "        id_dashboard = 1\n",
    "\n",
    "        return id_asset, id_dashboard, id_measurement\n",
    "    \n",
    "    def PostNotification(code_low, date_from, date_to, id_asset, id_dashboard, value, measurement_id, time_, name_pilot, url_pilot):\n",
    "        date_to = maya.parse(time_).add(minutes = 15).epoch\n",
    "        dict_post = {\n",
    "            \"notification_code\": code_low,\n",
    "            \"date_from\": date_from,\n",
    "            \"date_to\": date_to,\n",
    "            \"asset\": id_asset,\n",
    "            \"dashboard\": id_dashboard,\n",
    "            \"value\": value,\n",
    "            \"measurement\": measurement_id,\n",
    "        }\n",
    "        if name_pilot == \"Virtual\":\n",
    "            url = \"http://api-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "        else:\n",
    "            url = \"{url_pilot}/api-postgre/1.0/api/notification\".format(\n",
    "                url_pilot = url_pilot\n",
    "            )\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        try:\n",
    "            response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "            status_code = response.status_code\n",
    "        except:\n",
    "            print(url)\n",
    "            print(dict_post)\n",
    "            raise ValueError\n",
    "        \n",
    "        if response.status_code > 299:\n",
    "            print(response.text)\n",
    "            print(response.status_code)\n",
    "            print(url)\n",
    "            print(dict_post)\n",
    "            raise ValueError\n",
    "\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "            format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "\n",
    "        return response, status_code\n",
    "\n",
    "    def NotificationProcess(forecast_data, code_low, code_high, id_asset, id_dashboard, measurement_id, name_pilot):\n",
    "\n",
    "        failed_notifications = []\n",
    "        success_notification = []\n",
    "\n",
    "        max_ds = max(forecast_data[\"ds\"])\n",
    "        date_notification = datetime.strftime(maya.now().datetime(), \"%Y-%m-%d %H:%M:%S\")\n",
    "        mode = \"none\"\n",
    "        values = []\n",
    "\n",
    "        for index,row in forecast_data.iterrows():\n",
    "            value = row[\"yhat\"]\n",
    "            time_ = str(row[\"ds\"])\n",
    "            print(mode)\n",
    "            # SEND NOTIFICATION\n",
    "            if mode == \"none\":\n",
    "                if value < threshold_min:\n",
    "                    date_from = maya.parse(time_).epoch\n",
    "                    mode = \"lower\"\n",
    "\n",
    "                    if time_ == max_ds:\n",
    "                        response, status_code = PostNotification(code_low, date_from, date_to, id_asset, id_dashboard, value, measurement_id, time_, name_pilot, url_pilot)\n",
    "                        if status_code > 299:\n",
    "                            failed_notifications.append(\"Failed Notification send from {date_from} to {date_to}\".format(date_from = date_from, date_to = date_to))\n",
    "                            failed_notifications.append(response.text)\n",
    "                        else:\n",
    "                            success_notification.append(\"Success Notification send from {date_from} to {date_to}\".format(date_from = date_from, date_to = date_to))\n",
    "                    else:\n",
    "                        values.append(value)\n",
    "                if value > threshold_max:\n",
    "                    date_from = maya.parse(time_).epoch\n",
    "                    mode = \"upper\"\n",
    "\n",
    "                    if time_ == max_ds:\n",
    "                        response, status_code = PostNotification(code_low, date_from, date_to, id_asset, id_dashboard, value, measurement_id, time_, name_pilot, url_pilot)\n",
    "                        if status_code > 299:\n",
    "                            failed_notifications.append(\"Failed Notification send from {date_from} to {date_to}\".format(date_from = date_from, date_to = date_to))\n",
    "                            failed_notifications.append(response.text)\n",
    "                        else:\n",
    "                            success_notification.append(\"Success Notification send from {date_from} to {date_to}\".format(date_from = date_from, date_to = date_to))\n",
    "                    else:\n",
    "                        values.append(value)\n",
    "            elif mode == \"lower\":\n",
    "                if value > threshold_min or time_ == max_ds:\n",
    "                    date_to = maya.parse(time_).epoch\n",
    "                    if len(values) == 0:\n",
    "                        value_notification = 0\n",
    "                    else:\n",
    "                        value_notification = np.mean(values)\n",
    "\n",
    "                    response, status_code = PostNotification(code_low, date_from, date_to, id_asset, id_dashboard, value_notification, measurement_id, time_, name_pilot, url_pilot)\n",
    "                    if status_code > 299:\n",
    "                        failed_notifications.append(\"Failed Notification send from {date_from} to {date_to}\".format(date_from = date_from, date_to = date_to))\n",
    "                        failed_notifications.append(response.text)\n",
    "                    else:\n",
    "                        success_notification.append(\"Success Notification send from {date_from} to {date_to}\".format(date_from = date_from, date_to = date_to))\n",
    "\n",
    "                    if value > threshold_max:\n",
    "                        mode = \"upper\"\n",
    "                        date_from = maya.parse(time_).epoch\n",
    "                        values = [value]\n",
    "                    else:\n",
    "                        values = []\n",
    "                        mode = \"none\"\n",
    "                else:\n",
    "                    values.append(value)\n",
    "            elif mode == \"upper\":\n",
    "                if value < threshold_max or time_ == max_ds:\n",
    "                    date_to = maya.parse(time_).epoch\n",
    "                    if len(values) == 0:\n",
    "                        value_notification = 0\n",
    "                    else:\n",
    "                        value_notification = np.mean(values)\n",
    "\n",
    "                    response, status_code = PostNotification(code_low, date_from, date_to, id_asset, id_dashboard, value_notification, measurement_id, time_, name_pilot, url_pilot)\n",
    "                    if status_code > 299:\n",
    "                        failed_notifications.append(\"Failed Notification send from {date_from} to {date_to}\".format(date_from = date_from, date_to = date_to))\n",
    "                        failed_notifications.append(response.text)\n",
    "                    else:\n",
    "                        success_notification.append(\"Success Notification send from {date_from} to {date_to}\".format(date_from = date_from, date_to = date_to))\n",
    "\n",
    "\n",
    "                    if value < threshold_min:\n",
    "                        date_from = maya.parse(time_).epoch\n",
    "                        mode = \"lower\"\n",
    "                        values = [value]\n",
    "                    else:\n",
    "                        values = []\n",
    "                        mode = \"none\"\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    values.append(value)\n",
    "\n",
    "        return success_notification, failed_notifications\n",
    "\n",
    "    with open(threshold_data_path) as file:\n",
    "        dict_threshold = json.load(file)\n",
    "        try:\n",
    "            threshold_min = dict_threshold[asset_name]\n",
    "        except:\n",
    "            threshold_min = 0\n",
    "        \n",
    "        try:\n",
    "            threshold_max = dict_threshold[asset_name]\n",
    "        except:\n",
    "            threshold_max = 1000000000000000\n",
    "        \n",
    "        threshold_min = 10\n",
    "        threshold_max = 0\n",
    "    \n",
    "    forecast_data = pd.read_csv(forecast_data_path)\n",
    "    \n",
    "\n",
    "    codes = GetNotificationCodes(pilot_name, url_pilot)\n",
    "    if codes == None:\n",
    "        codes = {}\n",
    "    code_high, code_low = ObtainCodes(codes)\n",
    "    id_asset, id_dashboard, id_measurement = GetIds(asset_name, pilot_name, url_pilot)\n",
    "    success_notifications, failed_notifications = NotificationProcess(forecast_data, code_low, code_high, id_asset, id_dashboard, id_measurement, pilot_name)\n",
    "\n",
    "    print(\"SUCCESS\")\n",
    "    for not_ in success_notifications:\n",
    "        print(not_)\n",
    "    \n",
    "    print(\"-----------\")\n",
    "\n",
    "    print(\"FAILED\")\n",
    "    for not_ in failed_notifications:\n",
    "        print(not_)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUXILIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportModelToMinio(input_model_path: InputPath(str),measurement_name, \n",
    "    path_minio = \"minio.kubeflow-renergetic.svc:9000\",\n",
    "    access_key = \"minio\",\n",
    "    secret_key = \"DaTkKc45Hxr1YLR4LxR2xJP2\"\n",
    "    ):\n",
    "\n",
    "    from minio import Minio\n",
    "    import json\n",
    "    with open(input_model_path) as file:\n",
    "        model_serialiazed = json.load()\n",
    "    client = Minio(\n",
    "        path_minio,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_List_Assets(measurement_name, dict_assets) -> dict:\n",
    "    import json\n",
    "    dict_assets = json.loads(dict_assets)\n",
    "    print(measurement_name)\n",
    "    print(dict_assets)\n",
    "    print(type(dict_assets))\n",
    "    return dict_assets[measurement_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REN_Forecast_Test_Pipeline(url_pilot,\n",
    "    diff_time:int,\n",
    "    filter_vars:list = [],\n",
    "    filter_case:list = [],\n",
    "    url = \"minio-kubeflow-renergetic.apps.dcw1-test.paas.psnc.pl\",\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"DaTkKc45Hxr1YLR4LxR2xJP2\",\n",
    "    min_date = \"5 May 2023\",\n",
    "    max_date = \"today\",\n",
    "    mode = \"no notifications\",\n",
    "    list_measurements:list = [\"electricity_meter\", \"heat_meter\"],\n",
    "    dict_assets : dict = {\n",
    "        \"electricity_meter\": [\"building1\", \"building2\"],\n",
    "        \"heat_meter\": [\"building1\", \"building2\"]\n",
    "    },\n",
    "    key_measurement = \"energy\",\n",
    "    type_measurement = \"simulated\",\n",
    "    pilot_name = \"Virtual\",\n",
    "    hourly_aggregate = \"no\",\n",
    "    minute_aggregate = \"no\",\n",
    "    num_days: int = 1,\n",
    "    send_forecast = \"no\",\n",
    "    mae_threshold:float = 1000000,\n",
    "    mode_prophet: str = \"additive\",\n",
    "    daily_seasonality:int = 10,\n",
    "    weekly_seasonality: int = 10,\n",
    "    timestamp: float = time.time()\n",
    "    ):\n",
    "\n",
    "    env_var = V1EnvVar(name='HOME', value='/tmp')\n",
    "    download_data_op = comp.create_component_from_func(\n",
    "        GetData, packages_to_install = [\"requests\", \"numpy\", \"maya\",\"pandas\", \"icecream\", \"tqdm\", \"discord-webhook\", \"retry\"], output_component_file = \"download_data_op_component.yaml\")\n",
    "    download_weather_open_meteo_op = comp.create_component_from_func(\n",
    "        DownloadWeatherData_OpenMeteo, output_component_file= \"open_meteo_component.yaml\", packages_to_install=[\"requests\", \"numpy\", \"maya\",\"pandas\", \"icecream\", \"tqdm\", \"discord-webhook\", \"retry\", \"pyarrow\"]\n",
    "    )\n",
    "    download_weather_influx_db_op = comp.create_component_from_func(\n",
    "        DownloadDataFromInfluxDB, output_component_file=\"weather_influx_db_component.yaml\", packages_to_install=[\"pandas\", \"pyarrow\"]\n",
    "    )\n",
    "\n",
    "    check_metrics_forecast_op = comp.create_component_from_func(\n",
    "        CalculateForecastMetrics, packages_to_install=[\"maya\", \"icecream\", \"pandas\",\"scikit-learn\"], output_component_file = \"metric_check_op.yaml\"\n",
    "    )\n",
    "\n",
    "    get_thresholds_op = comp.create_component_from_func(\n",
    "        GetThresholds, packages_to_install= [\"requests\"], output_component_file= \"thresholds_component.yaml\"\n",
    "    )\n",
    "    get_list_op = comp.create_component_from_func(\n",
    "        Get_List_Assets, output_component_file= \"get_list_component.yaml\"\n",
    "    )\n",
    "    process_data_op = comp.create_component_from_func(\n",
    "        ProcessData, packages_to_install= [\"maya\", \"pandas\", \"icecream\", \"tqdm\"], output_component_file= \"process_data_op_component.yaml\"\n",
    "    )\n",
    "    forecast_and_train_data_op = comp.create_component_from_func(\n",
    "        ForecastProcess, packages_to_install = [],base_image= \"adcarras/ren-docker-forecast:0.0.1\", output_component_file = \"forecast_data_op_component.yaml\")\n",
    "    forecast_data_op = comp.create_component_from_func(\n",
    "        PredictFromPreviousModel, packages_to_install= [], base_image= \"adcarras/ren-docker-forecast:0.0.1\", output_component_file= \"forecast_from_previous.yaml\"\n",
    "    )\n",
    "    check_send_forecast_op = comp.create_component_from_func(\n",
    "        CheckSendForecast, packages_to_install=[\"discord-webhook\"], output_component_file= \"check_send_forecast_component.yaml\"\n",
    "    )\n",
    "    send_forecast_op = comp.create_component_from_func(SendForecast, packages_to_install=[\"requests\", \"numpy\", \"maya\",\"pandas\", \"icecream\", \"discord-webhook\", \"tqdm\", \"minio\", \"boto3\"], output_component_file= \"send_forecast_comp.yaml\")\n",
    "\n",
    "    check_send_notification_op = comp.create_component_from_func(\n",
    "        CheckSendNotification, output_component_file= \"check_send_notification.yaml\"\n",
    "    )\n",
    "\n",
    "    send_notification_op = comp.create_component_from_func(\n",
    "        SendNotification, packages_to_install=[\"pandas\", \"discord-webhook\", \"maya\", \"fuckit\", \"icecream\"],output_component_file=\"send_notification.yaml\"\n",
    "    )\n",
    "\n",
    "    # BEGIN PIPELINE DEFINITION\n",
    "\n",
    "    get_thresholds_task = get_thresholds_op(url_pilot, pilot_name)\n",
    "\n",
    "    download_weather_influx_task = download_weather_influx_db_op(timestamp)\n",
    "\n",
    "    download_weather_open_meteo_task = download_weather_open_meteo_op(download_weather_influx_task.output, pilot_name, min_date)\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    with dsl.ParallelFor(list_measurements) as measurement:\n",
    "        download_task = (download_data_op(measurement, min_date, max_date, url_pilot,pilot_name, type_measurement, key_measurement, filter_vars, filter_case).add_env_variable(env_var)\n",
    "                            .set_memory_request('2Gi')\n",
    "                            .set_memory_limit('4Gi')\n",
    "                            .set_cpu_request('2')\n",
    "                            .set_cpu_limit('4'))\n",
    "        process_task = (process_data_op(download_task.outputs[\"output_data_forecast\"], \n",
    "                        hourly_aggregate,\n",
    "                        minute_aggregate,\n",
    "                        min_date, \n",
    "                        max_date)\n",
    "                        .set_memory_request('2Gi')\n",
    "                            .set_memory_limit('4Gi')\n",
    "                            .set_cpu_request('2')\n",
    "                            .set_cpu_limit('4'))\n",
    "        \n",
    "        get_list_task = (get_list_op(measurement, dict_assets))\n",
    "\n",
    "        check_send_forecast_task = check_send_forecast_op(send_forecast)\n",
    "        check_send_notification_task = check_send_notification_op(mode)\n",
    "\n",
    "        \n",
    "        with dsl.ParallelFor(get_list_task.output) as asset:\n",
    "            check_forecast_task = (check_metrics_forecast_op(download_task.outputs[\"output_data_metric\"], asset, mae_threshold = mae_threshold)\n",
    "                                   .set_memory_request('2Gi')\n",
    "                                    .set_memory_limit('4Gi')\n",
    "                                    .set_cpu_request('2')\n",
    "                                    .set_cpu_limit('4'))\n",
    "            with dsl.Condition(check_forecast_task.output == True):\n",
    "                forecast_train_task = (forecast_and_train_data_op(process_task.output, download_weather_open_meteo_task.output,\n",
    "                measurement, \n",
    "                url, \n",
    "                access_key, \n",
    "                secret_key, \n",
    "                mode,\n",
    "                url_pilot,\n",
    "                diff_time,\n",
    "                pilot_name,\n",
    "                send_forecast,\n",
    "                asset,\n",
    "                num_days,\n",
    "                mode_prophet, daily_seasonality, weekly_seasonality).add_env_variable(env_var)\n",
    "                .set_memory_request('2Gi')\n",
    "                .set_memory_limit('4Gi')\n",
    "                .set_cpu_request('2')\n",
    "                .set_cpu_limit('4')\n",
    "                )\n",
    "\n",
    "                with dsl.Condition(check_send_forecast_task.output == True):\n",
    "                    send_forecast_task = send_forecast_op(forecast_train_task.outputs[\"forecast_data\"], url_pilot, pilot_name, asset, measurement, key_measurement, num_days)\n",
    "\n",
    "                with dsl.Condition(check_send_notification_task.output == True):\n",
    "                    send_notification_task = send_notification_op(forecast_train_task.outputs[\"forecast_data\"], get_thresholds_task.output, asset,pilot_name, url_pilot)\n",
    "            \n",
    "            with dsl.Condition(check_forecast_task.output == False):\n",
    "                forecast_task = forecast_data_op(process_task.output, download_weather_open_meteo_task.output, \n",
    "                                                 pilot_name, measurement, asset, \"\", \n",
    "                                                 max_date, num_days, diff_time)\n",
    "                with dsl.Condition(check_send_forecast_task.output == True):\n",
    "                    send_forecast_task = send_forecast_op(forecast_task.outputs[\"forecast_data\"], url_pilot, pilot_name, asset, measurement, key_measurement, num_days)\n",
    "\n",
    "                with dsl.Condition(check_send_notification_task.output == True):\n",
    "                    send_notification_task = send_notification_op(forecast_task.outputs[\"forecast_data\"], get_thresholds_task.output, asset,pilot_name, url_pilot)\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func = REN_Forecast_Test_Pipeline, package_path =\"Forecast_Data_Pipeline.yaml\")\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02e15b1dc1a9053df8aede2000f19bd423bcf01320081b3a1e044a45ff452adb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
