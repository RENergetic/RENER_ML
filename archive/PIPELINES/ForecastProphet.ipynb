{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "import kfp.components as comp\n",
    "from kfp import compiler, dsl\n",
    "from kfp import dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from kubernetes.client.models import V1EnvVar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. [Get Thresholds](#Thresholds)\n",
    "2. [Download Data](#Download)\n",
    "3. [Process Data](#process)\n",
    "4. [Forecast with previous model](#forecastprevious)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetThresholds(output_thresholds_path: OutputPath(str)):\n",
    "\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    thresholds = {\n",
    "        \"heat_meter\":{\n",
    "            \"building1\": [0,3],\n",
    "            \"building2\": [0,4],\n",
    "            \"datacenter1\": [3,5],\n",
    "            \"psnc_garden\": [3,5],\n",
    "            \"hvac\": [3,5],\n",
    "            \"office\":[3,5],\n",
    "            \"flat1\": [3,5],\n",
    "            \"eagle\": [3,5],\n",
    "            \"altair\": [3,5]\n",
    "        },\n",
    "        \"electricity_meter\": {\n",
    "            \"building1\": [0,6],\n",
    "            \"building2\": [0,6],\n",
    "            \"datacenter1\": [3,5],\n",
    "            \"psnc_garden\": [3,5],\n",
    "            \"hvac\": [3,5],\n",
    "            \"office\": [3,5],\n",
    "            \"flat1\": [3,5],\n",
    "            \"eagle\": [3,5],\n",
    "            \"altair\": [3,5]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(output_thresholds_path, \"w\") as file:\n",
    "        json.dump(thresholds, file)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(measurement_name: str, min_date: str, max_date: str,url_pilot : str,type_measurement :str,key_measurement : str,\n",
    "            filter_vars:list , filter_cases: list, output_data_forecast: OutputPath(str)):\n",
    "\n",
    "    import requests # To REQUIREMENTS\n",
    "    import json\n",
    "    import pandas as pd # To REQUIREMENTS\n",
    "    import maya # To REQUIREMENTS\n",
    "    from tqdm import tqdm\n",
    "    from icecream import ic\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    from retry import retry # TO REQUIREMENTS\n",
    "\n",
    "    #Functions definitions\n",
    "\n",
    "    def GetRequest(url, headers ={}, payload = {}):\n",
    "        response = requests.request(\"GET\", url, headers = headers, data = payload)\n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            dict_ = {\n",
    "                \"status_code\": response.status_code,\n",
    "                \"text\": response.text\n",
    "            }\n",
    "            return dict_\n",
    "    def DownloadAssetsData(measurement_name, url_pilot,bucket = \"renergetic\", min_date = \"yesterday\", max_date = \"tomorrow\"):\n",
    "        \n",
    "        from datetime import datetime\n",
    "        import pandas as pd\n",
    "        import maya\n",
    "        from tqdm import tqdm\n",
    "        from icecream import ic\n",
    "\n",
    "        test = True\n",
    "\n",
    "        try:\n",
    "            min_date_from = maya.when(min_date).datetime()\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MIN_DATE\")\n",
    "        \n",
    "        try: \n",
    "            max_date_from = maya.when(max_date).datetime()\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MAX_DATE\")\n",
    "        \n",
    "        datelist = pd.date_range(min_date_from, max_date_from)\n",
    "\n",
    "        data_ = []\n",
    "        for i in tqdm(range(len(datelist)-1)):\n",
    "            from_obj = datelist[i]\n",
    "            to_obj = datelist[i+1]\n",
    "            from_ = datetime.strftime(from_obj, \"%Y-%m-%d 00:00:00\")\n",
    "            to_ = datetime.strftime(to_obj, \"%Y-%m-%d 00:00:00\")\n",
    "\n",
    "            url = url_pilot + \"/data?measurements={measurement_name}&from={from_}&to={to_}\"\\\n",
    "                .format(measurement_name = measurement_name, from_ = from_, to_= to_)\n",
    "            info_ = GetRequest(url)\n",
    "            if type(info_) == list:\n",
    "                data_ = data_ + info_\n",
    "            elif type(info_) == dict:\n",
    "                print(\"Error\")\n",
    "                print(from_)\n",
    "                print(to_)\n",
    "        return data_\n",
    "    def DataFrameAssests(list_data, name_field):\n",
    "        dicts = []\n",
    "        for data in list_data:\n",
    "            try:\n",
    "                if \"energy\" in data[\"fields\"].keys():\n",
    "                    name_value = \"energy\"\n",
    "                else:\n",
    "                    name_value = name_field\n",
    "                dict_ = {\n",
    "                    \"asset_name\": data[\"tags\"][\"asset_name\"],\n",
    "                    \"value\": float(data[\"fields\"][name_value]),\n",
    "                    \"ds\": data[\"fields\"][\"time\"]\n",
    "                }\n",
    "\n",
    "                if \"type_data\" in data[\"tags\"].keys():\n",
    "                    dict_[\"type\"] = data[\"tags\"][\"type_data\"]\n",
    "                elif \"typeData\" in data[\"tags\"].keys():\n",
    "                    dict_[\"type\"] = data[\"tags\"][\"typeData\"]\n",
    "                else:\n",
    "                    dict_[\"type\"] = \"None\"\n",
    "\n",
    "                if \"measurement_type\" in data[\"tags\"].keys():\n",
    "                    dict_[\"measurement_type\"] = data[\"tags\"][\"measurement_type\"]\n",
    "                else:\n",
    "                    dict_[\"measurement_type\"] = \"None\"\n",
    "                \n",
    "                if \"direction\" in data[\"tags\"].keys():\n",
    "                    dict_[\"direction\"] = data[\"tags\"][\"direction\"]\n",
    "                else:\n",
    "                    dict_[\"direction\"] = \"None\"\n",
    "                if \"domain\" in data[\"tags\"].keys():\n",
    "                    dict_[\"domain\"] = data[\"tags\"][\"domain\"]\n",
    "                else:\n",
    "                    dict_[\"domain\"] = \"None\"\n",
    "                \n",
    "                if \"sensor_id\" in data[\"tags\"].keys():\n",
    "                    dict_[\"id_sensor\"] = data[\"tags\"][\"sensor_id\"]\n",
    "                else:\n",
    "                    dict_[\"id_sensor\"] = \"None\"\n",
    "                \n",
    "                if \"interpolation_method\" in data[\"tags\"].keys():\n",
    "                    dict_[\"interpolation\"] = data[\"tags\"][\"interpolation_method\"]\n",
    "                else:\n",
    "                    dict_[\"interpolation\"] = \"None\"\n",
    "\n",
    "                dicts.append(dict_)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return pd.DataFrame(dicts)\n",
    "    \n",
    "    @retry(tries= 3)\n",
    "    def DownloadAndProcess(measurement_name, url_pilot, min_date, max_date, key_measurement):\n",
    "        # max_date = maya.now().add(days = 3).iso8601()\n",
    "        list_ = DownloadAssetsData(measurement_name, url_pilot,min_date = min_date, max_date = max_date)\n",
    "        data = DataFrameAssests(list_, key_measurement)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def FilterCases(var_value, filter_cases):\n",
    "        if var_value in filter_cases:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def FilterData(data, type_measurement, filter_vars, filter_cases):\n",
    "\n",
    "        data = data[data.type == type_measurement]\n",
    "\n",
    "        for i in range(len(filter_vars)):\n",
    "            data[\"Filter\"] = data[filter_vars[i]].apply(FilterCases, filter_cases = filter_cases[i])\n",
    "            data = data[data[\"Filter\"] == True]\n",
    "        \n",
    "        return data    \n",
    "    \n",
    "    def SendAlert(data):\n",
    "        ic(data.shape[0])\n",
    "\n",
    "        if data.shape[0] == 0:\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Not enough data for {measurement_name}\".format(measurement_name = measurement_name)\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "            \n",
    "            raise ValueError(\"Void data to forecast\")\n",
    "\n",
    "    # Code Execution\n",
    "    \n",
    "    data = DownloadAndProcess(measurement_name, url_pilot, min_date, max_date, key_measurement)\n",
    "    \n",
    "    data = FilterData(data, type_measurement, filter_vars, filter_cases)\n",
    "    \n",
    "    SendAlert(data)\n",
    "    \n",
    "\n",
    "    data_output = {\n",
    "        \"value\": data[\"value\"].tolist(),\n",
    "        \"time_registered\": data[\"ds\"].tolist(),\n",
    "        \"asset_name\": data[\"asset_name\"].tolist()\n",
    "    }\n",
    "\n",
    "    with open(output_data_forecast, \"w\") as file:\n",
    "        json.dump(data_output, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessData(input_data_path: InputPath(str), hourly_aggregate, minute_aggregate ,min_date, max_date, output_data_forecast: OutputPath(str)):\n",
    "\n",
    "    import maya\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from icecream import ic\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    min_date = datetime.strftime(maya.when(min_date).datetime(), \"%Y-%m-%d\")\n",
    "    max_date = datetime.strftime(maya.when(max_date).datetime(), \"%Y-%m-%d\")\n",
    "    \n",
    "    ic(hourly_aggregate)\n",
    "    ic(minute_aggregate)\n",
    "\n",
    "    with open(input_data_path) as file:\n",
    "        data_str = json.load(file)\n",
    "    \n",
    "    data = pd.DataFrame(data_str)\n",
    "\n",
    "    # DEFINE HOURLY AGGREGATE PROCESS\n",
    "\n",
    "    def ProcessHourly(data, hourly_aggregate, min_date, max_date):\n",
    "        list_dicts = []\n",
    "        list_data = []\n",
    "\n",
    "        for asset_name in pd.unique(data.asset_name):\n",
    "            data_iter = data[data.asset_name == asset_name]\n",
    "            \n",
    "            def GetHourDate(str_):\n",
    "                import maya\n",
    "                from datetime import datetime\n",
    "\n",
    "                return datetime.strftime(maya.parse(str_).datetime(), \"%Y-%m-%d %H:00:00\")\n",
    "\n",
    "            data_iter[\"hour\"] = data_iter[\"time_registered\"].apply(GetHourDate)\n",
    "            data_group = (data_iter.groupby('hour')\n",
    "                .agg({'time_registered':'count', 'value': hourly_aggregate})\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            df = data_group[[\"hour\", \"value\"]]\n",
    "            df = df.rename(columns={'hour':'ds', 'value': 'y'})\n",
    "            last_value = df.y.tolist()[0]\n",
    "            for ds_obj in tqdm(pd.date_range(min_date, max_date)):\n",
    "                for i in range(24):\n",
    "                    if i < 10:\n",
    "                        str_i = \"0\"+str(i)\n",
    "                    else:\n",
    "                        str_i = str(i)\n",
    "                    ds_str = \"{date} {H}:00:00\".format(date = ds_obj.strftime(\"%Y-%m-%d\"), H = str_i)\n",
    "\n",
    "                    if df[df.ds == ds_str].shape[0] == 0:\n",
    "                        if hourly_aggregate == \"max\":\n",
    "                                value_ = last_value\n",
    "                        else:\n",
    "                            value_ = 0\n",
    "\n",
    "                        dict_ = {\n",
    "                            \"time_registered\": ds_str,\n",
    "                            \"value\": value_,\n",
    "                            \"asset_name\": asset_name\n",
    "                        }\n",
    "                    else:\n",
    "                        dict_ = {\n",
    "                            \"time_registered\": ds_str,\n",
    "                            \"value\": df[df.ds == ds_str].y.tolist()[0],\n",
    "                            \"asset_name\": asset_name\n",
    "                        }\n",
    "                        if minute_aggregate == \"max\":\n",
    "                            last_value = dict_[\"value\"]\n",
    "                    list_dicts.append(dict_)\n",
    "            if hourly_aggregate == \"max\":\n",
    "                data_1 = pd.DataFrame(list_dicts)\n",
    "                data_1[\"value_1\"] = data_1[\"value\"].shift(1)\n",
    "                data_1[\"value\"] = data_1[\"value\"] - data_1[\"value_1\"]\n",
    "                data_1 = data_1.drop([\"value_1\"], axis = 1)\n",
    "                data_1[\"value\"] = data_1[\"value\"].fillna(0)\n",
    "                list_data.append(data_1)\n",
    "                list_dicts = []\n",
    "\n",
    "        if hourly_aggregate == \"max\":\n",
    "            output_data = pd.concat(list_data, ignore_index = True)\n",
    "        else:\n",
    "            output_data = pd.DataFrame(list_dicts)\n",
    "        \n",
    "        return output_data\n",
    "    \n",
    "    def ProcessMinutely(data, minute_aggregate, min_date, max_date):\n",
    "        list_dicts = []\n",
    "        list_data = []\n",
    "        for asset_name in tqdm(pd.unique(data.asset_name)):\n",
    "            data_iter = data[data.asset_name == asset_name]\n",
    "            def GetHourMinuteDate(str_):\n",
    "                import maya\n",
    "                from datetime import datetime\n",
    "\n",
    "                return datetime.strftime(maya.parse(str_).datetime(), \"%Y-%m-%d %H:%M:00\")\n",
    "            \n",
    "            data_iter[\"minute\"] = data_iter[\"time_registered\"].apply(GetHourMinuteDate)\n",
    "            data_group = (data_iter.groupby('minute')\n",
    "                .agg({'time_registered':'count', 'value': minute_aggregate})\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            df = data_group[[\"minute\", \"value\"]]\n",
    "            df = df.rename(columns={'minute':'ds', 'value': 'y'})\n",
    "\n",
    "            last_value = df.y.tolist()[0]\n",
    "\n",
    "            for ds_obj in tqdm(pd.date_range(min_date, max_date)):\n",
    "                for i in range(24):\n",
    "                    for j in range(60):\n",
    "                        if i < 10:\n",
    "                            str_i = \"0\"+str(i)\n",
    "                        else:\n",
    "                            str_i = str(i)\n",
    "                        \n",
    "                        if j < 10:\n",
    "                            str_j = \"0\" + str(j)\n",
    "                        else:\n",
    "                            str_j = str(j)\n",
    "                        ds_str = \"{date} {H}:{M}:00\".format(date = ds_obj.strftime(\"%Y-%m-%d\"), H = str_i, M = str_j)\n",
    "\n",
    "                        if df[df.ds == ds_str].shape[0] == 0:\n",
    "\n",
    "                            if minute_aggregate == \"max\":\n",
    "                                value_ = last_value\n",
    "                            else:\n",
    "                                value_ = 0\n",
    "\n",
    "                            dict_ = {\n",
    "                                \"time_registered\": ds_str,\n",
    "                                \"value\": value_,\n",
    "                                \"asset_name\": asset_name\n",
    "                            }\n",
    "                        else:\n",
    "                            dict_ = {\n",
    "                                \"time_registered\": ds_str,\n",
    "                                \"value\": df[df.ds == ds_str].y.tolist()[0],\n",
    "                                \"asset_name\": asset_name\n",
    "                            }\n",
    "                            if minute_aggregate == \"max\":\n",
    "                                last_value = dict_[\"value\"]\n",
    "                        list_dicts.append(dict_)\n",
    "            if minute_aggregate == \"max\":\n",
    "                data_1 = pd.DataFrame(list_dicts)\n",
    "                data_1[\"value_1\"] = data_1[\"value\"].shift(1)\n",
    "                data_1[\"value_cummulative\"] = data_1[\"value\"].copy()\n",
    "                data_1[\"value\"] = data_1[\"value\"] - data_1[\"value_1\"]\n",
    "                data_1 = data_1.drop([\"value_1\"], axis = 1)\n",
    "                data_1[\"value\"] = data_1[\"value\"].fillna(0)\n",
    "                list_data.append(data_1)\n",
    "                list_dicts = []\n",
    "\n",
    "        if minute_aggregate == \"max\":\n",
    "            output_data = pd.concat(list_data, ignore_index = True)\n",
    "        else:\n",
    "            output_data = pd.DataFrame(list_dicts)\n",
    "        \n",
    "        return output_data\n",
    "    \n",
    "    if hourly_aggregate in [\"mean\",\"sum\", \"max\"]:\n",
    "        print(\"hourly process\")\n",
    "        output_data = ProcessHourly(data, hourly_aggregate, min_date, max_date)\n",
    "    elif minute_aggregate in [\"max\", \"sum\", \"mean\"]:\n",
    "        print(\"minutely process\")\n",
    "        output_data = ProcessMinutely(data, minute_aggregate, min_date, max_date)\n",
    "    else:\n",
    "        output_data = data\n",
    "\n",
    "    \n",
    "\n",
    "    data_output = {\n",
    "        \"value\": output_data[\"value\"].tolist(),\n",
    "        \"time_registered\": output_data[\"time_registered\"].tolist(),\n",
    "        \"asset_name\": output_data[\"asset_name\"].tolist()\n",
    "    }\n",
    "\n",
    "    if minute_aggregate == \"max\" or hourly_aggregate == \"max\":\n",
    "        data_output[\"value_cummulative\"] = output_data[\"value_cummulative\"].tolist()\n",
    "\n",
    "    with open(output_data_forecast, \"w\") as file:\n",
    "        json.dump(data_output, file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast_Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictFromPreviousModel(name_pilot, measurement_name, asset_name) -> bool:\n",
    "\n",
    "    import maya\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import json\n",
    "    from icecream import ic\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from prophet.serialize import model_to_json, model_from_json\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from minio import Minio\n",
    "    import boto3\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    from datetime import datetime\n",
    "\n",
    "    s3 = boto3.resource(\n",
    "        service_name='s3',\n",
    "        aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "        aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "        endpoint_url='https://s3.tebi.io'\n",
    "    )\n",
    "\n",
    "    s3_client = boto3.client('s3',\n",
    "        aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "        aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "        endpoint_url='https://s3.tebi.io'\n",
    "    )\n",
    "\n",
    "    # model_Ghent_pv_de-nieuwe-dokken-pv-017A-xxxxx9A1.json\n",
    "\n",
    "    def GetRequest(url, headers ={}, payload = {}):\n",
    "        response = requests.request(\"GET\", url, headers = headers, data = payload)\n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            dict_ = {\n",
    "                \"status_code\": response.status_code,\n",
    "                \"text\": response.text\n",
    "            }\n",
    "            return dict_\n",
    "    def DownloadAssetsData(measurement_name, url_pilot,bucket = \"renergetic\", min_date = \"yesterday\", max_date = \"tomorrow\"):\n",
    "        \n",
    "        from datetime import datetime\n",
    "        import pandas as pd\n",
    "        import maya\n",
    "        from tqdm import tqdm\n",
    "        from icecream import ic\n",
    "\n",
    "        test = True\n",
    "\n",
    "        try:\n",
    "            min_date_from = maya.when(min_date).datetime()\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MIN_DATE\")\n",
    "        \n",
    "        try: \n",
    "            max_date_from = maya.when(max_date).datetime()\n",
    "        except:\n",
    "            ValueError(\"Please introduce correct time format for MAX_DATE\")\n",
    "        \n",
    "        datelist = pd.date_range(min_date_from, max_date_from)\n",
    "\n",
    "        data_ = []\n",
    "        for i in tqdm(range(len(datelist)-1)):\n",
    "            from_obj = datelist[i]\n",
    "            to_obj = datelist[i+1]\n",
    "            from_ = datetime.strftime(from_obj, \"%Y-%m-%d 00:00:00\")\n",
    "            to_ = datetime.strftime(to_obj, \"%Y-%m-%d 00:00:00\")\n",
    "\n",
    "            url = url_pilot + \"/data?measurements={measurement_name}&from={from_}&to={to_}\"\\\n",
    "                .format(measurement_name = measurement_name, from_ = from_, to_= to_)\n",
    "            info_ = GetRequest(url)\n",
    "            if type(info_) == list:\n",
    "                data_ = data_ + info_\n",
    "            elif type(info_) == dict:\n",
    "                print(\"Error\")\n",
    "                print(from_)\n",
    "                print(to_)\n",
    "        return data_\n",
    "    \n",
    "    def DataFrameAssests(list_data, name_field):\n",
    "        dicts = []\n",
    "        for data in list_data:\n",
    "            try:\n",
    "                if \"energy\" in data[\"fields\"].keys():\n",
    "                    name_value = \"energy\"\n",
    "                else:\n",
    "                    name_value = name_field\n",
    "                dict_ = {\n",
    "                    \"asset_name\": data[\"tags\"][\"asset_name\"],\n",
    "                    \"value\": float(data[\"fields\"][name_value]),\n",
    "                    \"ds\": data[\"fields\"][\"time\"]\n",
    "                }\n",
    "\n",
    "                if \"type_data\" in data[\"tags\"].keys():\n",
    "                    dict_[\"type\"] = data[\"tags\"][\"type_data\"]\n",
    "                elif \"typeData\" in data[\"tags\"].keys():\n",
    "                    dict_[\"type\"] = data[\"tags\"][\"typeData\"]\n",
    "                else:\n",
    "                    dict_[\"type\"] = \"None\"\n",
    "\n",
    "                if \"measurement_type\" in data[\"tags\"].keys():\n",
    "                    dict_[\"measurement_type\"] = data[\"tags\"][\"measurement_type\"]\n",
    "                else:\n",
    "                    dict_[\"measurement_type\"] = \"None\"\n",
    "                \n",
    "                if \"direction\" in data[\"tags\"].keys():\n",
    "                    dict_[\"direction\"] = data[\"tags\"][\"direction\"]\n",
    "                else:\n",
    "                    dict_[\"direction\"] = \"None\"\n",
    "                if \"domain\" in data[\"tags\"].keys():\n",
    "                    dict_[\"domain\"] = data[\"tags\"][\"domain\"]\n",
    "                else:\n",
    "                    dict_[\"domain\"] = \"None\"\n",
    "                \n",
    "                if \"sensor_id\" in data[\"tags\"].keys():\n",
    "                    dict_[\"id_sensor\"] = data[\"tags\"][\"sensor_id\"]\n",
    "                else:\n",
    "                    dict_[\"id_sensor\"] = \"None\"\n",
    "                \n",
    "                if \"interpolation_method\" in data[\"tags\"].keys():\n",
    "                    dict_[\"interpolation\"] = data[\"tags\"][\"interpolation_method\"]\n",
    "                else:\n",
    "                    dict_[\"interpolation\"] = \"None\"\n",
    "\n",
    "                dicts.append(dict_)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return pd.DataFrame(dicts)\n",
    "    \n",
    "    \n",
    "    my_bucket = s3.Bucket('test-pf')\n",
    "    list_objects = []\n",
    "    for my_bucket_object in my_bucket.objects.all():\n",
    "        list_objects.append(my_bucket_object.key)\n",
    "    \n",
    "    try:\n",
    "        file_stats = \"stats_{name_pilot}_{measurement_name}_{asset_name}.json\".format(\n",
    "            name_pilot = name_pilot,measurement_name = measurement_name, asset_name = asset_name\n",
    "        )\n",
    "        \n",
    "        with open(file_stats, 'wb') as f:\n",
    "            s3_client.download_fileobj('test-pf', file_stats, f)\n",
    "        \n",
    "        with open(file_stats) as f:\n",
    "            stats_asset_models = json.load(f)\n",
    "        \n",
    "        if \"catboost\" in stats_asset_models.keys():\n",
    "            last_date_catboost = stats_asset_models[\"catboost\"][\"last_update_date\"]\n",
    "        else:\n",
    "            last_date_catboost = datetime.strftime(maya.when(\"1 Jan 1970\").datetime(), format = \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        if \"prophet\" in stats_asset_models.keys():\n",
    "            last_date_prophet = stats_asset_models[\"prophet\"][\"last_update_date\"]\n",
    "        else:\n",
    "            last_date_prophet = datetime.strftime(maya.when(\"1 Jan 1970\").datetime(), format = \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        if last_date_prophet == last_date_catboost:\n",
    "            raise ValueError(\"No models trained\")\n",
    "        elif last_date_catboost > last_date_prophet:\n",
    "            type_ = \"catboost\"\n",
    "        else:\n",
    "            type_ = \"prophet\"\n",
    "\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    name_model = \"model_{name_pilot}_{measurement_name}_{asset_name}_{type_}\".format(\n",
    "        name_pilot = name_pilot,measurement_name = measurement_name, asset_name = asset_name, type_ = type_\n",
    "    )\n",
    "\n",
    "    if type_ == \"prophet\":\n",
    "        with open('model_prophet.json', 'r') as fin:\n",
    "            m = model_from_json(fin.read())\n",
    "            from_date_obj = maya.parse(last_date_prophet).add(days = -2)\n",
    "        return True\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForecastProcess(input_data_path: InputPath(str), measurement_name,\n",
    "    path_minio,\n",
    "    access_key,\n",
    "    secret_key,\n",
    "    mode,\n",
    "    url_pilot,\n",
    "    diff_time,\n",
    "    pilot_name,\n",
    "    send_forecast,\n",
    "    asset_name,\n",
    "    num_days,\n",
    "    mlpipeline_metrics_path: OutputPath('Metrics'),\n",
    "    forecast_data_path: OutputPath(str)\n",
    "    ):\n",
    "\n",
    "    import maya\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import json\n",
    "    from icecream import ic\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from prophet.serialize import model_to_json\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from minio import Minio\n",
    "    import boto3\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        client = Minio(\n",
    "            path_minio,\n",
    "            access_key=access_key,\n",
    "            secret_key=secret_key,\n",
    "            secure = False\n",
    "        )\n",
    "\n",
    "        list_objects = client.list_objects(\"test\")\n",
    "        for obj_ in list_objects:\n",
    "            ic(obj_._object_name)\n",
    "    except:\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Cannot access minio server correctly - read data.\"\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "    \n",
    "\n",
    "    def ForecastData(data, asset_name, measurement_name, metrics_list, measures_per_hour, diff_time, mode = \"no notifications\"):\n",
    "        \n",
    "        # Generic Processing Functions\n",
    "\n",
    "        def ModifyData(data, asset_name):\n",
    "            data_ds = data[data.asset_name == asset_name][[\"time_registered\", \"value\"]]\n",
    "            try:\n",
    "                last_cummulative_value = data[data.asset_name == asset_name][\"value_cummulative\"].tolist()[-1]\n",
    "            except:\n",
    "                last_cummulative_value = 0\n",
    "            data_ds.columns = [\"ds\", \"y\"]\n",
    "            if data_ds.shape[0] == 0:\n",
    "                max_date = datetime.strftime(maya.when(\"now\").datetime(),\"%Y-%m-%d %H:%M:%S\")\n",
    "            else:\n",
    "                max_date = max(data_ds.ds)\n",
    "            ic(last_cummulative_value)\n",
    "            ic(data_ds.shape)\n",
    "            ic(len(pd.unique(data_ds.y)))\n",
    "\n",
    "            return data_ds, max_date, last_cummulative_value\n",
    "\n",
    "        # Categorical Processing Functions\n",
    "\n",
    "        def GetDateInfo(ds_str, time_value):\n",
    "            maya_obj = maya.parse(ds_str)\n",
    "            if time_value == \"year\":\n",
    "                return str(maya_obj.year)\n",
    "            elif time_value == \"month\":\n",
    "                return str(maya_obj.month)\n",
    "            elif time_value == \"weekday\":\n",
    "                return str(maya_obj.weekday)\n",
    "            elif time_value == \"hour\":\n",
    "                return str(maya_obj.hour)\n",
    "            \n",
    "        def ManageData(data_ds, num_prevs = 24):\n",
    "            data_ds[\"year\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"year\")\n",
    "            data_ds[\"month\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"month\")\n",
    "            data_ds[\"weekday\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"weekday\")\n",
    "            data_ds[\"hour\"]  = data_ds[\"ds\"].apply(GetDateInfo, time_value = \"hour\")\n",
    "\n",
    "            for var_ in [\"year\", \"month\", \"weekday\", \"hour\"]:\n",
    "                if len(pd.unique(data_ds[var_])) <= 2:\n",
    "                    bin_main = list(pd.unique(data_ds[var_]))[0]\n",
    "                    data_ds[var_] = (data_ds[var_] == bin_main)\n",
    "            \n",
    "            for i in range(1,num_prevs + 1):\n",
    "                name_var = \"prev_val_{i}\".format(i = i)\n",
    "                data_ds[name_var] = data_ds[\"y\"].shift(i).apply(str)\n",
    "            data_ds = data_ds[(num_prevs+1):]\n",
    "\n",
    "            cat_features_names = [\"year\", \"month\", \"weekday\", \"hour\"]\n",
    "            names_prevs_vars = []\n",
    "            for name_var in data_ds.columns.values:\n",
    "                if \"prev_val\" in name_var:\n",
    "                    names_prevs_vars.append(name_var)\n",
    "            cat_features_names = cat_features_names + names_prevs_vars\n",
    "\n",
    "\n",
    "            return data_ds, names_prevs_vars, cat_features_names\n",
    "        \n",
    "        def Train_CatBoost(data_ds, cat_features_names):\n",
    "\n",
    "                # Process data\n",
    "                X_train, X_test, Y_train, Y_test = train_test_split(data_ds.drop([\"y\", \"ds\"], axis = 1), data_ds.y, test_size = 0.2)\n",
    "                \n",
    "\n",
    "                # Pool Creation\n",
    "\n",
    "                train_pool = Pool(\n",
    "                    data = X_train, label = Y_train, \n",
    "                    cat_features = cat_features_names\n",
    "                    )\n",
    "                test_pool = Pool(\n",
    "                    data = X_test, label = Y_test, \n",
    "                    cat_features = cat_features_names\n",
    "                    )\n",
    "                \n",
    "                catboost_model = CatBoostClassifier(\n",
    "                    iterations = 10,\n",
    "                    learning_rate = 1,\n",
    "                    depth = 8\n",
    "                )\n",
    "                print(\"Model To Train\")\n",
    "                catboost_model.fit(train_pool)\n",
    "                print(\"Model trained\")\n",
    "                yhat_test = catboost_model.predict(test_pool)\n",
    "                yhat_train = catboost_model.predict(train_pool)\n",
    "                print(\"Y hat obtained\")\n",
    "                accuracy_score_train = accuracy_score(Y_train, yhat_train)\n",
    "                accuracy_score_test = accuracy_score(Y_test, yhat_test)\n",
    "                ic(accuracy_score_train)\n",
    "                ic(accuracy_score_test)\n",
    "            \n",
    "                return catboost_model, accuracy_score_train, accuracy_score_test\n",
    "\n",
    "        def Save_CatBoost(catboost_model, pilot_name, measurement_name, asset_name):\n",
    "            catboost_model.save_model(\"/tmp/catboost_model.cbm\", format = \"cbm\")\n",
    "            s3 = boto3.resource(\n",
    "                service_name='s3',\n",
    "                aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                endpoint_url='https://s3.tebi.io'\n",
    "            )\n",
    "\n",
    "            for bucket in s3.buckets.all():\n",
    "                ic(bucket.name)\n",
    "            \n",
    "            # Upload a new file\n",
    "            data = open('/tmp/catboost_model.cbm', 'rb')\n",
    "            f_name = \"model_{pilot}_{domain}_{asset}_latest_catboost.cbm\"\\\n",
    "            .format(pilot = pilot_name,domain = measurement_name, asset = asset_name)\n",
    "            s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Model sent to tebi for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "        def Predict_CatBoost(catboost_model, data_ds, last_cummulative_value, num_days, measures_per_hour, diff_time, cat_features_names, names_prevs_vars):\n",
    "            data_ds[\"yhat\"] = last_cummulative_value\n",
    "            for day_ in range(num_days):\n",
    "                for i in tqdm(range(24)):\n",
    "                    for j in range(measures_per_hour):\n",
    "                        last_row = data_ds.iloc[-1]\n",
    "                        ds_obj = maya.parse(last_row[\"ds\"]).add(minutes = int(diff_time))\n",
    "                        dict_input = {\n",
    "                            \"year\": ds_obj.year,\n",
    "                            \"month\": ds_obj.month,\n",
    "                            \"weekday\": ds_obj.weekday,\n",
    "                            \"hour\": ds_obj.hour\n",
    "                        }\n",
    "                        for k_var in names_prevs_vars:\n",
    "                            k = int(k_var[9:])\n",
    "                            if k == 1:\n",
    "                                dict_input[k_var] = str(last_row[\"y\"])\n",
    "                            else:\n",
    "                                dict_input[k_var] = str(last_row[\"prev_val_{i}\".format(i = k -1)])\n",
    "                        dict_input = [dict_input]\n",
    "                        data_input = pd.DataFrame(dict_input)\n",
    "                        pred_pool = Pool(\n",
    "                            data = data_input, \n",
    "                            cat_features = cat_features_names\n",
    "                            )\n",
    "                        pred_value = catboost_model.predict(pred_pool)[0][0]\n",
    "                        dict_input[0][\"y\"] = pred_value\n",
    "                        dict_input[0][\"ds\"] = datetime.strftime(ds_obj.datetime(), \"%Y-%m-%d %H:%M:%S\")\n",
    "                        dict_input[0][\"yhat\"] = last_row[\"yhat\"] + float(pred_value)\n",
    "                        data_add = pd.DataFrame(dict_input)\n",
    "                        data_ds = pd.concat([data_ds, data_add],ignore_index = True)\n",
    "            return data_ds\n",
    "\n",
    "        def Metrics_CatBoost(accuracy_score_train, accuracy_score_test, metrics_list):\n",
    "            metrics = {\n",
    "                'metrics': [\n",
    "                    {\n",
    "                    'name': 'accuracy_train',\n",
    "                    'numberValue':  float(accuracy_score_train),\n",
    "                    'format': \"PERCENTAGE\"\n",
    "                    },\n",
    "                    {\n",
    "                        'name': 'accuracy_test',\n",
    "                        \"numberValue\": float(accuracy_score_test),\n",
    "                        \"format\": \"PERCENTAGE\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"asset_number\",\n",
    "                        \"numberValue\": asset_name,\n",
    "                        \"format\": \"RAW\"\n",
    "                    }\n",
    "                ]}  \n",
    "            \n",
    "            metrics_list.append(metrics)\n",
    "            return metrics_list\n",
    "\n",
    "        # Prophet Functions\n",
    "\n",
    "        def Train_Prophet(train_data, num_days, measures_per_hour, diff_time):\n",
    "            from prophet import Prophet\n",
    "\n",
    "            m = Prophet(daily_seasonality=True, weekly_seasonality=True, changepoint_prior_scale = 0.05)\n",
    "            m.fit(train_data)\n",
    "            future = m.make_future_dataframe(periods= 24*(2 + num_days)*measures_per_hour , freq=\"{minutes}T\".format(minutes = diff_time))\n",
    "            forecast = m.predict(future)\n",
    "\n",
    "            forecast_test = forecast[\"yhat\"].tolist()[-24*measures_per_hour*(2 + num_days):-24*measures_per_hour]\n",
    "\n",
    "            return forecast, forecast_test, m\n",
    "\n",
    "        def GetMetricsProphet(forecast, train_data, test_data, num_days, dict_assets, metrics_list):\n",
    "            try:\n",
    "                asset_number = dict_asset[asset_name]\n",
    "            except:\n",
    "                asset_number = 3\n",
    "            try:\n",
    "                forecast_train = forecast[\"yhat\"].tolist()[:-24*4*(2 + num_days)]\n",
    "                real_vals_train = train_data[\"y\"].tolist()\n",
    "                r2_score_train = r2_score(real_vals_train, forecast_train)\n",
    "\n",
    "            except:\n",
    "                r2_score_train = 0\n",
    "            \n",
    "            real_vals_test = test_data[\"y\"].tolist()\n",
    "\n",
    "            if len(forecast_test) == len(real_vals_test):\n",
    "                r2_score_test = r2_score(real_vals_test, forecast_test)\n",
    "                metrics = {\n",
    "                    'metrics': [\n",
    "                        {\n",
    "                        'name': 'r2_score_test',\n",
    "                        'numberValue':  float(r2_score_test),\n",
    "                        'format': \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            'name': 'r2_score_train',\n",
    "                            \"numberValue\": float(r2_score_train),\n",
    "                            \"format\": \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"asset_number\",\n",
    "                            \"numberValue\": asset_name,\n",
    "                            \"format\": \"RAW\"\n",
    "                        }\n",
    "                    ]}  \n",
    "                \n",
    "                metrics_list.append(metrics)\n",
    "                \n",
    "            else:\n",
    "                ic(len(forecast_test))\n",
    "                ic(len(real_vals_test))\n",
    "            \n",
    "            return metrics_list\n",
    "\n",
    "        def SaveModelProphet(model, measurement_name, asset_name, pilot_name):\n",
    "            with open(\"/tmp/model_prophet.json\", 'w') as fout:\n",
    "                fout.write(model_to_json(model))  # Save model\n",
    "            \n",
    "\n",
    "            date = maya.when(\"now\").rfc2822()\n",
    "            f_name = \"model_{domain}_{asset}.json\"\\\n",
    "                .format(domain = measurement_name, asset = asset_name)\n",
    "            try:\n",
    "                result = client.fput_object(\n",
    "                    \"test\", f_name, \"/tmp/model_prophet.json\"\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    \"created {0} object; etag: {1}, version-id: {2}\".format(\n",
    "                        result.object_name, result.etag, result.version_id,\n",
    "                    ),\n",
    "                )\n",
    "            except:\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model not saved for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "                s3 = boto3.resource(\n",
    "                    service_name='s3',\n",
    "                    aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                    aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                    endpoint_url='https://s3.tebi.io'\n",
    "                )\n",
    "\n",
    "                for bucket in s3.buckets.all():\n",
    "                    ic(bucket.name)\n",
    "                \n",
    "                # Upload a new file\n",
    "                data = open('/tmp/model_prophet.json', 'rb')\n",
    "                f_name = \"model_{pilot}_{domain}_{asset}_latest_prophet.json\"\\\n",
    "                .format(pilot = pilot_name,domain = measurement_name, asset = asset_name)\n",
    "                s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model sent to tebi for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "        \n",
    "        # LSTM Functions\n",
    "\n",
    "        def Train_LSTM(data, split_proportion, diff_time=60, num_days=1, \n",
    "                    measures_per_hour=1, n_epochs=100, batch_size=16):\n",
    "            \n",
    "            from darts.models import RNNModel\n",
    "            from darts import TimeSeries\n",
    "            from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "            if not isinstance(data, pd.DataFrame) or 'ds' not in data.columns or 'y' not in data.columns:\n",
    "                raise ValueError(\"The input data must be a pandas DataFrame with 'ds' and 'y' columns.\")\n",
    "\n",
    "            if not (0 < split_proportion < 1):\n",
    "                raise ValueError(\"The split_proportion must be a float between 0 and 1.\")\n",
    "\n",
    "            # fill missing values with the last available value\n",
    "            data = data.fillna(method='ffill')\n",
    "            \n",
    "            # Create a time series\n",
    "            series = TimeSeries.from_dataframe(data, 'ds', 'y',fill_missing_dates=True, freq=\"{minutes}T\".format(minutes = diff_time))\n",
    "\n",
    "            # Create training and validation sets:\n",
    "            train, val = series.split_after(pd.Timestamp(series.start_time() + pd.Timedelta(hours=int(len(series) * split_proportion))))\n",
    "\n",
    "            # Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
    "            transformer = Scaler()\n",
    "            train_transformed = transformer.fit_transform(train)\n",
    "            val_transformed = transformer.transform(val)\n",
    "            series_transformed = transformer.transform(series)\n",
    "\n",
    "            # predict *num_days* days ahead\n",
    "            pred_ahead = 24 * (2 + num_days) * measures_per_hour\n",
    "\n",
    "            my_model = RNNModel(\n",
    "                input_chunk_length=2 * pred_ahead,\n",
    "                model=\"LSTM\",\n",
    "                hidden_dim=25, \n",
    "                n_rnn_layers=1,\n",
    "                dropout=0.2,\n",
    "                training_length=pred_ahead,\n",
    "                batch_size=batch_size,\n",
    "                n_epochs=n_epochs,\n",
    "                optimizer_kwargs={\"lr\": 1e-3},\n",
    "                model_name=\"data_RNN\",\n",
    "                log_tensorboard=True,\n",
    "                random_state=42,\n",
    "                force_reset=True,\n",
    "                save_checkpoints=True,\n",
    "            )\n",
    "\n",
    "            my_model.fit(\n",
    "                train_transformed,\n",
    "                val_series=val_transformed,\n",
    "                verbose=True,\n",
    "            )\n",
    "            \n",
    "            historical_forecast = my_model.historical_forecasts(\n",
    "                                            series_transformed,\n",
    "                                            start=pd.Timestamp(val.start_time() - pd.Timedelta(hours=1)),\n",
    "                                            forecast_horizon=pred_ahead,\n",
    "                                            retrain=False,\n",
    "                                            verbose=True,\n",
    "                                        )\n",
    "            \n",
    "            historical_forecast = transformer.inverse_transform(historical_forecast)\n",
    "            historical_forecast = historical_forecast.pd_dataframe().reset_index()\n",
    "            # rename columns\n",
    "            historical_forecast.columns = ['ds', 'y']\n",
    "            historical_forecast.columns.name = None\n",
    "\n",
    "            # Predict\n",
    "            forecast = my_model.predict(n=pred_ahead, series=val_transformed)\n",
    "\n",
    "            # Inverse-transform forecasts and obtain the real predicted values\n",
    "            forecast = transformer.inverse_transform(forecast)\n",
    "            forecast = forecast.pd_dataframe().reset_index()\n",
    "            forecast.columns.name = None\n",
    "\n",
    "            forecast = pd.concat([historical_forecast, forecast], axis=0).reset_index(drop=True)\n",
    "\n",
    "            # Check the dataframe if the frequency is always {diff_time} minute\n",
    "            full_range = pd.date_range(forecast['ds'].iloc[0], forecast['ds'].iloc[-1], freq=\"{minutes}T\".format(minutes = diff_time))\n",
    "            assert full_range.difference(forecast['ds']).shape[0] == 0\n",
    "\n",
    "            forecast_test = forecast[\"y\"].tolist()[-24*measures_per_hour*(2 + num_days):-24*measures_per_hour]\n",
    "\n",
    "            return forecast, forecast_test, my_model\n",
    "        \n",
    "        def GetMetricsLSTM(forecast, train_data, test_data, num_days, dict_assets, metrics_list):\n",
    "            try:\n",
    "                asset_number = dict_assets[asset_name]\n",
    "            except:\n",
    "                asset_number = 3\n",
    "            try:\n",
    "                # take the comman ds for forecast and train_data\n",
    "                forecast_train = forecast[forecast['ds'].isin(train_data['ds'])].reset_index(drop=True)\n",
    "                real_vals_train = train_data[train_data['ds'].isin(forecast['ds'])].reset_index(drop=True)\n",
    "                r2_score_train = r2_score(real_vals_train['y'].to_list(), forecast_train['y'].to_list())\n",
    "            except:\n",
    "                r2_score_train = 0\n",
    "            \n",
    "            real_vals_test = test_data[\"y\"].tolist()\n",
    "\n",
    "            if len(forecast_test) == len(real_vals_test):\n",
    "                r2_score_test = r2_score(real_vals_test, forecast_test)\n",
    "                metrics = {\n",
    "                    'metrics': [\n",
    "                        {\n",
    "                        'name': 'r2_score_test',\n",
    "                        'numberValue':  float(r2_score_test),\n",
    "                        'format': \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            'name': 'r2_score_train',\n",
    "                            \"numberValue\": float(r2_score_train),\n",
    "                            \"format\": \"PERCENTAGE\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"asset_number\",\n",
    "                            \"numberValue\": asset_name,\n",
    "                            \"format\": \"RAW\"\n",
    "                        }\n",
    "                    ]}  \n",
    "                \n",
    "                metrics_list.append(metrics)\n",
    "                \n",
    "            else:\n",
    "                ic(len(forecast_test))\n",
    "                ic(len(real_vals_test))\n",
    "            return metrics_list\n",
    "\n",
    "        def SaveModelLSTM(model, measurement_name, asset_name, pilot_name):\n",
    "            model.save(\"/tmp/lstm_model.pt\")\n",
    "            # model_loaded = RNNModel.load(\"/tmp/lstm_model.pt\")\n",
    "            \n",
    "            date = maya.when(\"now\").rfc2822()\n",
    "            f_name = \"model_{domain}_{asset}.pt\"\\\n",
    "                .format(domain = measurement_name, asset = asset_name)\n",
    "            try:\n",
    "                result = client.fput_object(\n",
    "                    \"test\", f_name, \"/tmp/lstm_model.pt\"\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    \"created {0} object; etag: {1}, version-id: {2}\".format(\n",
    "                        result.object_name, result.etag, result.version_id,\n",
    "                    ),\n",
    "                )\n",
    "            except:\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model not saved for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "                s3 = boto3.resource(\n",
    "                    service_name='s3',\n",
    "                    aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                    aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                    endpoint_url='https://s3.tebi.io'\n",
    "                )\n",
    "\n",
    "                for bucket in s3.buckets.all():\n",
    "                    ic(bucket.name)\n",
    "                \n",
    "                # Upload a new file\n",
    "                data = open('/tmp/lstm_model.pt', 'rb')\n",
    "                f_name = \"model_{pilot}_{domain}_{asset}_latest_lstm.pt\"\\\n",
    "                .format(pilot = pilot_name,domain = measurement_name, asset = asset_name)\n",
    "                s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "\n",
    "                url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                message = \"Model sent to tebi for {measurement_name} - {asset_name}\".format(measurement_name = measurement_name, asset_name = asset_name)\n",
    "                webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                webhook.execute()\n",
    "\n",
    "        data_ds, max_date, last_cummulative_value = ModifyData(data, asset_name)\n",
    "        \n",
    "        if len(pd.unique(data_ds.y)) >= 20:\n",
    "\n",
    "            n_train = int(data_ds.shape[0] - 24 * 2 * measures_per_hour)\n",
    "            train_data = data_ds[0:n_train]\n",
    "            test_data = data_ds[n_train:]\n",
    "\n",
    "            # Prophet\n",
    "            forecast, forecast_test, model = Train_Prophet(train_data, num_days, measures_per_hour, diff_time)\n",
    "            metrics_list = GetMetricsProphet(forecast, train_data, test_data, num_days, dict_assets, metrics_list)\n",
    "            SaveModelProphet(model, measurement_name, asset_name, pilot_name)\n",
    "\n",
    "            # LSTM\n",
    "            forecast, forecast_test, model = Train_LSTM(data=train_data, split_proportion=0.9, diff_time=diff_time,\n",
    "                                                        num_days=num_days, measures_per_hour=measures_per_hour, \n",
    "                                                        n_epochs=25)\n",
    "            metrics_list = GetMetricsLSTM(forecast, train_data, test_data, num_days, dict_assets, metrics_list)\n",
    "            SaveModelLSTM(model, measurement_name, asset_name, pilot_name)\n",
    "\n",
    "        elif data_ds.shape[0] < 10:\n",
    "            print(\"Not enough values\")\n",
    "            forecast = data_ds\n",
    "\n",
    "        else:\n",
    "            data_ds, names_prevs_vars, cat_features_names = ManageData(data_ds)\n",
    "            \n",
    "            catboost_model, accuracy_score_train, accuracy_score_test = Train_CatBoost(data_ds, cat_features_names)\n",
    "\n",
    "            forecast = Predict_CatBoost(catboost_model,\n",
    "                            data_ds, \n",
    "                            last_cummulative_value,\n",
    "                            num_days, measures_per_hour, diff_time,\n",
    "                            cat_features_names, names_prevs_vars)\n",
    "\n",
    "            Save_CatBoost(catboost_model, pilot_name, measurement_name, asset_name)\n",
    "            metrics_list = Metrics_CatBoost(accuracy_score_train, accuracy_score_test,\n",
    "                                            metrics_list)\n",
    "\n",
    "        ic(metrics_list)\n",
    "        return forecast[forecast.ds > max_date], metrics_list\n",
    "\n",
    "    \n",
    "    # Get Parameters\n",
    "    \n",
    "    dict_assets = {}\n",
    "    measures_per_hour = int(60/int(diff_time))\n",
    "    time_prediction = maya.now().epoch\n",
    "    num_days = int(num_days)\n",
    "\n",
    "\n",
    "    with open(input_data_path) as file:\n",
    "        data_str = json.load(file)\n",
    "    \n",
    "    data = pd.DataFrame(data_str)\n",
    "    metrics_list = []\n",
    "    ic(asset_name)\n",
    "    \n",
    "    \n",
    "\n",
    "    try:\n",
    "        forecasted_data, metrics_list = ForecastData(data, asset_name, measurement_name, metrics_list, measures_per_hour, diff_time)\n",
    "        max_ds = forecasted_data[\"ds\"].tolist()[-1]\n",
    "    except AttributeError:\n",
    "        forecasted_dict = {\n",
    "            \"ds\": [],\n",
    "            \"yhat\": []\n",
    "        }\n",
    "        forecasted_data = pd.DataFrame(forecasted_dict)\n",
    "        metrics_list = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        forecasted_data.to_csv('/tmp/forecast_test_{asset_name}.csv'.format(asset_name = asset_name), index = False)\n",
    "        data_to_send = open('/tmp/forecast_test_{asset_name}.csv'.format(asset_name = asset_name), 'rb')\n",
    "        f_name = \"forecast_test_{pilot}_{asset_name}.csv\".format(pilot = pilot_name, asset_name = asset_name)\n",
    "        s3 = boto3.resource(\n",
    "            service_name='s3',\n",
    "            aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "            aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "            endpoint_url='https://s3.tebi.io'\n",
    "        )\n",
    "        s3.Bucket('test-pf').put_object(Key=f_name, Body=data_to_send)\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Data File: {f_name} Saved to Tebi\".format(f_name = f_name)\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "    except:\n",
    "        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "        message = \"Unable to save data to tebi\"\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "\n",
    "        message = \"Values for {asset_name}: {list_values}\".format(asset_name = asset_name,list_values = forecasted_data.yhat.tolist())\n",
    "        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "        webhook.execute()\n",
    "\n",
    "    forecasted_data.to_csv(forecast_data_path, index = False)\n",
    "\n",
    "\n",
    "    domain_ = \"electricity\"\n",
    "\n",
    "    with open(\"/tmp/metrics_{domain}.json\".format(domain = domain_), \"w\") as file:\n",
    "        json.dump(metrics_list, file)\n",
    "    \n",
    "    \n",
    "\n",
    "    s3 = boto3.resource(\n",
    "                service_name='s3',\n",
    "                aws_access_key_id='QyvycO9kc2cm58K8',\n",
    "                aws_secret_access_key='tKtUrdQzQgWfhfBwhbQF3yGbyZ43oPn92iGAT7g0',\n",
    "                endpoint_url='https://s3.tebi.io'\n",
    "            )\n",
    "    data = open(\"/tmp/metrics_{domain}.json\".format(domain = domain_), 'rb')\n",
    "    f_name = \"metrics_{domain}_latest.json\"\\\n",
    "    .format(domain = measurement_name)\n",
    "    s3.Bucket('test-pf').put_object(Key=f_name, Body=data)\n",
    "    \n",
    "\n",
    "\n",
    "    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "    message = \"Forecasting done for {domain} and asset name : {asset_name}\".format(domain = domain_, asset_name = asset_name)\n",
    "    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "    webhook.execute()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckSendForecast(send_forecast:str) -> bool:\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "    message = \"Forecast not send, the option is not 'yes' or 'no', please check this, the option sent was{option}\".format(option = send_forecast)\n",
    "    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "    \n",
    "    if send_forecast == \"yes\":\n",
    "        return True\n",
    "    elif send_forecast == \"no\":\n",
    "        return False\n",
    "    else:\n",
    "        webhook.execute()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SendForecast(input_forecast_data_path: InputPath(str),url_pilot:str, pilot_name:str, asset_name : str, measurement_name: str):\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from icecream import ic\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import maya\n",
    "    from tqdm import tqdm\n",
    "    from datetime import datetime\n",
    "\n",
    "    forecasted_data = pd.read_csv(input_forecast_data_path)\n",
    "\n",
    "    if measurement_name == \"electricity_meter\":\n",
    "        domain_ = \"electricity\"\n",
    "    elif measurement_name == \"heat_meter\":\n",
    "        domain_ = \"heat\"\n",
    "    else:\n",
    "        domain_ = \"electricity\"\n",
    "\n",
    "    \n",
    "    time_prediction = datetime.strftime(maya.now().datetime(), \"%Y-%m-%d %H:%M:%S\")\n",
    "    if asset_name in [\"building1\", \"building2\", \"psnc\", \"office\"]:\n",
    "        direction_energy = \"in\"\n",
    "        type_ = \"None\"\n",
    "    elif measurement_name == \"pv\":\n",
    "        direction_energy = \"out\"\n",
    "        type_ = \"renewable\"\n",
    "    elif measurement_name == \"office\":\n",
    "        direction_energy = \"in\"\n",
    "        type_ = \"None\"\n",
    "    else:\n",
    "        direction_energy = \"out\"\n",
    "        type_ = \"renewable\"\n",
    "\n",
    "    def GetPostData(time_, value, \n",
    "                    measurement_name= measurement_name, \n",
    "                    asset_name= asset_name, domain_ = domain_, \n",
    "                    direction_energy = direction_energy, type_ = type_, \n",
    "                    time_prediction = time_prediction):\n",
    "\n",
    "        data_post = {\n",
    "                \"bucket\": \"renergetic\",\n",
    "                \"measurement\": measurement_name,\n",
    "                \"fields\":{\n",
    "                    \"energy\": value,\n",
    "                    \"time\": time_,\n",
    "                },\n",
    "                \"tags\":{\n",
    "                    \"domain\": domain_,\n",
    "                    \"typeData\": \"forecasting\",\n",
    "                    \"direction\": direction_energy,\n",
    "                    \"prediction_window\": \"24h\",\n",
    "                    \"asset_name\": asset_name,\n",
    "                    \"measurement_type\": type_,\n",
    "                    \"time_prediction\": time_prediction\n",
    "                }\n",
    "            }\n",
    "        return data_post\n",
    "\n",
    "    \n",
    "    for index, row in tqdm(forecasted_data.iterrows(), total = forecasted_data.shape[0]):\n",
    "        time_ = str(row[\"ds\"])\n",
    "        value = row[\"yhat\"]\n",
    "\n",
    "        \n",
    "        data_post = GetPostData(time_, value)\n",
    "        url = url_pilot\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.request(\"POST\", url, headers=headers, data=json.dumps(data_post))\n",
    "            status_code = response.status_code\n",
    "\n",
    "        except:\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Error in updating value for measurement name: {measurement_name} in asset: {asset_name} in time {time_pred}\"\\\n",
    "                .format(measurement_name = \"electricity_meter\", asset_name = asset_name, time_pred = data_post[\"fields\"][\"time\"])\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "            status_code = 200\n",
    "        \n",
    "        if status_code > 299:\n",
    "            url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "            message = \"Error in sending the value for measurement name: {measurement_name} in asset: {asset_name} in time {time_pred}\"\\\n",
    "                .format(measurement_name = measurement_name, asset_name = asset_name, time_pred = data_post[\"fields\"][\"time\"])\n",
    "            webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "            webhook.execute()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckSendNotification(send_notifications_check:str) -> bool:\n",
    "    if send_notifications_check == \"no notifications\":\n",
    "        return False\n",
    "    else: \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SendNotification(forecast_data_path: InputPath(str), threshold_data_path: InputPath(str), asset_name):\n",
    "    \n",
    "    import json \n",
    "    import pandas as pd\n",
    "    from discord_webhook import DiscordWebhook\n",
    "    import maya\n",
    "    import requests\n",
    "    import fuckit\n",
    "\n",
    "    forecast_data = pd.read_csv(forecast_data_path)\n",
    "\n",
    "    # get notification code for anomaly high and low\n",
    "    @fuckit\n",
    "    def GetNotificationCodes():\n",
    "        url_notification_definition = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification/definition\"\n",
    "        payload={}\n",
    "        headers = {}\n",
    "\n",
    "        response = requests.request(\"GET\", url_notification_definition, headers=headers, data=payload)\n",
    "        try:\n",
    "            dict_notifications = response.json()\n",
    "        except:\n",
    "            dict_notifications = []\n",
    "\n",
    "        for notif in dict_notifications:\n",
    "            if notif[\"message\"] == \"message.anomaly.high\":\n",
    "                code_high = notif[\"code\"]\n",
    "            elif notif[\"message\"] == \"message.anomaly.low\":\n",
    "                code_low = notif[\"code\"]\n",
    "        codes = {\n",
    "            \"code_high\": code_high,\n",
    "            \"code_low\": code_low\n",
    "        }\n",
    "        return codes\n",
    "    \n",
    "    def ObtainCodes(codes):\n",
    "        if \"code_low\" in codes.keys():\n",
    "            code_low = codes[\"code_low\"]\n",
    "        else:\n",
    "            code_low = 0\n",
    "\n",
    "        if \"code_high\" in codes.keys():\n",
    "            code_high = codes[\"code_high\"]\n",
    "        else:\n",
    "            code_high = 0\n",
    "        return code_high, code_low\n",
    "    \n",
    "    def GetIds(asset_name):\n",
    "        # get asset_id for asset_name\n",
    "        payload = {}\n",
    "        headers = {}\n",
    "        url_asset_name = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/assets?name={asset_name}\".format(asset_name = asset_name)\n",
    "        try:\n",
    "            response = requests.request(\"GET\", url_asset_name, headers=headers, data=payload)\n",
    "            dict_asset = response.json()[0]\n",
    "            id_asset = dict_asset[\"id\"]\n",
    "        except:\n",
    "            id_asset = -1\n",
    "\n",
    "        id_dashboard = 1\n",
    "\n",
    "        return id_asset, id_dashboard\n",
    "\n",
    "    def NotificationProcess(forecast_data, code_low, code_high, id_asset, id_dashboard):\n",
    "        max_ds = max(row[\"ds\"])\n",
    "        for index,row in forecast_data.iterrows():\n",
    "            value = row[\"yhat\"]\n",
    "            time_ = str(row[\"ds\"])\n",
    "            # SEND NOTIFICATION\n",
    "            if mode == \"none\":\n",
    "                if value < threshold_min:\n",
    "                    date_from = maya.parse(time_).epoch\n",
    "                    mode = \"lower\"\n",
    "\n",
    "                    if time_ == max_ds:\n",
    "                        date_to = maya.parse(time_).add(minutes = 15).epoch\n",
    "                        dict_post = {\n",
    "                            \"notification_code\": code_low,\n",
    "                            \"date_from\": date_from,\n",
    "                            \"date_to\": date_to,\n",
    "                            \"asset_id\": id_asset,\n",
    "                            \"dashboard_id\": id_dashboard\n",
    "                        }\n",
    "                        url = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "                        headers = {\n",
    "                            \"Content-Type\": \"application/json\"\n",
    "                        }\n",
    "                        response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "                        status_code = response.status_code\n",
    "\n",
    "                        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                        message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "                            format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "                        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                        webhook.execute()\n",
    "                \n",
    "                if value > threshold_max:\n",
    "                    date_from = maya.parse(time_).epoch\n",
    "                    mode = \"upper\"\n",
    "\n",
    "                    if time_ == max_ds:\n",
    "                        date_to = maya.parse(time_).add(minutes = 15).epoch\n",
    "                        dict_post = {\n",
    "                            \"notification_code\": code_low,\n",
    "                            \"date_from\": date_from,\n",
    "                            \"date_to\": date_to,\n",
    "                            \"asset_id\": id_asset,\n",
    "                            \"dashboard_id\": id_dashboard\n",
    "                        }\n",
    "                        url = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "                        headers = {\n",
    "                            \"Content-Type\": \"application/json\"\n",
    "                        }\n",
    "                        response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "                        status_code = response.status_code\n",
    "\n",
    "                        url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                        message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "                            format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "                        webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                        webhook.execute()\n",
    "                \n",
    "            elif mode == \"lower\":\n",
    "                if value > threshold_min or time_ == max_ds:\n",
    "                    date_to = maya.parse(time_).epoch\n",
    "                    dict_post = {\n",
    "                        \"notification_code\": code_low,\n",
    "                        \"date_from\": date_from,\n",
    "                        \"date_to\": date_to,\n",
    "                        \"asset_id\": id_asset,\n",
    "                        \"dashboard_id\": id_dashboard\n",
    "                    }\n",
    "\n",
    "                    url = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "                    headers = {\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    }\n",
    "                    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "                    status_code = response.status_code\n",
    "\n",
    "                    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                    message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "                        format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "                    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                    webhook.execute()\n",
    "\n",
    "                    if value > threshold_max:\n",
    "                        mode = \"upper\"\n",
    "                        date_from = maya.parse(time_).epoch\n",
    "                    else:\n",
    "                        mode = \"none\"\n",
    "            \n",
    "            elif (mode == \"upper\" or time_ == max_ds):\n",
    "                if value < threshold_max:\n",
    "                    date_to = maya.parse(time_).epoch\n",
    "                    dict_post = {\n",
    "                        \"notification_code\": code_high,\n",
    "                        \"date_from\": date_from*1000,\n",
    "                        \"date_to\": date_to*1000,\n",
    "                        \"asset_id\": id_asset,\n",
    "                        \"dashboard_id\": id_dashboard\n",
    "                    }\n",
    "\n",
    "                    if value < threshold_min:\n",
    "                        date_from = maya.parse(time_).epoch\n",
    "                        mode = \"lower\"\n",
    "                    else:\n",
    "                        mode = \"none\"\n",
    "\n",
    "                    url = \"http://api-swagger-ren-prototype.apps.paas-dev.psnc.pl/api/notification\"\n",
    "                    headers = {\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    }\n",
    "                    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(dict_post))\n",
    "                    status_code = response.status_code\n",
    "\n",
    "                    url_disc = \"https://discord.com/api/webhooks/1002537248622923816/_9XY9Hi_mjzh2LTVqnmSKXlIFJ5rgBO2b8xna5pynUrzALgtC4aXSFq89uMdlW_v-ZzT\"\n",
    "                    message = \"Anomaly detect between {date_from} and {date_to} to asset {asset_name}. Response of Notification {status_code}\".\\\n",
    "                        format(date_from = date_from, date_to = date_to, status_code = status_code, asset_name = asset_name)\n",
    "                    webhook = DiscordWebhook(url = url_disc, content = message)\n",
    "                    webhook.execute()\n",
    "\n",
    "    with open(threshold_data_path) as file:\n",
    "        dict_threshold = json.load(file)\n",
    "        try:\n",
    "            threshold_min = dict_threshold[asset_name]\n",
    "        except:\n",
    "            threshold_min = 0\n",
    "        \n",
    "        try:\n",
    "            threshold_max = dict_threshold[asset_name]\n",
    "        except:\n",
    "            threshold_max = 1000000000000\n",
    "    \n",
    "    codes = GetNotificationCodes()\n",
    "    code_high, code_low = ObtainCodes(codes)\n",
    "    id_asset, id_dashboard = GetIds(asset_name)\n",
    "    NotificationProcess(forecast_data, code_low, code_high, id_asset, id_dashboard)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUXILIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportModelToMinio(input_model_path: InputPath(str),measurement_name, \n",
    "    path_minio = \"minio.kubeflow-renergetic.svc:9000\",\n",
    "    access_key = \"minio\",\n",
    "    secret_key = \"DaTkKc45Hxr1YLR4LxR2xJP2\"\n",
    "    ):\n",
    "\n",
    "    from minio import Minio\n",
    "    import json\n",
    "    with open(input_model_path) as file:\n",
    "        model_serialiazed = json.load()\n",
    "    client = Minio(\n",
    "        path_minio,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_List_Assets(measurement_name, dict_assets) -> dict:\n",
    "    import json\n",
    "    dict_assets = json.loads(dict_assets)\n",
    "    print(measurement_name)\n",
    "    print(dict_assets)\n",
    "    print(type(dict_assets))\n",
    "    return dict_assets[measurement_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REN_Forecast_Test_Pipeline(url_pilot,\n",
    "    diff_time:int,\n",
    "    filter_vars:list = [],\n",
    "    filter_case:list = [],\n",
    "    url = \"minio-kubeflow-renergetic.apps.dcw1-test.paas.psnc.pl\",\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"DaTkKc45Hxr1YLR4LxR2xJP2\",\n",
    "    min_date = \"yesterday\",\n",
    "    max_date = \"today\",\n",
    "    mode = \"no notifications\",\n",
    "    list_measurements:list = [\"electricity_meter\", \"heat_meter\"],\n",
    "    dict_assets : dict = {\n",
    "        \"electricity_meter\": [\"building1\", \"building2\"],\n",
    "        \"heat_meter\": [\"building1\", \"building2\"]\n",
    "    },\n",
    "    key_measurement = \"energy\",\n",
    "    type_measurement = \"simulation\",\n",
    "    pilot_name = \"Virtual\",\n",
    "    hourly_aggregate = \"no\",\n",
    "    minute_aggregate = \"no\",\n",
    "    num_days: int = 1,\n",
    "    send_forecast = \"yes\"\n",
    "    ):\n",
    "\n",
    "    env_var = V1EnvVar(name='HOME', value='/tmp')\n",
    "    download_data_op = comp.create_component_from_func(\n",
    "        GetData, packages_to_install = [\"requests\", \"numpy\", \"maya\",\"pandas\", \"icecream\", \"tqdm\", \"discord-webhook\", \"retry\"], output_component_file = \"download_data_op_component.yaml\")\n",
    "    get_thresholds_op = comp.create_component_from_func(\n",
    "        GetThresholds, packages_to_install= [\"requests\"], output_component_file= \"thresholds_component.yaml\"\n",
    "    )\n",
    "    get_list_op = comp.create_component_from_func(\n",
    "        Get_List_Assets, output_component_file= \"get_list_component.yaml\"\n",
    "    )\n",
    "    process_data_op = comp.create_component_from_func(\n",
    "        ProcessData, packages_to_install= [\"maya\", \"pandas\", \"icecream\", \"tqdm\"], output_component_file= \"process_data_op_component.yaml\"\n",
    "    )\n",
    "    forecast_data_op = comp.create_component_from_func(\n",
    "        ForecastProcess, packages_to_install = [\"requests\", \"numpy\", \"maya\",\"pandas\", \"icecream\", \"prophet\", \"discord-webhook\", \"tqdm\", \"minio\", \"boto3\", \"scikit-learn\", \"catboost\"], output_component_file = \"forecast_data_op_component.yaml\")\n",
    "    check_send_forecast_op = comp.create_component_from_func(\n",
    "        CheckSendForecast, packages_to_install=[\"discord-webhook\"], output_component_file= \"check_send_forecast_component.yaml\"\n",
    "    )\n",
    "    send_forecast_op = comp.create_component_from_func(SendForecast, packages_to_install=[\"requests\", \"numpy\", \"maya\",\"pandas\", \"icecream\", \"discord-webhook\", \"tqdm\", \"minio\", \"boto3\"], output_component_file= \"send_forecast_comp.yaml\")\n",
    "\n",
    "    check_send_notification_op = comp.create_component_from_func(\n",
    "        CheckSendNotification, output_component_file= \"check_send_notification.yaml\"\n",
    "    )\n",
    "    send_notification_op = comp.create_component_from_func(\n",
    "        SendNotification, output_component_file=\"send_notification.yaml\"\n",
    "    )\n",
    "    get_thresholds_task = get_thresholds_op()\n",
    "\n",
    "\n",
    "    with dsl.ParallelFor(list_measurements) as measurement:\n",
    "        download_task = (download_data_op(measurement, min_date, max_date, url_pilot,type_measurement, key_measurement, filter_vars, filter_case).add_env_variable(env_var)\n",
    "                            .set_memory_request('500M')\n",
    "                            .set_memory_limit('1Gi')\n",
    "                            .set_cpu_request('1')\n",
    "                            .set_cpu_limit('2'))\n",
    "        process_task = (process_data_op(download_task.output, \n",
    "                        hourly_aggregate,\n",
    "                        minute_aggregate,\n",
    "                        min_date, \n",
    "                        max_date))\n",
    "        get_list_task = (get_list_op(measurement, dict_assets))\n",
    "\n",
    "        check_send_forecast_task = check_send_forecast_op(send_forecast)\n",
    "        check_send_notification_task = check_send_notification_op(mode)\n",
    "\n",
    "        if hourly_aggregate in [\"sum\", \"mean\"]:\n",
    "            diff_time = 60\n",
    "        \n",
    "        with dsl.ParallelFor(get_list_task.output) as asset:\n",
    "            forecast_task = (forecast_data_op(process_task.output,\n",
    "            measurement, \n",
    "            url, \n",
    "            access_key, \n",
    "            secret_key, \n",
    "            mode,\n",
    "            url_pilot,\n",
    "            diff_time,\n",
    "            pilot_name,\n",
    "            send_forecast,\n",
    "            asset,\n",
    "            num_days).add_env_variable(env_var))\n",
    "\n",
    "            with dsl.Condition(check_send_forecast_task.output == True):\n",
    "                send_forecast_task = send_forecast_op(forecast_task.outputs[\"forecast_data\"], url_pilot, pilot_name, asset, measurement)\n",
    "\n",
    "            with dsl.Condition(check_send_notification_task.output == True):\n",
    "                send_notification_task = send_notification_op(forecast_task.outputs[\"forecast_data\"], get_thresholds_task.output, asset)\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func = REN_Forecast_Test_Pipeline, package_path =\"Forecast_Data_Pipeline.yaml\")\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02e15b1dc1a9053df8aede2000f19bd423bcf01320081b3a1e044a45ff452adb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
